{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-15\n",
    "\n",
    "## Fixed points and contraction mappings\n",
    "\n",
    "As discussed in previous lectures, many iterations we consider have the form\n",
    "$$\n",
    "  x^{k+1} = G(x^k)\n",
    "$$\n",
    "where $G : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^n$. We call $G$ a\n",
    "*contraction* on $\\Omega$ if it is Lipschitz with constant less than\n",
    "one, i.e. \n",
    "$$\n",
    "  \\|G(x)-G(y)\\| \\leq \\alpha \\|x-y\\|, \\quad \\alpha < 1.\n",
    "$$\n",
    "A sufficient (but not necessary) condition for $G$ to be Lipschitz on $\\Omega$ is if\n",
    "$G$ is differentiable and $\\|G'(x)\\| \\leq \\alpha$ for all $\\alpha \\in \\Omega$.\n",
    "\n",
    "According to the *contraction mapping theorem* or *Banach fixed point theorem*,\n",
    "when $G$ is a contraction on a closed set $\\Omega$ and $G(\\Omega) \\subseteq \\Omega$, \n",
    "there is a unique fixed point $x^* \\in \\Omega$ (i.e. a point such that $x^* = G(x^*)$).\n",
    "If we can express the solution of a nonlinear equation as the fixed\n",
    "point of a contraction mapping, we get two immediate benefits:\n",
    "\n",
    "-   We know that a solution exists and is unique (at least, it is unique\n",
    "    within some $\\Omega$). This is a nontrivial advantage, as it is easy\n",
    "    to write nonlinear equations that have no solutions, or have\n",
    "    continuous families of solutions, without realizing that there is a\n",
    "    problem.\n",
    "\n",
    "-   We have a numerical method — albeit a potentially slow one — for\n",
    "    computing the fixed point. We take the fixed point iteration\n",
    "    $$\n",
    "      x^{k+1} = G(x^k)\n",
    "    $$\n",
    "    started from some $x^0 \\in \\Omega$, and we\n",
    "    subtract the fixed point equation $x^* = G(x^*)$ to get an iteration\n",
    "    for $e^k = x^k-x^*$: \n",
    "    $$\n",
    "      e^{k+1} = G(x^* + e^k) - G(x^*).\n",
    "    $$\n",
    "    Using contractivity, we get\n",
    "    $$\n",
    "      \\|e^{k+1}\\| = \\|G(x^* + e^k)-G(x^*)\\| \\leq \\alpha \\|e^k\\|,\n",
    "    $$\n",
    "    which implies that $\\|e^k\\| \\leq \\alpha^k \\|e^0\\| \\rightarrow 0$.\n",
    "\n",
    "When error goes down by a factor of $\\alpha$ at each step, we say the\n",
    "iteration is *linearly convergent* (or *geometrically convergent*).\n",
    "The name reflects a semilogarithmic plot of (log) error\n",
    "versus iteration count; if the errors lie on a straight line, we have\n",
    "linear convergence. Contractive fixed point iterations converge at least\n",
    "linearly, but may converge more quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  The mapping $x \\mapsto x/2$ is a contraction on $(0,\\infty)$, but does not have a fixed point on that interval.\n",
    "    Why does this not contradict the contraction mapping theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. For $S > 0$, show the mapping $g(x) = \\frac{1}{2} (x-S/x)$ is a contraction on the\n",
    "   interval $[\\sqrt{S}, \\infty)$.  What is the fixed point?  What is the Lipschitz constant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton’s method for nonlinear equations\n",
    "\n",
    "The idea behind Newton’s method is to approximate a nonlinear\n",
    "$f \\in C^1$ by linearizations around successive guesses. We then get the\n",
    "next guess by finding where the linearized approximation is zero. That\n",
    "is, we set \n",
    "$$\n",
    "  f(x^{k+1}) \\approx f(x^k) + f'(x^k) (x^{k+1}-x^k) = 0,\n",
    "$$\n",
    "which we can rearrange to \n",
    "$$\n",
    "  x^{k+1} = x^k - f'(x^k)^{-1} f(x^k).\n",
    "$$\n",
    "To emphasize that we do not want to actually form an inverse, and to set\n",
    "the stage for later variations on the method, we also write the\n",
    "iteration as \n",
    "$$\\begin{aligned}\n",
    "  f'(x^k) p^k &= -f(x^k) \\\\\n",
    "  x^{k+1} &= x^k + p^k.\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A toy example\n",
    "\n",
    "Consider the problem of finding the solutions to the system\n",
    "$$\\begin{aligned}\n",
    "  x + 2y &= 2 \\\\\n",
    "  x^2 + 4y^2 &= 4.\n",
    "\\end{aligned}$$\n",
    "That is, we are looking for the intersection of a straight line and an ellipse.  Note that this is a simple\n",
    "enough problem that we can compute the solution in closed form: there are intersections at $(0,1)$ and at $(2,0)$.\n",
    "Suppose we did not know this, and instead wanted to solve the system by Newton's iteration.  To do this, we need to\n",
    "write the problem in zero-finding form:\n",
    "$$\n",
    "  f(x,y) = \\begin{bmatrix} x + 2y - 2 \\\\ x^2 + 4y^2 - 4 \\end{bmatrix}\n",
    "$$\n",
    "We also need to compute the Jacobian $J = f'$:\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial (x,y)} = \n",
    "  \\begin{bmatrix} \n",
    "    \\frac{\\partial f_1}{\\partial x} & \\frac{\\partial f_1}{\\partial y} \\\\ \n",
    "    \\frac{\\partial f_2}{\\partial x} & \\frac{\\partial f_2}{\\partial y}\n",
    "  \\end{bmatrix} = \n",
    "  \\begin{bmatrix} 1 & 2 \\\\ 2x & 8y \\end{bmatrix}.\n",
    "$$\n",
    "Now let's run the iteration from an initial guess of $(1,2)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up function and Jacobian\n",
    "f(x) = [x[1] + 2*x[2] - 2 ;\n",
    "        x[1]^2 + 4*x[2]^2 - 4 ]\n",
    "J(x) = [1      2     ;\n",
    "        2*x[1] 8*x[2]]\n",
    "\n",
    "# Run ten steps of Newton from the initial guess at [1; 2]\n",
    "x = [1; 2]\n",
    "fx = f(x)\n",
    "resids = zeros(10)\n",
    "for k = 1:10\n",
    "    x -= J(x) \\ fx\n",
    "    fx = f(x)\n",
    "    resids[k] = norm(fx)\n",
    "end\n",
    "\n",
    "# Print the computed solution and the residual norms at each step\n",
    "println(x)\n",
    "plot(resids[resids .> 0], yscale=:log10, xlabel=\"\\$k\\$\", ylabel=\"\\$f(x_k)\\$\", legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Finding an eigenvalue of $A$ can be posed as a nonlinear equation solving problem:\n",
    "    we want to find $x$ and $\\lambda$ such that $Ax = \\lambda x$ and $x^T x = 1$.\n",
    "    Write a Newton iteration for this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Superlinear convergence\n",
    "\n",
    "Suppose $f(x^*) = 0$. Taylor expansion about $x^k$ gives\n",
    "$$\n",
    "  0 = f(x^*) = f(x^k) + f'(x^k) (x^*-x^k) + r(x^k)\n",
    "$$\n",
    "where the remainder term $r(x^k)$ is $o(\\|x^k-x^*\\|) = o(\\|e^k\\|)$. Hence,\n",
    "$$\n",
    "  x^{k+1} = x^* + f'(x^k)^{-1} r(x^k)\n",
    "$$\n",
    "and subtracting from $x^*$ from both sides gives\n",
    "$$\n",
    "  e^{k+1} = f'(x^k)^{-1} r(x^k) = f'(x^k)^{-1} o(\\|e^k\\|).\n",
    "$$\n",
    "If $\\|f'(x)^{-1}\\|$ is bounded for $x$ near $x^*$ and $x^0$ is close\n",
    "enough, this is sufficient to guarantee *superlinear convergence*.\n",
    "When we have a stronger condition, such as $f'$ Lipschitz, we get\n",
    "*quadratic convergence*, i.e. $e^{k+1} = O(\\|e^k\\|^2)$. Of course,\n",
    "this is all local theory – we need a good initial guess!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A more complex worked example\n",
    "\n",
    "We now consider a more serious example problem, a nonlinear system that\n",
    "comes from a discretized PDE reaction-diffusion model describing (for example) the\n",
    "steady state heat distribution in a medium going an auto-catalytic\n",
    "reaction. The physics is that heat is generated in the medium due to a\n",
    "reaction, with more heat where the temperature is higher. The heat then\n",
    "diffuses through the medium, and the outside edges of the domain are\n",
    "kept at the ambient temperature. The PDE and boundary conditions are\n",
    "$$\\begin{aligned}\n",
    "  v_{,xx} + \\exp(v) &= 0, \\quad x \\in (0,1) \\\\\n",
    "  v(0) = v(1) &= 0.\n",
    "\\end{aligned}$$\n",
    "We discretize the PDE for computer solution by introducing a mesh\n",
    "$x_i = ih$ for $i = 0, \\ldots, N+1$ and $h = 1/(N+1)$; the solution is\n",
    "approximated by $v(x_i) \\approx v_i$. We set $v_0 = v_{N+1} = 0$; \n",
    "when we refer to $v$ without subscripts, we mean the vector of entries\n",
    "$v_1$ through $v_N$. This discretization leads to the nonlinear system\n",
    "$$\n",
    "  f_i(v) \\equiv \\frac{v_{i-1}-2v_i+v_{i+1}}{h^2} + \\exp(v_i) = 0.\n",
    "$$\n",
    "for $i = 1, \\ldots, N$. This equation has two solutions;\n",
    "physically, these correspond to stable and unstable steady-state solutions\n",
    "of the time-dependent version of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function autocatalytic(v)\n",
    "    N = length(v)\n",
    "    fv        = exp.(v)\n",
    "    fv        -= 2*(N+1)^2*v\n",
    "    fv[1:N-1] += (N+1)^2*v[2:N  ]\n",
    "    fv[2:N  ] += (N+1)^2*v[1:N-1]\n",
    "    fv\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To solve for a Newton step, we need the Jacobian of $f$ with respect to\n",
    "the variables $v_j$ is the tridiagonal matrix. We write this as\n",
    "$$\n",
    "  J(v) = -h^{-2} T_N + \\operatorname{diag}(\\exp(v))\n",
    "$$\n",
    "where \n",
    "$$\n",
    "T_N =\n",
    "\\begin{bmatrix}\n",
    "  2 & -1 \\\\\n",
    "  -1 & 2 & -1 \\\\\n",
    "  & \\ddots & \\ddots & \\ddots \\\\\n",
    "  & & -1 & 2 & -1 \\\\\n",
    "  & & & -1 & 2\n",
    "\\end{bmatrix} \\in {\\mathbb{R}}^{N \\times N}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Jautocatalytic(v)\n",
    "    N = length(v)\n",
    "    SymTridiagonal(exp.(v) .- 2*(N+1)^2, (N+1)^2 * ones(N-1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an initial guess, we use $v_i = \\alpha x_i (1-x_i)$ for different values \n",
    "of $\\alpha$. For $\\alpha = 0$, we converge to the stable solution; for $\\alpha = 20$ and\n",
    "$\\alpha = 40$, we converge to the unstable solution. We eventually see\n",
    "quadratic convergence in all cases, but for $\\alpha = 40$ there is a\n",
    "longer period before convergence sets in.\n",
    "For $\\alpha = 60$, the method does not converge at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_autocatalytic(α, N=100, nsteps=50, rtol=1e-8; monitor = (v, resid) -> nothing)\n",
    "    v_all = [α*x*(1-x) for x in range(0.0, stop=1.0, length=N+2)]\n",
    "    v = v_all[2:N+1]\n",
    "    for step = 1:nsteps\n",
    "        fv = autocatalytic(v)\n",
    "        resid = norm(fv)\n",
    "        monitor(v, resid)\n",
    "        if resid < rtol\n",
    "            v_all[2:N+1] = v\n",
    "            return v_all\n",
    "        end\n",
    "        v -= Jautocatalytic(v)\\fv\n",
    "    end\n",
    "    error(\"Newton did not converge after $nsteps steps\")\n",
    "end\n",
    "\n",
    "function newton_autocatalytic_rhist(α, N=100, nsteps=50, rtol=1e-8)\n",
    "    rhist = []\n",
    "    monitor(v, resid) = push!(rhist, resid)\n",
    "    v = newton_autocatalytic(α, N, nsteps, rtol; monitor=monitor)\n",
    "    return v, rhist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0, rhist0 = newton_autocatalytic_rhist(0)\n",
    "v20, rhist20 = newton_autocatalytic_rhist(20)\n",
    "v40, rhist40 = newton_autocatalytic_rhist(40)\n",
    "\n",
    "xx = range(0.0, stop=1.0, length=102)\n",
    "p1 = plot(xx, v0, label=\"\\$\\\\alpha = 0\\$\")\n",
    "plot!(xx, v20, label=\"\\$\\\\alpha = 20\\$\")\n",
    "plot!(xx, v40, linestyle=:dash, label=\"\\$\\\\alpha = 40\\$\")\n",
    "\n",
    "p2 = plot(rhist0, yscale=:log10, label=\"\\$\\\\alpha = 0\\$\")\n",
    "plot!(rhist20, label=\"\\$\\\\alpha = 20\\$\")\n",
    "plot!(rhist40, linestyle=:dash, label=\"\\$\\\\alpha = 40\\$\")\n",
    "\n",
    "l = @layout [a b]\n",
    "plot(p1, p2, layout=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can derive a Newton-like fixed point iteration from the observation\n",
    "that if $v$ remains modest, the Jacobian is pretty close to $-h^2 T_N$.\n",
    "This gives us the iteration\n",
    "$$\n",
    "  h^{-2} T_N v^{k+1} = \\exp(v^k).\n",
    "$$ \n",
    "Below, we compare the convergence of this fixed\n",
    "point iteration to Newton’s method. The fixed point iteration does\n",
    "converge, but it shows the usual linear convergence, while Newton’s\n",
    "method converges quadratically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function fp_autocatalytic(α, N=100, nsteps=1000, rtol=1e-6; monitor = (v, resid) -> nothing)\n",
    "    v_all = [α*x*(1-x) for x in range(0.0, stop=1.0, length=N+2)]\n",
    "    v = v_all[2:N+1]\n",
    "    TN = SymTridiagonal(2*ones(N), -ones(N-1))\n",
    "    fact_TN = ldlt(TN)\n",
    "    for step = 1:nsteps\n",
    "        fv = autocatalytic(v)\n",
    "        resid = norm(fv)\n",
    "        monitor(v, resid)\n",
    "        if resid < rtol\n",
    "            v_all[2:N+1] = v\n",
    "            return v_all\n",
    "        end\n",
    "        v[:] = fact_TN \\ (exp.(v)/(N+1)^2)\n",
    "    end\n",
    "    error(\"Fixed point iteration did not converge after $nsteps steps (α=$α)\")\n",
    "end\n",
    "\n",
    "function fp_autocatalytic_rhist(α, N=100, nsteps=500, rtol=1e-8)\n",
    "    rhist = []\n",
    "    monitor(v, resid) = push!(rhist, resid)\n",
    "    v = fp_autocatalytic(α, N, nsteps, rtol; monitor=monitor)\n",
    "    return v, rhist\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v0f, rhistf0 = fp_autocatalytic_rhist(0)\n",
    "xx = range(0.0, stop=1.0, length=102)\n",
    "\n",
    "p3 = plot(xx, v0, label=\"Newton\")\n",
    "plot!(xx, v0f, label=\"Fixed point\")\n",
    "\n",
    "p4 = plot(rhist0, yscale=:log10, label=\"Newton\")\n",
    "plot!(rhistf0, label=\"Fixed point\")\n",
    "\n",
    "l = @layout [a b]\n",
    "plot(p3, p4, layout=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Modify the Newton solver for the discretization of the equation $v_{,xx} + \\lambda \\exp(v) = 0$.\n",
    "    What happens as $\\lambda$ grows greater than one?  For what size $\\lambda$ can you get a solution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beyond asymptotic convergence (optional)\n",
    "\n",
    "We saw in our example that Newton’s method may indeed fail if the\n",
    "initial guess is not sufficiently good. Let’s now be a little more\n",
    "precise about our error analysis in order to see where Newton gets into\n",
    "trouble.\n",
    "\n",
    "Let $M$ be a Lipschitz constant on $f'$ in some ball around $x^*$, i.e.\n",
    "$\\|f'(x)-f'(y)\\| \\leq M \\|x-y\\|$ for any points in the ball.  Then for $x^k = x^* + e^k$\n",
    "inside the ball, we know that\n",
    "$$\n",
    "  0 = f(x^*) = f(x^k) - f'(x^k) e^k + r^k, \\quad \\|r^k\\| \\leq \\frac{M}{2} \\|e^k\\|^2\n",
    "$$\n",
    "and from our earlier analysis,\n",
    "$$\n",
    "  \\|e^{k+1}\\| = \\|f'(x^k)^{-1} r^k\\| \\leq \\frac{M}{2} \\|f'(x^k)^{-1}\\| \\|e^k\\|^2\n",
    "$$\n",
    "Recall from earlier in the semester that a\n",
    "Neumann bound says if $A$ is nonsingular and $\\|A^{-1}\\| \\|E\\| < 1$,\n",
    "then \n",
    "$$\n",
    "  \\|(A+E)^{-1}\\| \\leq \\frac{\\|A^{-1}\\|}{1-\\|A^{-1}\\| \\|E\\|}.\n",
    "$$\n",
    "Applying this to $f'(x) = f'(x^*) + E$ with $\\|E\\| \\leq M \\|e^k\\|$ from the Lipschitz\n",
    "condition on $f'$, we have that if\n",
    "$$\n",
    "  \\gamma_k \\equiv M \\|f'(x^*)^{-1}\\| \\|e^k\\| < 1\n",
    "$$\n",
    "then\n",
    "$$\n",
    "  \\|f'(x)^{-1}\\| \\leq \\frac{\\|f'(x^*)^{-1} \\|}{1-\\gamma_k}.\n",
    "$$\n",
    "Plugging this into our error iteration, we have that if $\\gamma_k < 1$ then\n",
    "$$\n",
    "  \\|e^{k+1}\\| \\leq \\frac{\\gamma_k}{2 (1-\\gamma_k)} \\|e^k\\|\n",
    "$$\n",
    "Putting everything together, we have that if $\\gamma_k < 2/3$, then\n",
    "$\\|e^{k+1}\\| < \\|e^k\\|$ (and $\\gamma_{k+1} < \\gamma_k$). Therefore, we\n",
    "can guarantee convergence of Newton if the initial error satisfies\n",
    "$$\n",
    "  \\|e^0\\| < \\frac{2}{3 M \\|f'(x^*)^{-1}\\|}.\n",
    "$$\n",
    "The radius of guaranteed convergence will be small if $f'(x^*)$ is very \n",
    "close to singular or if $f'(x)$ can change very quickly ($M$ large). Of course, \n",
    "this is only a bound! In practice, the quickest way to see if Newton converges for a\n",
    "particular problem is to try and see what happens.\n",
    "\n",
    "The bound we gave is somewhat unsatisfactory in that it involves the\n",
    "norm of $f'(x^*)^{-1}$, and we do not know $f'(x^*)$ a priori. Moreover,\n",
    "we have assumed that there is a zero of the function, which is not\n",
    "always a given.  A slightly more subtle argument due to Kantorovich shows that Newton\n",
    "converges to a zero of $f$ close to the initial guess under some\n",
    "conditions on $f'(x^0)$ and $f(x^0)$, along with a Lipschitz condition\n",
    "on $f'(x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton’s method for optimization\n",
    "\n",
    "We now turn to using Newton’s method to solve optimization problems. We\n",
    "can approach this in two ways: either by applying Newton’s method to\n",
    "solve the critical point equations or by developing the method directly\n",
    "from the optimization problem. We will follow the latter approach.\n",
    "\n",
    "Suppose $f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}$ is $C^2$, and\n",
    "$x^*$ is a (strong) local minimizer of $f$. We want a sequence of points\n",
    "$x^k$ that converge to $x^*$, assuming the initial guess $x^0$ is good\n",
    "enough. As with solving nonlinear equations, our approach will be to\n",
    "approximate $f$ locally by a Taylor expansion:\n",
    "$$\n",
    "  f(x^k + z) \\approx f(x^k) + f'(x^k) z + \\frac{1}{2} z^T H(x^k) z\n",
    "$$\n",
    "Assuming $H(x^k)$ is positive definite, the quadratic approximation has\n",
    "a minimum at $z = -H(x^k)^{-1} \\nabla f(x^k)$. This gives us the Newton\n",
    "update \n",
    "$$\n",
    "  x^{k+1} = x^k - H(x^k)^{-1} \\nabla f(x^k).\n",
    "$$\n",
    "This is the same update we would get by applying Newton to the critical point equations.\n",
    "\n",
    "Newton’s method for optimization is special in that there is more\n",
    "structure in the critical point equations than there is for a nonlinear\n",
    "system of equations. For example, if $x^k$ is sufficiently close to a\n",
    "strong local minimum, then $H(x^k)$ is guaranteed to be positive\n",
    "definite. Not only does this mean we can use Cholesky (rather than LU)\n",
    "to solve the Newton update system, but the update\n",
    "$$\n",
    "  p = -H(x^k)^{-1} \\nabla f(x^k)\n",
    "$$\n",
    "has the property that\n",
    "$$\n",
    "  \\nabla f(x^k)^T p = -p^T H(x^k) p < 0,\n",
    "$$\n",
    "i.e. moving in the direction of $p$ decreases the objective function. That is, if $H(x^k)$ is\n",
    "positive definite, then $p$ is a *descent direction*. Of course,\n",
    "just because $p$ points in a downhill direction does not necessarily\n",
    "mean that the Newton step will reduce the objective function value – if\n",
    "we take a full Newton step, we may move so far that the Taylor expansion\n",
    "ceases to give good information.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1.  Consider the problem of finding a minimum of $\\phi : \\mathbb{R}^N \\rightarrow \\mathbb{R}$ given by\n",
    "    $$\n",
    "      \\phi(v) = \\frac{1}{2h^2} \\sum_{j=0}^N (v_j-v_{j+1})^2 - \\sum_{j=1}^N \\exp(v_j).\n",
    "    $$\n",
    "    where $v_0 = v_{N+1} = 0$.\n",
    "    What are $\\nabla \\phi$ and $H_{\\phi}$?  Sanity check against finite difference estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some practical issues\n",
    "\n",
    "In general, there is no guarantee that a given solution of nonlinear\n",
    "equations will have a solution; and if there is a solution, there is no\n",
    "guarantee of uniqueness. This has a practical implication: many\n",
    "incautious computationalists have been embarrassed to find that they\n",
    "have “solved” a problem that was later proved to have no solution!\n",
    "\n",
    "When we have reason to believe that a given system of equations has a\n",
    "solution — whether through mathematical argument or physical intuition —\n",
    "we still have the issue of finding a good enough initial estimate that\n",
    "Newton’s method will converge. In coming lectures, we will discuss\n",
    "“globalization” methods that expand the set of initial guesses for which\n",
    "Newton’s method converges; but globalization does not free us from the\n",
    "responsibility of trying for a good guess. Finding a good guess helps\n",
    "ensure that our methods will converge quickly, and to the “correct”\n",
    "solution (particularly when there are multiple possible solutions).\n",
    "\n",
    "We saw one explicit example of the role of the initial guess in our\n",
    "analysis of the discretized blowup PDE problem. Another example occurs\n",
    "when we use unguarded Newton iteration for optimization. Given a poor\n",
    "choice of initial guess, we are as likely to converge to a saddle point\n",
    "or a local maximum as to a minimum! But we will address this pathology\n",
    "in our discussion of globalization methods.\n",
    "\n",
    "If we have a nice problem and an adequate initial guess, Newton’s\n",
    "iteration can converge quite quickly. But even then, we still have to\n",
    "think about when we will be satisfied with an approximate solution. A\n",
    "robust solver should check a few possible termination criteria:\n",
    "\n",
    "-   *Iteration count*: It makes sense to terminate (possibly with\n",
    "    a diagnostic message) whenever an iteration starts to take more\n",
    "    steps than one expects — or perhaps more steps than one can afford.\n",
    "    If nothing else, this is necessary to deal with issues like poor\n",
    "    initial guesses.\n",
    "\n",
    "-   *Residual check*: We often declare completion when\n",
    "    $\\|f(x^k)\\|$ is sufficiently close to zero. What is “close to zero”\n",
    "    depends on the scaling of the problem, so users of black box solvers\n",
    "    are well advised to check that any default residual checks make\n",
    "    sense for their problems.\n",
    "\n",
    "-   *Update check*: Once Newton starts converging, a good estimate\n",
    "    for the error at step $x^k$ is $x^{k+1}-x^k$. A natural test is then\n",
    "    to make sure that $\\|x^{k+1}-x^k\\|/\\|x^{k+1}\\| < \\tau$ for some\n",
    "    tolerance $\\tau$. Of course, this is really an estimate of the\n",
    "    relative error at step $k$, but we will report the (presumably\n",
    "    better) answer $x^{k+1}$ — like anyone else who can manage it,\n",
    "    numerical analysts like to have their cake and eat it, too.\n",
    "\n",
    "A common problem with many solvers is to make the termination criteria\n",
    "too lax, so that a bad solution is accepted; or too conservative, so\n",
    "that good solutions are never accepted.\n",
    "\n",
    "One common mistake in coding Newton’s method is to goof in computing the\n",
    "Jacobian matrix. This error is not only very common, but also very often\n",
    "overlooked. After all, a good approximation to the Jacobian often still\n",
    "produces a convergent iteration; and when iterations diverge, it is hard\n",
    "to distinguish between problems due to a bad Jacobian and problems due\n",
    "to a bad initial guess. However, there is a simple clue to watch for\n",
    "that can help alert the user to a bad Jacobian calculation. In most\n",
    "cases, Newton converges quadratically, while “almost Newton” iterations\n",
    "converge linearly. If you think you have coded Newton’s method and a\n",
    "plot of the residuals shows linear behavior, look for issues with your\n",
    "Jacobian code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
