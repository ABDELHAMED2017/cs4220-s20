{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gauss-Newton\n",
    "\n",
    "Before beginning our (brief) discussion of trust region methods, we\n",
    "first turn to another popular iterative solver: the Gauss-Newton method\n",
    "for nonlinear least squares problems. Given\n",
    "$f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^m$ for $m > n$, we seek to\n",
    "minimize the objective function \n",
    "$$\n",
    "  \\phi(x) = \\frac{1}{2} \\|f(x)\\|^2.\n",
    "$$\n",
    "\n",
    "Examples are always helpful; let's take the example of localization from range data.\n",
    "We measure the Euclidean distance from an unknown point $x \\in \\mathbb{R}^d$ (typically\n",
    "$d = 2$ or $3$) to \"anchor\" points $a_1, a_2, \\ldots, a_m \\in \\mathbb{R}^d$.  Each distance\n",
    "is contaminated with a small amount of noise, i.e. the measurements are\n",
    "$$\n",
    "  \\rho_i = \\|x-a_i\\|_2 + w_i\n",
    "$$\n",
    "where $w_i$ is some small noise term (which we assume to be independent zero-mean Gaussians\n",
    "with common standard deviation).  The maximum likelihood estimate for $x$ is then obtained\n",
    "by minimizing $\\|f(x)\\|^2/2$ where $f_i(x) = \\|x-a_i\\|-\\rho_i$.  We will also need the\n",
    "Jacobian of $f$; note that\n",
    "$$\n",
    "  \\frac{\\partial f_i}{\\partial x} = \\frac{x-a_i}{\\|x-a_i\\|}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ntarget = 3   # Number of targets\n",
    "σdist = 5e-2  # Standard deviation in range estimate noise\n",
    "\n",
    "a = randn(2,ntarget)  # Targets\n",
    "xtrue = randn(2)      # True value\n",
    "\n",
    "# Set up range measurements with noise\n",
    "ρrange = zeros(ntarget)\n",
    "for k = 1:ntarget\n",
    "    ρrange[k] = norm(xtrue-a[:,k]) + σdist * randn()\n",
    "end\n",
    "\n",
    "# Objective f for nonlinear least squares problem\n",
    "function frange(x)\n",
    "    fx = zeros(ntarget)\n",
    "    for k = 1:ntarget\n",
    "        fx[k] = norm(x-a[:,k])-ρrange[k]\n",
    "    end\n",
    "    return fx\n",
    "end\n",
    "\n",
    "# Jacobian of the objective function\n",
    "function Jrange(x)\n",
    "    J = zeros(ntarget, length(x))\n",
    "    for k = 1:ntarget\n",
    "        rk = x-a[:,k]\n",
    "        J[k,:] = rk/norm(rk)\n",
    "    end\n",
    "    return J\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-3, 3, length=100)\n",
    "plot(xx, xx, (x,y) -> norm(frange([x; y]))^2/2, st=:contour)\n",
    "scatter!(a[1,:], a[2,:], marker=true, color=:red, label=\"a points\")\n",
    "scatter!([xtrue[1]], [xtrue[2]], marker=true, color=:blue, label=\"xtrue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gauss-Newton approach to this optimization is to approximate $f$ by\n",
    "a first order Taylor expansion in order to obtain a proposed step: \n",
    "$$\n",
    "  p_k = \\operatorname{argmin}_p \\frac{1}{2} \\|f(x_k) + f'(x_k) p\\|^2\n",
    "      = -f'(x_k)^\\dagger f(x_k).\n",
    "$$\n",
    "Writing out the pseudo-inverse more explicitly, we have\n",
    "$$\\begin{aligned}\n",
    "  p_k\n",
    "  &= -[f'(x_k)^T f'(x_k)]^{-1} f'(x_k)^T f(x_k) \\\\\n",
    "  &= -[f'(x_k)^T f'(x_k)]^{-1} \\nabla \\phi(x_k).\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_gauss_newton(x0, f, J; nsteps=100, rtol=1e-8, monitor=(x, rnorm) -> nothing)\n",
    "    x = copy(x0)\n",
    "    for k = 1:nsteps\n",
    "        fx = f(x)\n",
    "        Jx = J(x)\n",
    "        ∇ϕx = Jx'*fx\n",
    "        monitor(x, norm(∇ϕx))\n",
    "        x[:] -= Jx\\fx\n",
    "        if norm(∇ϕx) < rtol\n",
    "            monitor(x, norm(J(x)'*f(x)))\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge in $nsteps iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist = []\n",
    "x = simple_gauss_newton([0.0; 0.0], frange, Jrange, monitor=(x, rnorm)->push!(rhist, rnorm))\n",
    "plot(rhist, yscale=:log10, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $f'(x_k)^T f'(x_k)$ is positive definite if $f'(x_k)$ is full\n",
    "rank; hence, the direction $p_k$ is always a descent direction provided\n",
    "$x_k$ is not a stationary point and $f'(x_k)$ is full rank. However, the\n",
    "Gauss-Newton step is *not* the same as the Newton step, since the\n",
    "Hessian of $\\phi$ is\n",
    "$$\n",
    "  H_{\\phi}(x) = f'(x)^T f'(x) + \\sum_{j=1}^m f_j(x) H_{f_j}(x).\n",
    "$$\n",
    "Thus, the Gauss-Newton iteration can be seen as a modified Newton in which we\n",
    "drop the inconvenient terms associated with second derivatives of the\n",
    "residual functions $f_j$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Hfrange(x)\n",
    "    H = zeros(length(x), length(x))\n",
    "    for k = 1:ntarget\n",
    "        rk = x-a[:,k]\n",
    "        ρk = norm(rk)\n",
    "        uk = rk/ρk\n",
    "        H[:,:] += uk*uk' + (1-ρrange[k]/ρk)*(I-uk*uk')\n",
    "    end\n",
    "    return H\n",
    "end\n",
    "\n",
    "function simple_newton(x0, ∇ϕ, Hϕ; nsteps=100, rtol=1e-8, monitor=(x, rnorm) -> nothing)\n",
    "    x = copy(x0)\n",
    "    for k = 1:nsteps\n",
    "        ∇ϕx = ∇ϕ(x)\n",
    "        monitor(x, norm(∇ϕx))\n",
    "        x[:] -= Hϕ(x)\\∇ϕx\n",
    "        if norm(∇ϕx) < rtol\n",
    "            monitor(x, norm(∇ϕ(x)))\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge after $nsteps steps\")\n",
    "end\n",
    "\n",
    "rhist_newton = []\n",
    "x = simple_newton([0.0; 0.0], (x) -> Jrange(x)'*frange(x), Hfrange, \n",
    "                  monitor=(x, rnorm)->push!(rhist_newton, rnorm))\n",
    "plot(rhist_newton, yscale=:log10, label=\"Newton residuals\")\n",
    "plot!(rhist, yscale=:log10, label=\"Gauss-Newton residuals\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming $f'$ is Lipschitz with constant $L$, an error analysis about a\n",
    "minimizer $x_*$ yields\n",
    "$$\n",
    "  \\|e_{k+1}\\| ~ \\leq ~ L \\|f'(x_*)^\\dagger\\|^2 \\|f(x_*)\\| \\|e_k\\| + O(\\|e_k\\|^2).\n",
    "$$\n",
    "Thus, if the optimal residual norm $\\|f(x_*)\\|$ is small, then from good\n",
    "initial guesses, Gauss-Newton converges nearly quadratically (though the\n",
    "linear term will eventually dominate). On the other had, if $\\|f(x_*)\\|$\n",
    "is larger than $\\|f'(x_*)^\\dagger\\|$, then the iteration may not even be\n",
    "locally convergent unless we apply some type of globalization strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "Consider the following code to produce a noisy signal $\\cos(k\\theta + \\psi)$.  Use a Gauss-Newton\n",
    "iteration to try to recover $k$ and $\\psi$ from the signal.  Plot the convergence of the nonlinear least\n",
    "squares gradient to zero; also show the fit between the recovered $\\cos(\\hat{k} \\theta + \\hat{\\psi})$\n",
    "and the noisy signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kψref = [1.23; 0.56]\n",
    "θs = range(0.0, 2.0*Float64(π), length=100)\n",
    "fsignal(kψ) = cos.(kψ[1]*θs .+ kψ[2])\n",
    "signal = fsignal(kψref) + 1e-1*randn(100)\n",
    "plot(θs, fsignal(kψref), linewidth=2, legend=false)\n",
    "plot!(θs, signal, marker=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "J(kψ) = [-θs .* sin.(kψ[1]*θs .+ kψ[2])  -sin.(kψ[1]*θs .+ kψ[2])]\n",
    "f(kψ) = fsignal(kψ) - signal\n",
    "\n",
    "kψ = [1.0; 0.0]\n",
    "rhist = []\n",
    "for step = 1:20\n",
    "    Jkψ = J(kψ)\n",
    "    fkψ = f(kψ)\n",
    "    push!(rhist, norm(Jkψ'*fkψ))\n",
    "    kψ[:] -= (Jkψ'*Jkψ) \\ (Jkψ'*fkψ)\n",
    "end\n",
    "\n",
    "println(\"Reconstructed: $kψ ≈ $kψref\")\n",
    "p1 = plot(rhist, yscale=:log10, legend=false)\n",
    "p2 = plot(θs, fsignal(kψ))\n",
    "plot!(θs, signal, marker=true)\n",
    "plot(p1, p2, layout=@layout[a b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization and Levenberg-Marquardt\n",
    "\n",
    "While we can certainly apply line search methods to globalize\n",
    "Gauss-Newton iteration, an alternate proposal due to Levenberg and\n",
    "Marquardt is solve a *regularized* least squares problem to compute\n",
    "the step; that is, \n",
    "$$\n",
    "  p_k = \\operatorname{argmin}_p\n",
    "    \\frac{1}{2} \\|f(x_k) + f'(x_k) p\\|^2 +\n",
    "    \\frac{\\mu}{2} \\|Dp\\|^2.\n",
    "$$\n",
    "The scaling matrix $D$ may be an identity matrix (per Levenberg), \n",
    "or we may choose $D^2 = \\operatorname{diag}(f'(x_k)^T f'(x_k))$ \n",
    "(as suggested by Marquardt).\n",
    "\n",
    "For $\\lambda = 0$, the Levenberg-Marquardt step is the same as a\n",
    "Gauss-Newton step. As $\\lambda$ becomes large, though, we have the\n",
    "(scaled) gradient step\n",
    "$$\n",
    "  p_k = -\\frac{1}{\\mu} D^{-2} f(x_k) + O(\\mu^{-2}).\n",
    "$$\n",
    "Unlike Gauss-Newton with line search, changing the parameter $\\mu$ affects\n",
    "not only the distance we move, but also the direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_lm(x0, f, J, μ; nsteps=100, rtol=1e-8, monitor=(x, rnorm) -> nothing)\n",
    "    x = copy(x0)\n",
    "    for k = 1:nsteps\n",
    "        fx = f(x)\n",
    "        Jx = J(x)\n",
    "        ∇ϕx = Jx'*fx\n",
    "        monitor(x, norm(∇ϕx))\n",
    "        Hgn = Jx'*Jx\n",
    "        x[:] -= (Hgn + μ*I)\\∇ϕx\n",
    "        if norm(∇ϕx) < rtol\n",
    "            monitor(x, norm(J(x)'*f(x)))\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge in $nsteps iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist_lm0 = []\n",
    "x = simple_lm([0.0; 0.0], frange, Jrange, 1e-2, monitor=(x, rnorm)->push!(rhist_lm0, rnorm))\n",
    "plot(rhist_lm0, yscale=:log10, label=\"Levenberg-Marguardt\")\n",
    "plot!(rhist, yscale=:log10, label=\"Gauss-Newton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get both ensure global convergence (under sufficient\n",
    "hypotheses on $f$, as usual) and to ensure that convergence is not too\n",
    "slow, a variety of methods have been proposed that adjust $\\lambda$\n",
    "dynamically. To judge whether $\\mu$ has been chosen too aggressively\n",
    "or conservatively, we monitor the *gain ratio*, or the ratio of\n",
    "actual reduction in the objective to the reduction predicted by the\n",
    "(Gauss-Newton) model: \n",
    "$$\n",
    "  \\rho =\n",
    "  \\frac{\\|f(x_k)\\|^2-\\|f(x_k+p_k)\\|^2}\n",
    "       {\\|f(x_k)\\|^2 - \\|f(x_k)+f'(x_k)p_k\\|^2}.\n",
    "$$\n",
    "If the step decreases the function value enough ($\\rho$ is sufficiently positive),\n",
    "then we accept the step; otherwise, we reject it. For the next step (or the next\n",
    "attempt), we may increase or decrease the damping parameter $\\mu$\n",
    "depending on whether $\\rho$ is close to one or far from one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function levenberg_marquardt(x0, f, J; nsteps=100, rtol=1e-8, τ=1e-3, monitor=(x, rnorm, μ))\n",
    "    \n",
    "    # Evaluate everything at the initial point\n",
    "    x = copy(x0)\n",
    "    Jx = J(x)\n",
    "    fx = f(x)\n",
    "    Hx = Jx'*Jx\n",
    "\n",
    "    μ = τ * maximum(diag(Hx))  # Default damping parameter\n",
    "    ν = 2.0                    # Step re-scaling parameter (default value)\n",
    "    \n",
    "    for k = 1:nsteps\n",
    "        \n",
    "        # Check for convergence\n",
    "        g = Jx'*fx\n",
    "        rnorm = norm(Jx'*fx)\n",
    "        monitor(x, rnorm, μ)\n",
    "        if rnorm < rtol\n",
    "            return x\n",
    "        end\n",
    "        \n",
    "        # Compute a proposed step and re-evaluate residual vector\n",
    "        p = (Hx + μ*I)\\(-g)\n",
    "        xnew = x + p\n",
    "        fxnew = f(xnew)\n",
    "        \n",
    "        # Compute the gain ratio\n",
    "        ρ = (norm(fx)^2 - norm(fxnew)^2) / (norm(fx)^2 - norm(fx+Jx*p)^2)\n",
    "        \n",
    "        if ρ > 0  # Success!\n",
    "            \n",
    "            # Accept new point\n",
    "            x = xnew\n",
    "            fx = fxnew\n",
    "            Jx = J(x)\n",
    "            Hx = Jx'*Jx\n",
    "            \n",
    "            # Reset re-scaling parameter, update damping\n",
    "            μ *= max(1.0/3.0, 1.0-2.0*(ρ-1.0)^3)\n",
    "            ν = 2.0\n",
    "        \n",
    "        else\n",
    "                \n",
    "            # Rescale damping\n",
    "            μ *= ν\n",
    "            ν *= 2.0\n",
    "\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge in $nsteps iterations\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist_lm1 = []\n",
    "μhist_lm1 = []\n",
    "function monitor_rμ(x, rnorm, μ)\n",
    "    push!(rhist_lm1, rnorm)\n",
    "    push!(μhist_lm1, rnorm)\n",
    "end\n",
    "\n",
    "x = levenberg_marquardt([0.0; 0.0], frange, Jrange, rtol=1e-8, monitor=monitor_rμ)\n",
    "p1 = plot(rhist_lm1, yscale=:log10, label=\"Levenberg-Marguardt\")\n",
    "plot!(rhist_lm0, yscale=:log10, label=\"Non-adaptive LM\")\n",
    "p2 = plot(μhist_lm1, yscale=:log10, legend=false)\n",
    "plot(p1, p2, layout=@layout[a ; b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consider constraints\n",
    "\n",
    "There is another way to think of the Levenberg-Marquardt step. Consider\n",
    "the minimization problem\n",
    "$$\n",
    "  p_k = \\operatorname{argmin}_p \\frac{1}{2} \\|f(x) + f'(x)p \\|^2 \\mbox{ s.t. }\n",
    "  \\|Dp\\| \\leq \\Delta.\n",
    "$$\n",
    "There are two possible cases in this problem:\n",
    "\n",
    "1.  $\\|f'(x_k)^\\dagger f(x)\\| < \\Delta$, and the solution is the\n",
    "    Gauss-Newton step\n",
    "\n",
    "2.  Otherwise the Gauss-Newton step is too big, and we have to enforce\n",
    "    the constraint $\\|Dp\\| = \\Delta$. For convenience, we rewrite this\n",
    "    constraint as $(\\|Dp\\|^2-\\Delta^2)/2 = 0$.\n",
    "\n",
    "We define the Langrangian for the optimization problem to be \n",
    "$$\n",
    "  L(p,\\lambda) =\n",
    "    \\frac{1}{2} \\|f(x_k)+f'(x_k) p\\|^2 +\n",
    "    \\frac{\\lambda}{2} \\left( \\|Dp\\|^2-\\Delta^2 \\right).\n",
    "$$\n",
    "The solution to the constrained optimization problem satisfies the critical point\n",
    "equation $\\partial L/\\partial p = 0$ and $\\partial L/\\partial \\lambda = 0$. The equation\n",
    "$\\partial L/\\partial p = 0$ is the same as the Tikhonov-regularized\n",
    "least squares problem with regularization parameter $\\lambda$. Whether\n",
    "$\\lambda$ is treated as a regularization parameter or a multiplier that\n",
    "enforces a constraint is thus simply a matter of perspective. Hence, we\n",
    "can consider the Levenberg-Marquardt method as minimizing the model\n",
    "$\\|f(x_k) + f(x_k) p\\|$ subject to the constraint $\\|Dp\\| \\leq \\Delta$,\n",
    "where a larger or smaller value of $\\lambda$ corresponds to a smaller or\n",
    "larger value of $\\Delta$. We think of the region $\\|Dp\\| \\leq \\Delta$ as\n",
    "the region where the Gauss-Newton model provides good guidance for\n",
    "optimization; that is, it is a region where we trust the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trust regions\n",
    "\n",
    "A *trust region* method for mininizing $\\phi$ involves a *model*\n",
    "$\\mu(p)$ that is supposed to approximate the decrease\n",
    "$\\phi(x_k+p)-\\phi(x_k)$ associated with taking a step $p$; and a \n",
    "*trust region*, often chosen to be a sphere $\\|p\\| \\leq \\Delta$, where we\n",
    "believe the model to provide reasonable predictions.\n",
    "\n",
    "The simplest model $\\mu(p)$ is linear, but the more interesting (and common)\n",
    "case involves a quadratic model\n",
    "$$\n",
    "  \\mu(p) = g^T p + \\frac{1}{2} p^T H p.\n",
    "$$\n",
    "Minimizing a quadratic $\\mu(p)$ subject to the constraint $\\|\\mu(p)\\| \\leq \\Delta$\n",
    "is *not* easy.  We turn to this *trust region subproblem* next.\n",
    "\n",
    "Compared to a line search strategy, trust region methods have the\n",
    "advantage that we adapt not just the step length but also the direction\n",
    "of the search. Consequently, trust region methods often exhibit more\n",
    "robust convergence, though both line search and trust region approaches\n",
    "exhibit good global convergence properties, and both approaches lead to\n",
    "eventual superlinear convergence when paired with a Newton model (i.e. a\n",
    "quadratic approximation centered at $x_k$) or a quasi-Newton method such\n",
    "as BFGS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The trust region subproblem\n",
    "\n",
    "The problem\n",
    "$$\n",
    "  \\mbox{minimize } g^T p + \\frac{1}{2} p^T H p \\mbox{ s.t. } \\|p\\| \\leq \\Delta\n",
    "$$\n",
    "is the *trust region subproblem*.  Sometimes people use the more general constraint\n",
    "$$\n",
    "  p^T M p \\leq \\Delta^2\n",
    "$$\n",
    "for some positive definite $M$, but we will stick to the usual 2-norm.  There are\n",
    "two possible solutions:\n",
    "\n",
    "1.  If $H$ is positive definite and $\\|H^{-1} g\\| \\leq \\Delta$, then the solution is\n",
    "    $p = -H^{-1} g$.  This is the *interior* case.\n",
    "2.  If $H$ is not positive definite or $\\|H^{-1} g\\| > \\Delta$, then the solution iis\n",
    "    $p = -(H+\\lambda I)^{-1} g$ for some $\\lambda > 0$ such that $\\|p\\| = \\Delta$.\n",
    "    At the appropriate $\\lambda$, we have $H+\\lambda I$ is positive semi-definite.\n",
    "    This is the *boundary case*\n",
    "\n",
    "Most of the effort is spent on the boundary case, which itself has two subcases:\n",
    "\n",
    "1.  If $H + \\lambda I$ is positive definite, then there is a unique solution to\n",
    "    the trust region subproblem.\n",
    "2.  If $H + \\lambda I$ is singular, then there are multiple solutions to the trust\n",
    "    region subproblem, and we seen the problem with minimum norm.\n",
    "\n",
    "The case when $H + \\lambda I$ is singular (i.e. $-\\lambda$ is an eigenvalue of $H$)\n",
    "is consistently known as the *hard case* in the literature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exact solves\n",
    "\n",
    "The standard solver for the trust-region subproblem is due to [Moré and Sorensen](https://doi.org/10.1137/0904038),\n",
    "and involves a safeguarded Newton iteration for finding the relevant $\\lambda$, with careful treatment of the hard\n",
    "case.  A number of authors have also adapted this approach to the large sparse case.\n",
    "However, I am particularly fond of a method proposed by\n",
    "[Gander, Golub, and Von Matt](https://doi.org/10.1007/978-3-642-75536-1_57)\n",
    "that recasts the trust-region subproblem in terms of an eigenvalue problem.\n",
    "That paper concluded that the eigenvalue formulation was numerically inferior to the Moré-Sorensen approach,\n",
    "but a 2017 paper of [Adachi, Iwata, Nakatsukasa, and Takeda](https://doi.org/10.1137/16M1058200)\n",
    "concluded that this was in part because the eigensolvers available in 1989 were not as good as the solvers\n",
    "currently available.  The Adachi et al paper provides a nice discussion of the formulation, including\n",
    "the hard case, which results in a mercifully brief code (which you are nonetheless not required to digest).\n",
    "One of the nice things about this formulation is that it adapts naturally to large-scale problems where\n",
    "$H$ is sparse or data sparse, though we will only worry about the dense case in our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function solve_tr(g, H, Δ)\n",
    "    n = length(g)\n",
    "\n",
    "    # Check interior case\n",
    "    try\n",
    "        F = cholesky(H)\n",
    "        p = -(F\\g)\n",
    "        if norm(p) <= Δ\n",
    "            return p, false\n",
    "        end\n",
    "    catch e\n",
    "        # Hit this case if Cholesky errors (not pos def)\n",
    "    end    \n",
    "\n",
    "    # Compute the relevant eigensolve\n",
    "    w = g/Δ\n",
    "    M = [H    -I ;\n",
    "         -w*w' H ]\n",
    "    λs, V = eigen(M)\n",
    "    \n",
    "    # The right most eigenvalue (always sorted to the end in Julia) is real,\n",
    "    # and corresponds to the desired λ\n",
    "    λ = -real(λs[1])\n",
    "    v = real(V[:,1])\n",
    "    y2 = v[1:n]\n",
    "    y1 = v[n+1:end]\n",
    "    \n",
    "    # Check if we are in the hard case (to some tolerance)\n",
    "    gap = real(λs[2])-real(λs[1])\n",
    "    if norm(y1) <= 1e-8/sqrt(gap)\n",
    "        # Hard case -- we punt a little and assume only one null vector\n",
    "        #  Compute min-norm solution plus a multiple of the null vector.\n",
    "        v = y2/norm(y2)\n",
    "        q = -(H+norm(H)/n^2*v*v')\\g\n",
    "        return q + v*sqrt(Δ^2-q'*q), true\n",
    "    else\n",
    "        # Standard case -- extract solution from eigenvector\n",
    "        return -sign(g'*y2) * Δ * y1/norm(y1), true\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful picture is a plot of the step for various $\\Delta$ values for a sample quadratic model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the TR solve\n",
    "\n",
    "g = [2.0; 1.0]\n",
    "H = [2.0  0.0;\n",
    "     0.0  0.5]\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "θs = range(0, Float64(2*π), length=100)\n",
    "p1 = plot(xx, xx, (x,y) -> g'*[x; y] + ([x; y]'*H*[x; y])/2, st=:contour, legend=false)\n",
    "\n",
    "for Δ in range(0.1, 2.5, length=20)\n",
    "    p, hit_boundary = solve_tr(g, H, Δ)\n",
    "    plot!(Δ*cos.(θs), Δ*sin.(θs), linewidth=2, linecolor=:black)\n",
    "    plot!([p[1]], [p[2]], marker=true, markercolor=(hit_boundary ? :blue : :white))\n",
    "end\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Rerun the demo above for $$H = \\begin{bmatrix} 2 & 0 \\\\ 0 & -0.5 \\end{bmatrix}.$$\n",
    "    Does the path eventually reach $$-H^{-1} g = \\begin{bmatrix} -1 \\\\ 2 \\end{bmatrix}?$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*:  The modified matrix $H$ is not positive definite, so $-H^{-1} g$ is a saddle of the quadratic,\n",
    "not a minimum.  The model $g^T p + p^T H p/2$ has no global maximum or minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the TR solve\n",
    "\n",
    "g = [2.0; 1.0]\n",
    "H = [2.0  0.0;\n",
    "     0.0  -0.5]\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "θs = range(0, Float64(2*π), length=100)\n",
    "p1 = plot(xx, xx, (x,y) -> g'*[x; y] + ([x; y]'*H*[x; y])/2, st=:contour, legend=false)\n",
    "\n",
    "for Δ in range(0.1, 2.5, length=20)\n",
    "    p, hit_boundary = solve_tr(g, H, Δ)\n",
    "    plot!(Δ*cos.(θs), Δ*sin.(θs), linewidth=2, linecolor=:black)\n",
    "    plot!([p[1]], [p[2]], marker=true, markercolor=(hit_boundary ? :blue : :white))\n",
    "end\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inexact solves\n",
    "\n",
    "One of the main difficulties with the trust region approach is solving a\n",
    "constrained quadratic optimization as a subproblem.  As with line search, \n",
    "the thinking goes, the cost of doing an exact search is probably not \n",
    "worthwhile — we would rather get a good-enough approximate solution and move on.\n",
    "\n",
    "A popular inexact search approach is the *dog leg* method. The\n",
    "idea of the dog leg method is to approximate the shape of the curve\n",
    "$$p(\\Delta) = \\operatorname{argmin}_p \\mu(p) \\mbox{ s.t. } \\|p\\| \\leq \\Delta$$\n",
    "based on the observation that\n",
    "\n",
    "-   $p(0) = 0$.\n",
    "\n",
    "-   $p'(0) \\propto -\\nabla \\phi(x_k)$.\n",
    "\n",
    "-   For large $\\Delta$, $p(\\Delta) = p_{\\infty}$ is the unconstrained\n",
    "    minimizer of $\\mu$.\n",
    "\n",
    "We thus approximate the $\\rho(\\Delta)$ curve by a piecewise linear curve\n",
    "with\n",
    "\n",
    "-   A line segment from $0$ to $-\\alpha \\nabla \\phi(x_k)$ where\n",
    "    $\\mu(-\\alpha \\nabla \\phi(x_k))$ is mimimized.\n",
    "\n",
    "-   Another line segment from $-\\alpha \\nabla \\phi(x_k)$ to\n",
    "    $p_{\\infty}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dogleg_tr(g, H, Δ)\n",
    "    n = length(g)\n",
    "\n",
    "    # Positive definite case (Cholesky succeeds)\n",
    "    try\n",
    "        F = cholesky(H)\n",
    "        p∞ = -(F\\g)\n",
    "        \n",
    "        # Check for interior case\n",
    "        if norm(p∞) <= Δ\n",
    "            return p∞, false\n",
    "        end\n",
    "        \n",
    "        # Compute a Cauchy step (first part of the dog leg)\n",
    "        τ = (g'*g)/(g'*H*g)\n",
    "        pc = -τ*g\n",
    "        if norm(pc) >= Δ\n",
    "            return (Δ/norm(pc)) * pc, true\n",
    "        end\n",
    "        \n",
    "        # If the Cauchy step is interior, do the dog leg:\n",
    "        #   p = pc + η*(p∞-pc) s.t. norm(p) = Δ\n",
    "        # This corresponds to solving the quadratic\n",
    "        #   pc'*pc + 2*η*pc'*(p∞-pc) + η^2*(p∞-pc)'*(p∞-pc) = Δ^2\n",
    "        #\n",
    "        a = (p∞-pc)'*(p∞-pc)\n",
    "        b = pc'*(p∞-pc)\n",
    "        c = pc'*pc - Δ^2\n",
    "        η = (-b + sqrt(b^2 - a*c))/a\n",
    "        return pc + η*(p∞-pc), true\n",
    "\n",
    "    catch e\n",
    "        # Hit this case if Cholesky errors (not pos def)\n",
    "    end    \n",
    "\n",
    "    # Compute a Cauchy step\n",
    "    τ = (g'*g)/(g'*H*g)\n",
    "    pc = -τ*g\n",
    "    if τ < 0.0 || norm(pc) >= Δ\n",
    "        return (Δ/norm(pc)) * pc, true\n",
    "    end\n",
    "    return -(Δ/norm(g)) * g, true\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A plot illustrates what happens with the dogleg path as a function of $\\Delta$, compared to the true\n",
    "trust region solution path: the dogleg is a piecewise linear approximation to the true path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare dogleg vs exact TR solve\n",
    "\n",
    "g = [2.0; 1.0]\n",
    "H = [2.0  0.0;\n",
    "     0.0  0.5]\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "θs = range(0, Float64(2*π), length=100)\n",
    "p1 = plot(xx, xx, (x,y) -> g'*[x; y] + ([x; y]'*H*[x; y])/2, st=:contour, legend=false)\n",
    "\n",
    "for Δ in range(0.1, 2.5, length=20)\n",
    "    p, hit_boundary = solve_tr(g, H, Δ)\n",
    "    pd, hit_boundaryd = dogleg_tr(g, H, Δ)\n",
    "    plot!(Δ*cos.(θs), Δ*sin.(θs), linewidth=2, linecolor=:black)\n",
    "    plot!([p[1]], [p[2]], marker=true, markercolor=(hit_boundary ? :blue : :white))\n",
    "    plot!([pd[1]], [pd[2]], marker=true, markercolor=(hit_boundaryd ? :red : :white))\n",
    "end\n",
    "p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A related approach is *two-dimensional subspace minimization*, which\n",
    "involves a constrained miminization over the two-dimensional subspace\n",
    "spanned by $-\\nabla \\phi(x_k)$ and $p_{\\infty}$.\n",
    "\n",
    "The *Steighaug* method combines the trust region approach with a\n",
    "(linear) conjugate gradient solve on the quadratic model problem. The\n",
    "idea is to trace out a polygonal path (as in the dog leg method)\n",
    "connecting the CG iterates, until that path intersects the trust region\n",
    "boundary. If the (approximate) Hessian used by the model is indefinite,\n",
    "CG runs until it discovers the indefiniteness, then plots a path toward\n",
    "where the model descends to $-\\infty$. There are more recent variants\n",
    "which combine Newton, trust regions, and Krylov subspaces in various\n",
    "clever ways; other than mentioning that they exist, though, we leave\n",
    "this topic for the interested student to pursue in her copious free\n",
    "time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapting the trust region\n",
    "\n",
    "At each step of the method, we (approximately) minimize the model within the trust region to\n",
    "get a proposed step $p$, then check the gain ratio associated with taking that step:\n",
    "$$\n",
    "  \\rho_k = \\frac{\\phi(x_k)-\\phi(x_k+p_k)}{\\mu(0)-\\mu(p_k)}.\n",
    "$$\n",
    "Depending on whether the gain ratio, we adjust $\\Delta$; a strategy proposed in\n",
    "Nocedal and Wright is:\n",
    "\n",
    "-   If $\\rho_k < 1/4$, we were too aggressive; set $\\Delta_{k+1} =\n",
    "      \\Delta_k/4$.\n",
    "\n",
    "-   If $\\rho_k > 3/4$ and $\\|p_k\\| = \\Delta_k$, we were too\n",
    "    conservative; set $\\Delta_{k+1} = \\min(2\\Delta_k, \\Delta_{\\max})$.\n",
    "\n",
    "-   Otherwise, leave $\\Delta_{k+1} = \\Delta_k$.\n",
    "\n",
    "We also use the gain ratio to decide whether to accept or reject the\n",
    "step. For $\\rho_k > \\eta$ for a fixed $\\eta \\in [0,1/4)$, we accept\n",
    "($x_{k+1} = x_k+p$); otherwise we reject ($x_{k+1} = x_k$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function tr_newton(x0, ϕ, ∇ϕ, Hϕ; nsteps=100, rtol=1e-6, Δmax=Inf, monitor=(x, rnorm, Δ)->nothing)\n",
    "    \n",
    "    # Compute an intial step and try trusting it\n",
    "    x = copy(x0)\n",
    "    ϕx = ϕ(x)\n",
    "    gx = ∇ϕ(x)\n",
    "    Hx = Hϕ(x)\n",
    "    p = -Hx\\gx\n",
    "    Δ = 1.2 * norm(p)^2\n",
    "    hit_constraint = false\n",
    "    \n",
    "    for k = 1:nsteps\n",
    "\n",
    "        # Compute gain ratio for new point and decide to accept or reject\n",
    "        xnew = x + p\n",
    "        ϕnew = ϕ(xnew)\n",
    "        μdiff = -( gx'*p + (p'*Hx*p)/2 )\n",
    "        ρ = (ϕx - ϕnew)/μdiff\n",
    "        \n",
    "        # Adjust radius\n",
    "        if ρ < 0.25\n",
    "            Δ /= 4.0\n",
    "        elseif ρ > 0.75 && hit_constraint\n",
    "            Δ = min(2*Δ, Δmax)\n",
    "        end\n",
    "        \n",
    "        # Accept if enough gain (and check convergence)\n",
    "        if ρ > 0.1\n",
    "            x[:] = xnew\n",
    "            ϕx = ϕnew\n",
    "            gx = ∇ϕ(x)\n",
    "            monitor(x, norm(gx), Δ)\n",
    "            if norm(gx) < rtol\n",
    "                return x\n",
    "            end\n",
    "            Hx = Hϕ(x)\n",
    "        end\n",
    "\n",
    "        # Otherwise, solve the trust region subproblem for new step\n",
    "        p, hit_constraint = solve_tr(gx, Hx, Δ)\n",
    "\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An illustrative computation\n",
    "\n",
    "It is always useful to see how these things work on a problem we've already\n",
    "looked at in another context.  Let's consider as an example the Rosenbrock\n",
    "banana function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frosen(x) = 100*(x[2]-x[1]^2)^2 + (1-x[1])^2\n",
    "grosen(x) = [400*x[1]*(x[1]^2-x[2]) + 2*(x[1]-1);\n",
    "             200*(x[2]-x[1]^2)]\n",
    "Hrosen(x) = [ 400*(3*x[1]^2-x[2])+2  -400*x[1] ;\n",
    "             -400*x[1]                   200   ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist = []\n",
    "Δhist = []\n",
    "xhist = []\n",
    "yhist = []\n",
    "ϕhist = []\n",
    "function monitor_rΔ(x, rnorm, Δ)\n",
    "    push!(xhist, x[1])\n",
    "    push!(yhist, x[2])\n",
    "    push!(rhist, rnorm)\n",
    "    push!(Δhist, Δ)\n",
    "    push!(ϕhist, frosen(x))\n",
    "end\n",
    "\n",
    "x = tr_newton([-1.9; 2.0], frosen, grosen, Hrosen, Δmax=1.0, monitor=monitor_rΔ)\n",
    "p1 = plot(rhist, yscale=:log10, label=\"normg\")\n",
    "p2 = plot(Δhist, yscale=:log10, label=\"Delta\")\n",
    "plot!([norm([xhist[i+1]-xhist[i]; yhist[i+1]-yhist[i]]) for i in 1:length(xhist)-1], label=\"step\")\n",
    "p3 = plot(ϕhist, label=\"Values\")\n",
    "plot(p3, p2, layout=@layout [a; b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(p1, p2, layout=@layout [a; b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-3, 3, length=100)\n",
    "plot(xx, xx, (x,y) -> frosen([x y]), st=:contour)\n",
    "plot!(xhist, yhist, linecolor=:black, linewidth=2, legend=false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
