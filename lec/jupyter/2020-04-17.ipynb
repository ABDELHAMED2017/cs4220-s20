{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-17\n",
    "\n",
    "## Life beyond Newton\n",
    "\n",
    "Newton’s method has many attractive properties, but Newton steps may not\n",
    "be cheap. At each step, we need to:\n",
    "\n",
    "-   Form the function $f$ *and* the Jacobian. This involves not only\n",
    "    computational work, but also analytical work – someone needs to\n",
    "    figure out those derivatives!\n",
    "\n",
    "-   Solve a linear system with the Jacobian. This is no easier than any\n",
    "    other linear solve problem! Indeed, it may be rather expensive for\n",
    "    large systems, and factorization costs cannot (in general) be\n",
    "    amortized across Newton steps.\n",
    "\n",
    "The Jacobian (or the Hessian if we are looking at optimization problems)\n",
    "is the main source of difficulty. Now we consider several iterations\n",
    "that deal with this difficulty in one way or the other.\n",
    "\n",
    "## A running example, redux\n",
    "\n",
    "It is always helpful to illustrate methods with an actual example.\n",
    "We will continue to work with the example from last time of a nonlinear\n",
    "reaction-diffusion problem:\n",
    "$$\n",
    "  f_i(v) \\equiv \\frac{v_{i-1}-2v_i+v_{i+1}}{h^2} + \\exp(v_i) = 0.\n",
    "$$\n",
    "with $h = (N+1)^{-1}$ and $v_0 = v_{N+1} = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function ϕ_autocatalytic(v)\n",
    "    N = length(v)\n",
    "    C = 0.5*(N+1)^2\n",
    "    ϕ = C*v[1]^2 - exp(v[1])\n",
    "    for j = 1:N-1\n",
    "        ϕ += C*(v[j]-v[j+1])^2 - exp(v[j])\n",
    "    end\n",
    "    ϕ += C*v[N]^2 - exp(v[N])\n",
    "    return ϕ\n",
    "end\n",
    "\n",
    "function autocatalytic(v)\n",
    "    N = length(v)\n",
    "    fv        = exp.(v)\n",
    "    fv        -= 2*(N+1)^2*v\n",
    "    fv[1:N-1] += (N+1)^2*v[2:N  ]\n",
    "    fv[2:N  ] += (N+1)^2*v[1:N-1]\n",
    "    fv\n",
    "end\n",
    "\n",
    "function Jautocatalytic(v)\n",
    "    N = length(v)\n",
    "    SymTridiagonal(exp.(v) .- 2*(N+1)^2, (N+1)^2 * ones(N-1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the last lecture, we used an initial guess of the form\n",
    "$$\n",
    "  v_i^0 = \\alpha x_i \\left( 1-x_i \\right) = \\alpha q_i, \\quad x_i \\equiv \\frac{i}{N+1}\n",
    "$$\n",
    "and then tried various values of $\\alpha$.  An alternative is to characterize the solution\n",
    "as a stationary point of the objective $\\phi$ and try to find $\\alpha$ such that $\\phi(\\alpha q)$\n",
    "is minimum (or at least stationary).  It is helpful to get a picture -- and an estimate -- by first doing a plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100\n",
    "xx = range(1, N, length=N)/(N+1)\n",
    "q = xx.*(1.0 .- xx)\n",
    "\n",
    "αs = range(0, 20, length=1001)\n",
    "ϕs = [ϕ_autocatalytic(α*q) for α in αs]\n",
    "\n",
    "println(\"Optimal α (stable eq):   $(αs[argmin(ϕs)])\")\n",
    "println(\"Optimal α (unstable eq): $(αs[argmax(ϕs)])\")\n",
    "\n",
    "plot(αs, ϕs, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Write a one-dimensional Newton iteration for finding critical points of $\\phi(\\alpha q)$.\n",
    "    Use the initial guesses of $0$ and $15$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Newton and almost-Newton analysis (optional)\n",
    "\n",
    "In these notes, we will be somewhat careful about the analysis, but in\n",
    "general you are *not* responsible for remembering this level of\n",
    "detail. We will try to highlight the points that are important in\n",
    "practice for understanding when solvers might run into trouble, and why.\n",
    "\n",
    "A common theme in the analysis of “almost Newton” iterations is that we\n",
    "can build on Newton convergence.  We assume throughout\n",
    "that $f$ is $C^1$ and the Jacobian is Lipschitz with constant $M$.\n",
    "To simplify life, we will also assume that $\\|f'(x)^{-1}\\|$ is bounded\n",
    "in some neighborhood of a desired $x^*$ such that $f(x^*) = 0$.\n",
    "Consider what happens when we subtract the equation defining \n",
    "the Newton step from a Taylor expansion *with remainder* of $f(x^*)$\n",
    "centered at $f(x)$:\n",
    "$$\\begin{aligned}\n",
    "  f(x) + f'(x) &p(x) = 0 \\\\\n",
    "  -[f(x) + f'(x) &(x^*-x) + R(x) = 0] \\\\\\hline\n",
    "  f'(x) &[p(x) - (x^*-x)] - R(x) = 0\n",
    "\\end{aligned}$$\n",
    "or\n",
    "$$\n",
    "  p(x) = -(x-x^*) + f'(x)^{-1} R(x) = -(x-x^*) + d(x)\n",
    "$$\n",
    "Under the bounded inverse hypothesis and Lipschitz boundedness of $f'$,\n",
    "we know\n",
    "$$\n",
    "  \\|x + p(x) - x^*\\| = \\|d(x)\\| \\leq \\frac{BM}{2} \\|x-x^*\\|^2,\n",
    "$$\n",
    "and so the iteration $x \\mapsto x + p(x)$ converges quadratically from\n",
    "starting points near enough $x^*$.  Moreover, a sufficient condition for\n",
    "convergence is that the initial error is less than $2/(BM)$. \n",
    "This differs from our earlier bound of $2/(3BM)$ only because we assumed\n",
    "a uniform bound on the inverse of the Jacobian in the relevant region rather\n",
    "than assuming a bound at the solution and using the Lipschitz behavior to\n",
    "get everything else.\n",
    "\n",
    "Now suppose we have an iteration \n",
    "$$\n",
    "  x^{k+1} = x^k + \\hat{p}^k\n",
    "$$ \n",
    "where $\\hat{p}^k$ is an approximation to the Newton step $p(x^k)$. Subtracting\n",
    "$x^*$ from both sides and adding $p(x^k)-p(x^k)$ to the right side gives\n",
    "$$\n",
    "  e^{k+1} = e^k + p(x^k) - (p(x^k)-\\hat{p}^k),\n",
    "$$\n",
    "and taking norms gives\n",
    "$$\n",
    "  \\|e^{k+1}\\| \\leq \\frac{BM}{2} \\|e^k\\|^2 + \\|p(x^k)-\\hat{p}^k\\|.\n",
    "$$\n",
    "Therefore, we can think of our convergence analysis in two steps: \n",
    "we first analyze the error in the Newton iteration, \n",
    "then analyze how close our approximate Newton step is to a true Newton step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton iteration\n",
    "\n",
    "We ran Newton iteration for the autocatalytic problem last time, but let's run it again this\n",
    "time using an initial guess of $\\alpha = 0.5$.  Convergence from this initial guess is extremely rapid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = 0.5*q\n",
    "rhist = []\n",
    "for k = 1:10\n",
    "    fv = autocatalytic(v)\n",
    "    v -= Jautocatalytic(v)\\fv\n",
    "    push!(rhist, norm(fv))\n",
    "    if norm(fv) < 1e-9\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "rhist_newton = rhist\n",
    "plot(rhist_newton, yscale=:log10, label=\"Newton\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  What do you observe if you change the tolerance from $10^{-9}$ to $10^{-16}$?  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chord iteration\n",
    "\n",
    "The *chord iteration* is $$x^{k+1} = x^k - f'(x^0)^{-1} f(x^k).$$\n",
    "Written in this way, the method differs from Newton in only one\n",
    "character — but what a difference it makes! By re-using the Jacobian at\n",
    "$x^0$ for all steps, we degrade the progress per step, but each step\n",
    "becomes cheaper. In particular, we can benefit from re-using a\n",
    "factorization across several steps (though this is admittedly more of\n",
    "an issue when the matrix is not tridiagonal!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist = []\n",
    "v = 0.5*q\n",
    "J0F = ldlt(Jautocatalytic(v))  # Compute an LDL^T factorization of J\n",
    "for k = 1:10\n",
    "    fv = autocatalytic(v)\n",
    "    v -= J0F\\fv\n",
    "    push!(rhist, norm(fv))\n",
    "    if norm(fv) < 1e-9\n",
    "        break\n",
    "    end\n",
    "end\n",
    "\n",
    "rhist_chord = rhist\n",
    "plot(rhist_newton, yscale=:log10, label=\"Newton\")\n",
    "plot!(rhist_chord, label=\"Chord\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of the approximate Newton framework, the chord iteration\n",
    "involves errors $\\|E_k\\| = \\|f'(x^k)-f'(x^0)\\| \\leq M \\|e^0\\|$.\n",
    "Therefore, the iteration is guaranteed to converge for starting points\n",
    "such that $\\|e^0\\| < 1/(3BM)$, and the error in successive iterates is\n",
    "bounded by.\n",
    "$$\n",
    "  \\|e^{k+1}\\| \\leq \\left( \\frac{BM(\\|e^k\\| + \\|e^0\\|)}{1-BM\\|e^0\\|}\n",
    "  \\right) \\|e^k\\| = O(\\|e^0\\| \\|e^k\\|).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Run the chord iterations from several different starting values of $\\alpha$ to verify that the linear\n",
    "    rate of convergence depends on the initial error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shamanskii iteration\n",
    "\n",
    "The chord method involves using one approximate Jacobian forever. The\n",
    "Shamanskii method involves freezing the Jacobian for $m$ steps before\n",
    "getting a new Jacobian; that is, one step of Shaminskii looks like\n",
    "$$\\begin{aligned}\n",
    "  x^{k+1,0} & = x^k \\\\\n",
    "  x^{k+1,j+1} &= x^{k+1,j} - f'(x^k)^{-1} f(x^{k+1,j}) \\\\\n",
    "  x^{k+1} &= x^{k+1,m}.\n",
    "\\end{aligned}$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist = []\n",
    "v = 0.5*q\n",
    "JF = ldlt(Jautocatalytic(v))  # Compute an LDL^T factorization of J\n",
    "for k = 1:10\n",
    "    fv = autocatalytic(v)\n",
    "    v -= JF\\fv\n",
    "    push!(rhist, norm(fv))\n",
    "    if norm(fv) < 1e-9\n",
    "        break\n",
    "    end\n",
    "    if mod(k, 2) == 0\n",
    "        JF = ldlt(Jautocatalytic(v))\n",
    "    end\n",
    "end\n",
    "\n",
    "rhist_shamanskii = rhist\n",
    "plot(rhist_newton, yscale=:log10, label=\"Newton\")\n",
    "plot!(rhist_shamanskii, label=\"Shamanskii\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like the chord iteration,\n",
    "Shaminskii is guaranteed to converge for starting points such that\n",
    "$\\|e^0\\| < 1/(3BM)$. The error for each iteration (from $x^k$ to\n",
    "$x^{k+1}$, not from $x^{k+1,j}$ to $x^{k+1,j+1}$) satisfies\n",
    "$$\n",
    "  \\|e^{k+1}\\|\n",
    "  \\leq \\left( \\frac{2BM}{1-BM\\|e^k\\|} \\right) \\|e^{k}\\|^{m+1}\n",
    "  = O(\\|e^k\\|^{m+1}).\n",
    "$$\n",
    "Beyond the chord and Shaminskii iterations, the\n",
    "idea of re-using Jacobians occurs in several other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finite-difference Newton\n",
    "\n",
    "So far, we have assumed that we can compute the Jacobian if we want it.\n",
    "What if we just don’t want to do the calculus to compute Jacobians? A\n",
    "natural idea is to approximate each column of the Jacobian by a finite\n",
    "difference estimate:\n",
    "$$\n",
    "  f'(x^k) e_j \\approx \\frac{f(x^k+he_j)-f(x^k)}{h}.\n",
    "$$\n",
    "In general, the more analytic information that we have about the derivatives,\n",
    "the better off we are.  Even knowing only the sparsity pattern of the Jacobian\n",
    "gives us a lot of information.  In our example, changing $v_j$ affects \n",
    "$f_{j-1}$, $f_j$, and $f_{j+1}$, but not any other.  Hence, we don't actually\n",
    "need $N+1$ evaluations of $f$ to get the Jacobian; we can do it with four\n",
    "that are cleverly chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Jtridiagonal_fd(f, x, h)\n",
    "    N = length(x)\n",
    "\n",
    "    dd = zeros(N)   # Diagonal elements\n",
    "    dl = zeros(N-1) # Subdiagonal elements\n",
    "    du = zeros(N-1) # Superdiagonal elements\n",
    "    \n",
    "    fx = f(x)\n",
    "    xp = copy(x)\n",
    "    for j = 1:3\n",
    "        xp[:] = x\n",
    "        xp[j:3:N] .+= h\n",
    "        df = (f(xp)-fx)/h\n",
    "        for i = 1:N\n",
    "            if mod(i-j,3) == 0\n",
    "                dd[i] = df[i]\n",
    "            elseif mod(i-j,3) == 1 && i > 1\n",
    "                dl[i-1] = df[i]\n",
    "            elseif mod(i-j,3) == 2 && i < N\n",
    "                du[i] = df[i]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "\n",
    "    return Tridiagonal(dl, dd, du)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jfd = Jtridiagonal_fd(autocatalytic, 0.5*q, 1e-6)\n",
    "Jref = Jautocatalytic(0.5*q)\n",
    "norm(Jfd-Jref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Lipschitz bounds on $f'$ gives the error bound\n",
    "$$\n",
    "  \\left\\| f'(x^k) - \\frac{f(x^k+he_j)-f(x^k)}{h} \\right\\| \\leq Mh,\n",
    "$$\n",
    "and an approximation to $f'(x^k)$ based on finite difference approximation\n",
    "would have a two norm error of at most $\\|E^k\\| \\leq \\sqrt{n} M h$. The\n",
    "convergence is bounded by \n",
    "$$\n",
    "  \\|e^{k+1}\\| \\leq\n",
    "  \\left( \\frac{BM\\|e^k\\| + \\sqrt{n}BMh}{1-\\sqrt{n} BMh} \\right) \\|e^k\\| =\n",
    "  O(h\\|e^k\\|).\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Can you explain what is going on in the `Jtridiagonal_fd` code above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inexact Newton\n",
    "\n",
    "So far, we have considered approximations to the Newton step based on\n",
    "approximation of the Jacobian matrix. What if we instead used the exact\n",
    "Jacobian matrix, but allowed the update linear systems to be solved\n",
    "using an iterative solver? In this case, there is a small residual, i.e.\n",
    "$$\n",
    "  f'(x^k) \\hat{p}^k = -f(x^k) + r^k\n",
    "$$\n",
    "where $\\|r^k\\| \\leq \\eta_k \\|f(x^k)\\|$ (i.e. $\\eta_k$ is a relative residual\n",
    "tolerance on the solve). In this case,\n",
    "$$\n",
    "  \\|\\hat{p}^k-p(x^k)\\| = \\|f'(x^k)^{-1} r^k\\| \\leq B \\|r^k\\| \\leq\n",
    "  \\eta_k B \\|f(x^k)\\|.\n",
    "$$\n",
    "We also have that\n",
    "$$\n",
    "  \\|f(x^k)\\| = \\|f(x^k)-f(x^*)\\| = \\|f'(\\tilde{x}) e^k\\| \\leq C \\|e^k\\|\n",
    "$$\n",
    "where $C$ is a bound on the norm of $f'$. Thus\n",
    "$$\n",
    "  \\|\\hat{p}^k-p(x^k)\\| \\leq \\eta_k BC \\|e^k\\|,\n",
    "$$\n",
    "which we combine with the bound from the start of the notes to give\n",
    "$$\n",
    "  \\|e^{k+1}\\| \\leq B(M \\|e^k\\| + \\eta_k C) \\|e^k\\|\n",
    "  = O(\\|e^k\\|^2) + O(\\eta_k \\|e^k\\|).\n",
    "$$\n",
    "Hence, we have the following trade-off. If we solve the systems very \n",
    "accurately ($\\eta_k$ small), then inexact Newton will behave much like ordinary Newton. \n",
    "Thus, we expect to require few steps of the outer, nonlinear iteration; but the\n",
    "inner iteration (the linear solver) may require many steps to reach an\n",
    "acceptable residual tolerance. In contrast, if we choose $\\eta_k$ to be\n",
    "some modest constant independent of $k$, then we expect linear\n",
    "convergence of the outer nonlinear iteration, but each step may run\n",
    "relatively fast, since the linear systems are not solved to high\n",
    "accuracy.\n",
    "\n",
    "One attractive feature of Krylov subspace solvers for the Newton system\n",
    "is that they only require matrix-vector multiplies with the Jacobian —\n",
    "also known as directional derivative computations. We can approximate\n",
    "these directional derivaties by finite differences to get a method that\n",
    "may be rather more attractive than computing a full Jacobian\n",
    "approximation by finite differencing. However, it is necessary to use a\n",
    "Krylov subspace method that tolerates inexact matrix vector multiplies\n",
    "(e.g. FGMRES)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
