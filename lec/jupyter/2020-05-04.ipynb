{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-05-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing with constraints\n",
    "\n",
    "Recall that our basic problem is\n",
    "$$\n",
    "  \\mbox{minimize } \\phi(x) \\mbox{ s.t. } x \\in \\Omega\n",
    "$$\n",
    "where the feasible set $\\Omega$ is defined by equality and inequality conditions\n",
    "$$\n",
    "  \\Omega = \\{ x \\in {\\mathbb{R}}^n : c_i(x) = 0, i \\in \\mathcal{E} \\mbox{ and }\n",
    "    c_i(x) \\leq 0, i \\in \\mathcal{I} \\}.\n",
    "$$\n",
    "In the last lecture, we described three different ways to formulate constrained optimization\n",
    "problem that allow us to build on techniques we previously explored from\n",
    "unconstrained optimization and equation solving:\n",
    "\n",
    "1.  *Constraint elimination* (for equality constraints): Find a\n",
    "    parameterization $g : {\\mathbb{R}}^{n-m} \\rightarrow \\Omega$\n",
    "    formulations and minimize $\\phi(g(y))$ without constraints. This\n",
    "    requires that the constraints be simple (e.g. affine equality\n",
    "    constraints).\n",
    "\n",
    "2.  *Barriers and penalties:* Add a term to the objective function\n",
    "    depending on some parameter $\\mu$. This term penalizes $x$ values\n",
    "    that violate the constraint (penalty methods) or that come close to\n",
    "    $\\partial \\Omega$ from the inside (barrier methods). As\n",
    "    $\\mu \\rightarrow 0$, the unconstrained minimum of the modified\n",
    "    problems converges to the constrained minimum of the original.\n",
    "\n",
    "3.  *Lagrange multipliers*: Add new variables (multipliers)\n",
    "    corresponding to “forces” needed to enforce the constraints. The\n",
    "    *KKT conditions* are a set of nonlinear equations in the\n",
    "    original unknowns and the multipliers that characterize constrained\n",
    "    stationary points.\n",
    "\n",
    "Our goal now is to sketch how modern constrained optimization algorithms\n",
    "incorporate these different ways of looking at the problem. A full\n",
    "treatment is well beyond the scope of the class, but we hope to give you\n",
    "at least the keywords you will need should you encounter them in a\n",
    "textbook, paper, or a cocktail party. Ideally, knowing something\n",
    "about what happens in the algorithms will also help you think about\n",
    "which of various equivalent formulations of an optimization problem will\n",
    "be more (or less) friendly to solvers. The plan is to first give a \"lay\n",
    "of the land\" of different families of algorithms, then to give a more\n",
    "detailed treatment with the running example of linearly constrained\n",
    "quadratic programs.\n",
    "\n",
    "For more details, there are some excellent textbooks in the field;\n",
    "some texts that I really like from my own shelf include:\n",
    "\n",
    "- [*Numerical Optimization*, Nocedal and Wright](https://link.springer.com/book/10.1007/978-0-387-40065-5)\n",
    "- [*Practical Optimization*, Gill, Murray, and Wright](https://doi.org/10.1137/1.9781611975604)\n",
    "- [*Nonlinear Programming*, Bertsekas](http://www.athenasc.com/nonlinbook.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lay of the Land\n",
    "\n",
    "As we mentioned before, problems with *inequality* constraints tend\n",
    "to be more difficult than problems with *equality* constraints\n",
    "alone, because it involves the combinatorial subproblem of figuring out\n",
    "which constraints are *active* (a constraint $c_i(x) \\leq 0$ is\n",
    "active if $c_i(x) = 0$ at the optimum). Once we have figured out the set\n",
    "of active constraints, we can reduce an inequality-constrained problem\n",
    "to an equality-constrained problem. Hence, the purely\n",
    "equality-constrained case is an important subproblem for\n",
    "inequality-constrained optimizers, as well as a useful problem class in\n",
    "its own right.\n",
    "\n",
    "For problems with only equality constraints, there are several standard\n",
    "options:\n",
    "\n",
    "-   *Null space methods* deal with linear equality constraints by\n",
    "    reducing to an unconstrained problem in a lower-dimensional space.\n",
    "\n",
    "-   *Projected gradient methods* deal with simple equality\n",
    "    constraints by combining a (scaled) gradient step and a projection\n",
    "    onto a constraint set.\n",
    "\n",
    "-   *Penalty methods* approximately solve an equality-constrained\n",
    "    problem through an unconstrained problem with an extra term that\n",
    "    penalizes proposed soutions that violate the constraints. That is,\n",
    "    we use some constrained minimizer to solve\n",
    "    $$\n",
    "      \\mbox{minimize } \\phi(x) + \\frac{1}{\\mu} \\sum_{i \\in\\mathcal{E}} c_i(x)^2.\n",
    "    $$\n",
    "    As $\\mu \\rightarrow 0$, the minimizers to these approximate problems\n",
    "    approach the true minimizer, but the Hessians that we encounter\n",
    "    along the way become increasingly ill-conditioned (with condition\n",
    "    number proportional to $\\mu^{-1}$).\n",
    "\n",
    "-   *KKT solvers* directly tackle the first-order optimality\n",
    "    conditions (the KKT conditions), simultaneously computing the\n",
    "    constrained minimizer and the associated Lagrange multipliers.\n",
    "\n",
    "-   *Augmented Lagrangian* methods combine the advantages of penalty\n",
    "    methods and the advantages of the penalty formulation. In an\n",
    "    augmented Lagrangian solver, one finds critical points for the\n",
    "    augmented Lagrangian \n",
    "    $$\n",
    "      \\mathcal{L}(x, \\lambda; \\mu) =\n",
    "        \\phi(x) + \\frac{1}{\\mu} \\sum_{i \\in \\mathcal{E}} c_i(x)^2 + \\lambda^T c(x)\n",
    "    $$\n",
    "    by alternately adjusting the penalty parameter $\\mu$ and the\n",
    "    Lagrange multipliers.\n",
    "\n",
    "In the inequality-constrained case, we have\n",
    "\n",
    "-   *Active set methods* solve (or approximately solve) a sequence\n",
    "    of equality-constrained subproblems, shuffling constraints into and\n",
    "    out of the proposed working set along the way. These methods are\n",
    "    particularly attractive when one has a good initial estimate of the\n",
    "    active set.\n",
    "\n",
    "-   *Projected gradient methods* deal with simple inequality\n",
    "    constraints by combining a (scaled) gradient step and a projection\n",
    "    onto a constraint set.\n",
    "\n",
    "-   *Barrier methods* and *penalty methods* add a term to the\n",
    "    objective function in order to penalize constraint violations or\n",
    "    near-violations; as in the equality-constrained case, a parameter\n",
    "    $\\mu$ governs a tradeoff between solution quality and conditioning\n",
    "    of the Hessian matrix.\n",
    "\n",
    "-   *Interior point methods* solve a sequence of barrier subproblems\n",
    "    using a continuation strategy, where the barrier or penalty\n",
    "    parameter $\\mu$ is the continuation parameter. This is one of the\n",
    "    most popular modern solver strategies, though active set methods may\n",
    "    show better performance when one “warm starts” with a good initial\n",
    "    guess for the solution and the active set of constraints.\n",
    "\n",
    "As with augmented Lagrangian strategies in the equality-constrained\n",
    "case, state-of-the art strategies for inequality-constrained problems\n",
    "often combine approaches, using continuation with respect to a barrier\n",
    "parameters as a method of determining the active set of constraints in\n",
    "order to get to an equality-constrained subproblem with a good initial\n",
    "guess for the solution and the Lagrange multipliers.\n",
    "\n",
    "The *sequential quadratic programming* (SQP) approach for nonlinear\n",
    "optimization solves a sequence of linearly-constrained quadratic\n",
    "optimization problems based on Taylor expansion of the objective and\n",
    "constraints about each iterate. This generalizes simple Newton iteration\n",
    "for unconstrained optimization, which similarly solves a sequence of\n",
    "quadratic optimization problems based on Taylor expansion of the\n",
    "objective. Linearly-constrained quadratic programming problems are hence\n",
    "an important subproblem in SQP solvers, as well as being an important\n",
    "problem class in their own right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quadratic programs with equality constraints\n",
    "\n",
    "We begin with a simple case of a quadratic objective and linear equality\n",
    "constraints: \n",
    "$$\\begin{aligned}\n",
    "  \\phi(x) &= \\frac{1}{2} x^T H x - x^T d \\\\\n",
    "  c(x) &= A^T x-b = 0,\n",
    "\\end{aligned}$$\n",
    "where $H \\in {\\mathbb{R}}^{n \\times n}$ is symmetric and positive definite\n",
    "*on the null space of $A^T$* (it may be indefinite or singular\n",
    "overall), $A \\in {\\mathbb{R}}^{n \\times m}$ is full rank with $m < n$,\n",
    "and $b \\in {\\mathbb{R}}^m$. Not only are such problems useful in their\n",
    "own right, solvers for these problems are also helpful building blocks\n",
    "for more sophisticated problems — just as minimizing an unconstrained\n",
    "quadratic can be seen as the starting point for Newton’s method for\n",
    "unconstrained optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a test problem for linearly-constrained QP (2D so that we can plot)\n",
    "H = [4.0  1.0 ;\n",
    "     1.0  4.0 ]\n",
    "d = [0.5 ; -2.0]\n",
    "A = [1.0 ; 1.0]\n",
    "b = [1.0]\n",
    "\n",
    "ϕ1(xy) = xy'*H*xy/2 - xy'*d\n",
    "c1(xy) = A'*x - b\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "plot(xx, xx, (x,y) -> ϕ1([x; y]), st=:contour, legend=false)\n",
    "plot!(xx, 1.0 .- xx, linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constraint elimination (linear constraints)\n",
    "\n",
    "As discussed last time, we can write the space of solutions to the\n",
    "constraint equations in terms of a (non-economy) QR decomposition of\n",
    "$A$:\n",
    "$$\n",
    "A =\n",
    "  \\begin{bmatrix} Q_1 & Q_2 \\end{bmatrix}\n",
    "  \\begin{bmatrix} R_1 \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "where $Q_2$ is a basis for the null space of $A^T$. The set of solutions satisfying\n",
    "the constraints $A^T x = b$ is\n",
    "$$\n",
    "  \\Omega = \\{ u + Q_2 y : y \\in {\\mathbb{R}}^{(n-m)}, u = Q_1 R_1^{-T} b \\};\n",
    "$$\n",
    "here $u$ is a *particular solution* to the problem. If we substitute\n",
    "this parameterization of $\\Omega$ into the objective, we have the\n",
    "unconstrained problem \n",
    "$$\n",
    "  \\mbox{minimize } \\phi(u + Q_2 y).\n",
    "$$\n",
    "While we can substitute directly to get a quadratic objective in terms of $y$, \n",
    "it is easier (and a good exercise in remembering the chain rule) to compute\n",
    "the stationary equations\n",
    "$$\\begin{aligned}\n",
    "  0\n",
    "  &= \\nabla_y \\phi(u + Q_2 y) \n",
    "  = \\left(\\frac{\\partial x}{\\partial y}\\right)^T \\nabla_x \\phi(u+Q_2 y) \\\\\n",
    "  &= Q_2^T (H (Q_2 y + u) - d) \n",
    "  = (Q_2^T H Q_2) y - Q_2^T (d-Hu).\n",
    "\\end{aligned}$$\n",
    "In general, even if $A$ is sparse, $Q_2$ may be dense, and so even if $H$ is dense,\n",
    "we find that $Q_2^T H Q_2$ is dense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve the 2-by-2 problem via a null-space approach\n",
    "\n",
    "F = qr(A)\n",
    "Q = F.Q * I\n",
    "Q1 = Q[:,[1]]\n",
    "Q2 = Q[:,[2]]\n",
    "\n",
    "u = Q1*(F.R'\\b)\n",
    "H22 = Q2'*H*Q2\n",
    "r2  = Q2'*(d-H*u)\n",
    "y   = H22\\r2\n",
    "x   = u + Q2*y\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "plot(xx, xx, (x,y) -> ϕ1([x; y]), st=:contour, legend=false)\n",
    "plot!(xx, 1.0 .- xx, linewidth=2)\n",
    "plot!([u[1]], [u[2]], markercolor=:white, marker=true)\n",
    "plot!([x[1]], [x[2]], marker=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a particular solution and a null space basis via QR is great for numerical\n",
    "stability, but it may not be ideal when the matrices involved are sparse or structured.\n",
    "An alternative is to use a sparse LU factorization of $A^T$:\n",
    "$$\n",
    "  P A^T Q = L \\begin{bmatrix} U_1 U_2 \\end{bmatrix}.\n",
    "$$\n",
    "where the $U_1$ submatrix is upper triangular.  A particular solution is then\n",
    "$$\n",
    "  x = Q \\begin{bmatrix} U_1^{-1} L^{-1} P b \\\\ 0 \\end{bmatrix}\n",
    "$$\n",
    "and the null space is spanned by\n",
    "$$\n",
    "  Q^T \n",
    "  \\begin{bmatrix}\n",
    "    -U_1^{-1} U_2 \\\\\n",
    "    I\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "This reformulation may be particularly attractive if $A$ is large, sparse, and close\n",
    "to square.  Note that pivoting on rectangular constraint matrices needs to be done\n",
    "carefully, e.g. using so-called *rook pivoting* strategies that maintain numerical\n",
    "stability on full-rank matrices with rank-deficient submatrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projected gradient and conjugate gradients\n",
    "\n",
    "The *projected gradient* is a variant of gradient descent for constrained problem.  One assumes that we have\n",
    "a projection operator $P$ such that $P(x)$ is the closest point to $x$ satisfying the constraint; the iteration\n",
    "is then\n",
    "$$\n",
    "  x_{k+1} = P\\left( x_k - \\alpha_k \\nabla \\phi(x_k) \\right).\n",
    "$$\n",
    "That is, we take an (unconstrained) gradient descent step, then project back to satisfy the constraint.\n",
    "It's an easy enough method to code, provided you have the projection $P$.\n",
    "\n",
    "For our linear equality constraints the projection can be computed by a least squares type of solve:\n",
    "$$\\begin{aligned}\n",
    "  P(x) &= x + A(A^T A)^{-1} (b-A^T x) \\\\\n",
    "       &= (A^T)^\\dagger b + (I-AA^\\dagger) x \\\\\n",
    "       &= (A^T)^\\dagger b + (I-\\Pi) x\n",
    "\\end{aligned}$$\n",
    "Note that $(A^T)^\\dagger b$ is the minimal norm solution to the constraint equation, and the range space of\n",
    "$I-\\Pi = I-AA^\\dagger$ is the null space of $A^T$, so this is similar to the picture we saw with the constraint\n",
    "elimination approach.  And, of course, the gradient in this case is just the residual $r_k = Hx_k - d$.\n",
    "\n",
    "If we start with a point $x_0$ that is consistent with the constraints, then each successive\n",
    "point remains on our linear constraint surface; in this case, we can simplify the iteration to\n",
    "$$\n",
    "  x_{k+1} = x_k - \\alpha_k (I-\\Pi) r_k\n",
    "$$\n",
    "This is a stationary iteration for the underdetermined consistent equation\n",
    "$$\n",
    "  (I-\\Pi) (Hx_k-d) = 0.\n",
    "$$\n",
    "\n",
    "Unfortunately, the projected gradient iteration may converge rather slowly.  A tempting thought is to\n",
    "use a scaled version of the gradient, but the naive version of this iteration will in general converge\n",
    "to the wrong point unless the projection operator is re-defined in terms of the distance associated with\n",
    "the same scaling matrix.\n",
    "\n",
    "If the relevant projection is available, a potentially more attractive route for this problem \n",
    "is to write $x = u + z$ for some particular solution $u$ (as in the null space approach) and then\n",
    "use a method like conjugate gradients on the system\n",
    "$$\n",
    "  (I-\\Pi) H (I-\\Pi) z = (I-\\Pi) (d - Hu)\n",
    "$$\n",
    "It turns out that the Krylov subspace generated by this iteration remains consistent with the constraint,\n",
    "and so -- somewhat surprisingly at first glance -- the method continues to work even\n",
    "though $(I-\\Pi) H (I-\\Pi)$ is singular."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalties and conditioning\n",
    "\n",
    "Now consider a penalty formulation of the same equality-constrained\n",
    "optimization function, where the penalty is quadratic:\n",
    "$$\n",
    "  \\mbox{minimize } \\phi(x) + \\frac{1}{2\\mu} \\|A^T x-b\\|^2.\n",
    "$$\n",
    "In fact, the augmented objective function is again quadratic, and the critical\n",
    "point equations are\n",
    "$$\n",
    "  (H + \\mu^{-1} AA^T) x = d + \\mu^{-1} A b.\n",
    "$$\n",
    "If $\\mu$ is small enough and the equality-constrained quadratic program (QP)\n",
    "has a minimum, then $H+\\mu^{-1} AA^T$ is guaranteed to be positive definite.\n",
    "This means we can solve via Cholesky; or (if the linear system is larger)\n",
    "we might use conjugate gradients.\n",
    "\n",
    "We can analyze this more readily by changing to the $Q$ basis from the QR\n",
    "decomposition of $A$ that we saw in the constraint elimination approach:\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "  Q_1^T H Q_1 + \\mu^{-1} R_1 R_1^T & Q_1^T H Q_2 \\\\\n",
    "  Q_2^T H Q_1 & Q_2^T H Q_2\n",
    "\\end{bmatrix}\n",
    "(Q^T x) =\n",
    "\\begin{bmatrix}\n",
    "  Q_1^T d + \\mu^{-1} R_1 b \\\\\n",
    "  Q_2^T d\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "Taking a Schur complement, we have\n",
    "$$\n",
    "(\\mu^{-1} R_1 R_1^T + F)(Q_1^T x) = \\mu^{-1} R_1 b - g\n",
    "$$\n",
    "where\n",
    "$$\\begin{aligned}\n",
    "  F &= Q_1^T H Q_1 - Q_1^T H Q_2 (Q_2^T H Q_2)^{-1} Q_2^T H Q_1 \\\\\n",
    "  g &= [I - Q_1^T H Q_2 (Q_2^T H Q_2)^{-1} Q_2^T] d\n",
    "\\end{aligned}$$\n",
    "As $\\mu \\rightarrow 0$, the first row of equations is dominated by the\n",
    "$\\mu^{-1}$ terms, and we are left with\n",
    "$$\n",
    "  R_1 R_1^T (Q_1^T x) - R_1 b \\rightarrow 0\n",
    "$$\n",
    "i.e. $Q_1 Q_1^T x$ is converging to $u = Q_1 R_1^{-T} b$, the particular\n",
    "solution that we saw in the case of constraint elimination. Plugging this\n",
    "behavior into the second equation gives\n",
    "$$\n",
    "  (Q_2^T H Q_2) (Q_2^T x) - Q_2^T (d-Hu) \\rightarrow 0,\n",
    "$$\n",
    "i.e. $Q_2^T x$ asymptotically behaves like $y$ in the previous example.\n",
    "We need large $\\mu$ to get good results if the constraints are ill-posed or if\n",
    "$Q_2^T H Q_2$ is close to singular. But in general the condition number\n",
    "scales like $O(\\mu^{-1})$, and so large values of $\\mu$ correspond to\n",
    "problems that are numerically unattractive, as they may lead to large\n",
    "errors or (for iterative solvers) to slow convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate the solve with a moderate penalty\n",
    "\n",
    "μ = 1e-4\n",
    "xhat = (H+A*A'/μ)\\(d+A*b[1]/μ)\n",
    "println(\"Error at μ=$μ: $(xhat-x)\")\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "plot(xx, xx, (x,y) -> ϕ1([x; y]), st=:contour, legend=false)\n",
    "plot!(xx, 1.0 .- xx, linewidth=2)\n",
    "plot!([u[1]], [u[2]], markercolor=:white, marker=true)\n",
    "plot!([xhat[1]], [xhat[2]], marker=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vary penalty to illustrate issues -- uniform improvement with smaller penalty until ill-conditioning kills us\n",
    "μs = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14]\n",
    "errs = []\n",
    "for μ in μs\n",
    "    xhat = (H+A*A'/μ)\\(d+A*b[1]/μ)\n",
    "    push!(errs, norm(xhat-x))\n",
    "end\n",
    "plot(μs, errs, xscale=:log10, yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagrange multipliers and KKT systems\n",
    "\n",
    "The KKT conditions for our equality-constrained problem say that the\n",
    "gradient of $$L(x,\\lambda) = \\phi(x) + \\lambda^T (A^T x-b)$$ should be\n",
    "zero. In matrix form, the KKT system (saddle point system)\n",
    "$$\\begin{bmatrix}\n",
    "    H & A \\\\\n",
    "    A^T & 0\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} x \\\\ \\lambda \\end{bmatrix} =\n",
    "  \\begin{bmatrix} d \\\\ b \\end{bmatrix}.$$ If $A$ and $H$ are\n",
    "well-conditioned, then so is this system, so there is no bad numerical\n",
    "behavior. The system also retains whatever sparsity was present in the\n",
    "original system matrices $H$ and $A$. However, adding the Lagrange\n",
    "multipliers not only increases the number of variables, but the extended\n",
    "system lacks any positive definiteness that $H$ may have.\n",
    "\n",
    "When there are relatively few constraints and a fast solver with $H$\n",
    "is available, an attractive way to solve this KKT system is the so-called\n",
    "range-space method, which we recognize as just block Gaussian elimination:\n",
    "$$\\begin{aligned}\n",
    "  A^T H^{-1} A \\lambda &= A^T H^{-1} d - b \\\\\n",
    "  x = H^{-1} (d - A\\lambda)\n",
    "\\end{aligned}$$\n",
    "Rewritten as we might implement it, we have\n",
    "$$\\begin{aligned}\n",
    "  H x_0 &= d \\\\\n",
    "  H Y   &= A \\\\\n",
    "  (A^T Y) \\lambda &= A^T x_0 - b \\\\\n",
    "  x &= x_0 - Y \\lambda\n",
    "\\end{aligned}$$\n",
    "\n",
    "The KKT system is closely related to the penalty formulation that we saw\n",
    "in the previous subsection, in that if we use Gaussian elimination to\n",
    "remove the variable $\\lambda$ in $$\\begin{bmatrix}\n",
    "    H & A \\\\\n",
    "    A^T & -\\mu I\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} \\hat{x} \\\\ \\lambda \\end{bmatrix} =\n",
    "  \\begin{bmatrix} d \\\\ b \\end{bmatrix},$$ we have the Schur complement\n",
    "system $$(H+\\mu^{-1} AA^T) \\hat{x} = d + \\mu^{-1} A b,$$ which is\n",
    "identical to the stationary point condition for the quadratically\n",
    "penalized objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xλ = [H A; A' 0.0] \\ [d; b]\n",
    "println(\"Error: $(x-xλ[1:end-1])\")\n",
    "println(\"λ = $(xλ[end])\")\n",
    "println(\"∇ϕ(x) = $(H*xλ[1:end-1]-d)\")\n",
    "println(\"λ × ∇c(x) = $(xλ[end]) × $A = $(xλ[end]*A)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the constrained stationarity condition\n",
    "$$\n",
    "  \\nabla \\phi(x_*) + \\lambda \\nabla c(x_*) = 0,\n",
    "$$\n",
    "and we can use this to estimate the Lagrange multipliers from an approximation via a penalty method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "μ = 1e-4\n",
    "xhat = (H+A*A'/μ)\\(d+A*b[1]/μ)\n",
    "r = H*xhat-d\n",
    "println(\"λ ≈ $(-norm(r)^2/(A'*r))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uzawa iteration\n",
    "\n",
    "Block Gaussian elimination is an attractive approach when we have a fast solver for $H$ and there are not\n",
    "too many constraints.  When there are a relatively large number of constraints, we might seek an alternate\n",
    "method.  One such method is the *Uzawa iteration*\n",
    "$$\\begin{aligned}\n",
    "  H x_{k+1} &= d - A \\lambda_k \\\\\n",
    "  \\lambda_{k+1} &= \\lambda_{k} + \\omega (A^T x_{k+1}-b)\n",
    "\\end{aligned}$$\n",
    "where $\\omega > 0$ is a relaxation parameter.  We can eliminate $x_{k+1}$ to get the iteration\n",
    "$$\n",
    "  \\lambda_{k+1} \n",
    "  = \\lambda_k + \\omega (A^T H^{-1} (d-A\\lambda_k) - b)\n",
    "  = (I-\\omega A^T H^{-1} A) \\lambda_k + \\omega (A^T H^{-1} d  - b),\n",
    "$$\n",
    "which is a Richardson iteration on the Schur complement equation $(A^T H^{-1} A) \\lambda = A^T H^{-1} d - b$.\n",
    "We can precondition and accelerate the Uzawa iteration in a variety of ways, as you might guess from our earlier\n",
    "discussion of iterative methods for solving linear systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmenting the Lagrangian\n",
    "\n",
    "From a solver perspective, the block 2-by-2 structure of the KKT system\n",
    "looks highly attractive. Alas, we do *not* require that $H$ be\n",
    "positive definite, nor even that it be nonsingular; to have a unique\n",
    "global minimum, we only need positive definiteness of the projection of\n",
    "$H$ onto the null space (i.e. $Q_2^T H Q_2$ should be positive\n",
    "definite). This means we cannot assume that (for example) $H$ will admit\n",
    "a Cholesky factorization.\n",
    "\n",
    "The augmented Lagrangian approach can be seen as solving the constrained\n",
    "system\n",
    "$$\n",
    "  \\mbox{minimize } \\frac{1}{2} x^T H x - d^T x + \\frac{1}{2\\mu} \\|A^T x-b\\|^2\n",
    "  \\mbox{ s.t. } A^T x = b.\n",
    "$$\n",
    "The term penalizing nonzero $\\|A^T x-b\\|$ is, of course, irrelevant at points \n",
    "satisfying the constraint $A^T x = b$. Hence, the constrained minimum for this\n",
    "augmented objective is identical to the constrained minimum of the original objective.\n",
    "However, if the KKT conditions for the modified objective take the form\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "    H+\\mu^{-1}AA^T & A \\\\\n",
    "    A^T & 0\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} x \\\\ \\lambda \\end{bmatrix} =\n",
    "  \\begin{bmatrix} d + \\mu^{-1} A b \\\\ b \\end{bmatrix}.\n",
    "$$\n",
    "Now we do not necessarily need to drive $\\mu$ to zero to obtain a good solution;\n",
    "but by choosing $\\mu$ small enough, we can ensure that $H + \\mu^{-1} AA^T$\n",
    "is positive definite (assuming that the problem is convex subject to the\n",
    "constraint).  This can be helpful when we want to use a Cholesky factorization\n",
    "or a method like CG, but the original $H$ matrix is indefinite or singular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a non-positive-definite problem for linearly-constrained QP (2D so that we can plot)\n",
    "H = [4.0  1.0 ;\n",
    "     1.0  -1.0 ]\n",
    "d = [0.5 ; -2.0]\n",
    "A = [1.0 ; 1.0]\n",
    "b = 1.0\n",
    "\n",
    "ϕ2(xy) = xy'*H*xy/2 - xy'*d\n",
    "c2(xy) = A'*xy - b\n",
    "ϕ3(xy) = ϕ2(xy) + 3*norm(c2(xy))^2\n",
    "xy_sol = [H A; A' 0.0] \\ [d; b]\n",
    "\n",
    "xx = range(-3, 3, length=100)\n",
    "plot(xx, xx, (x,y) -> ϕ2([x; y]), st=:contour, legend=false)\n",
    "plot!(xx, xx, (x,y) -> ϕ3([x; y]), st=:contour, linestyle=:dash)\n",
    "plot!(xx, 1.0 .- xx, linewidth=2)\n",
    "plot!([xy_sol[1]], [xy_sol[2]], marker=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: do we have a constrained min (vs max or saddle)?\n",
    "plot(xx, [ϕ2([x; 1.0-x]) for x in xx], legend=false)\n",
    "plot!(xx, [ϕ2aug([x; 1.0-x]) for x in xx], linestyle=:dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the solution via method of Lagrange multipliers\n",
    "xλ = [H A; A' 0.0] \\ [d; b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the Lagrangian so the (1,1) submatrix is positive definite\n",
    "σ = 2.0\n",
    "xλ_augmented = [H+A*A'/σ A; A' 0.0] \\ [d+A*b[1]/σ; b]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
