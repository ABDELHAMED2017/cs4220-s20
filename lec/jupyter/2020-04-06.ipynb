{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-06\n",
    "\n",
    "## Nonlinear equations and optimization\n",
    "\n",
    "If $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, then solving the system $f(x) = 0$\n",
    "is equivalent to minimizing $\\|f(x)\\|^2$.  Similarly, if $g : \\mathbb{R}^n\n",
    "\\rightarrow \\mathbb{R}$ is continuously differentiable, then any local\n",
    "minimizer $x_*$ satisfies the nonlinear equations $\\nabla g(x_*) = 0$.\n",
    "There is thus a close connection between nonlinear equation solving\n",
    "on the one hand and optimization on the other, and methods used for\n",
    "one problem can serve as the basis for methods for the other.\n",
    "\n",
    "As with nonlinear equations, the one-dimensional case is the simplest,\n",
    "and may be the right place to start our discussion.  As\n",
    "with the solution of nonlinear equations, our main strategy for\n",
    "dealing with multi-variable optimization problems will be to find a\n",
    "promising search direction and then solve (approximately) a\n",
    "one-dimensional line search problem.\n",
    "\n",
    "For illustrating the basics, we will use the objective function\n",
    "$g(x) = \\cos(x) \\log(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtest(x) = cos(x) * log(x)\n",
    "xx = range(0.1, stop=2*π, length=100)\n",
    "plot(xx, gtest.(xx), legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A little calculus gives us\n",
    "$$\n",
    "  g'(x) = -\\sin(x) \\log(x) + \\cos(x)/x.\n",
    "$$\n",
    "and\n",
    "$$\n",
    "  g''(x) = -\\cos(x) \\log(x) - 2\\sin(x)/x - \\cos(x)/x^2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgtest(x) = -sin(x)*log(x) + cos(x)/x\n",
    "d2gtest(x) = -cos(x)*log(x) - 2*sin(x)/x - cos(x)/x^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I usually code up finite difference sanity checks when coding derivatives,\n",
    "so let's not let this be an exception.  We discussed in the last lecture the\n",
    "centered finite difference approximation for a first derivative\n",
    "$$\n",
    "  f'(x) \\approx \\frac{f(x+h)-f(x-h)}{2h} + O(h^2),\n",
    "$$\n",
    "and we can similarly derive a finite difference approximation for the second\n",
    "derivative\n",
    "$$\n",
    "  f''(x) \\approx \\frac{f(x+h)-2f(x)+f(x+h)}{h^2} + O(h^2).\n",
    "$$\n",
    "Some sense of magnitudes is useful in cases like this.  The second derivative\n",
    "gets pretty big for values of $x$ near zero, and there is a lot of cancellation\n",
    "in these formulas, so we don't necessarily expect that a tiny finite difference step\n",
    "will give great results.  Fortunately, we are not currently using the finite\n",
    "difference formulas in lieu of the true derivatives, but as a sanity check to\n",
    "make sure that we did not swap a negative sign or add a factor of two somewher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_fd(f, h)  = (x) -> (f(x+h)-f(x-h))/2/h\n",
    "deriv2_fd(f, h) = (x) -> (f(x+h)-2*f(x)+f(x-h))/h^2\n",
    "\n",
    "dgtest_fd  = deriv_fd(gtest, 1e-4)\n",
    "d2gtest_fd = deriv2_fd(gtest, 1e-4)\n",
    "\n",
    "dg_diff = maximum(abs.(dgtest.(xx) - dgtest_fd.(xx)))\n",
    "d2g_diff = maximum(abs.(d2gtest.(xx) - d2gtest_fd.(xx)))\n",
    "\n",
    "println(\"Check on g': $(dg_diff)\")\n",
    "println(\"Check on g'': $(d2g_diff)\")\n",
    "plot(xx, abs.(d2gtest.(xx) - d2gtest_fd.(xx)), yscale=:log10, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimization via Newton\n",
    "\n",
    "Suppose $g : {\\mathbb{R}}\\rightarrow {\\mathbb{R}}$ has at least two\n",
    "continuous derivatives. If we can compute $g'$ and $g''$, then one of\n",
    "the simplest ways to find a local minimum is to use Newton iteration to\n",
    "find a stationary point: $$x_{k+1} = x_k - \\frac{g'(x_k)}{g''(x_k)}.$$\n",
    "Geometrically, this is equivalent to finding the maximum (or minimum) of\n",
    "a second-order Taylor expansion about $x_{k}$; that is, $x_{k+1}$ is\n",
    "chosen to minimize (or maximize)\n",
    "$$\\hat{g}(x_{k+1}) = g(x_k) + g'(x_k)(x_{k+1}-x_k) + \\frac{1}{2} g''(x_k) (x_{k+1}-x_k)^2.$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_v1(x, dg, d2g; dtol=1e-6, atol=1e-6, nsteps=100, monitor=(x)->nothing)\n",
    "    monitor(x)\n",
    "    for k = 1:nsteps\n",
    "        dgx = dg(x)\n",
    "        p = -dgx/d2g(x)\n",
    "        x += p\n",
    "        monitor(x)\n",
    "        if abs(dgx) < dtol || abs(p) < atol\n",
    "            return x\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge within $(nsteps) steps\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_newton_v1(x0, xrange=(0.1, 2*π))\n",
    "    xhist = Array{Float64,1}([])\n",
    "    x = newton_v1(x0, dgtest, d2gtest, monitor=(x)->push!(xhist,x))\n",
    "    xx = range(xrange[1], stop=xrange[2], length=100)\n",
    "    l = @layout [a b]  \n",
    "    p1 = plot(xx, gtest.(xx), xlabel=\"x\", ylabel=\"g(x)\", legend=false)\n",
    "    plot!([x], [gtest(x)], marker=true)\n",
    "    yy = abs.(dgtest.(xhist))\n",
    "    p2 = plot(yy[yy .> 0], yscale=:log10, xlabel=\"k\", ylabel=\"|g'(x_k)|\", legend=false)\n",
    "    plot(p1, p2, layout=l)\n",
    "end\n",
    "plot_newton_v1(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in the case of root finding, we can illustrate the behavior of the method by plotting successive quadratic approximations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xhist = Array{Float64,1}([])\n",
    "xref = newton_v1(2.7, dgtest, d2gtest, monitor=(x)->push!(xhist,x))\n",
    "yy = abs.(dgtest.(xhist))\n",
    "\n",
    "anim = @animate for (i, x) in enumerate(xhist)\n",
    "    err = abs(dgtest(x))\n",
    "    l = @layout [a b]    \n",
    "    p1 = plot(xx, gtest.(xx), legend=false)\n",
    "    plot!([xref], [gtest(xref)], marker=true)\n",
    "    plot!([x], [gtest(x)], marker=true)\n",
    "    plot!(xx, gtest(x) .+ dgtest(x)*(xx.-x) .+ d2gtest(x)/2*(xx.-x).^2, linestyle=:dash)\n",
    "    plot!([x - dgtest(x)/d2gtest(x)], [gtest(x)-dgtest(x)^2/d2gtest(x)/2], marker=true, markercolor=\"white\")\n",
    "    plot!(yaxis=[-2,2])\n",
    "    p2 = plot(yy[yy .> 0], yscale=:log10, legend=false)\n",
    "    if err > 0\n",
    "        plot!([i], [err], marker=true)\n",
    "    end\n",
    "    plot(p1, p2, layout=l)   \n",
    "end\n",
    "gif(anim, fps=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two gotchas in using Newton iteration in this way. We have\n",
    "already run into the first issue: Newton’s method is only locally\n",
    "convergent. We can take care of that problem by combining Newton with\n",
    "bisection, or by scaling down the length of the Newton step. But there\n",
    "is another issue, too: saddle points and local maxima are also\n",
    "stationary points!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_newton_v1(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a simple precaution we can take to avoid converging to a\n",
    "maximum: insist that $g(x_{k+1}) < g(x_k)$. If\n",
    "$x_{k+1} = x_k - \\alpha_k u$ for some $\\alpha_k > 0$, then\n",
    "$$g(x_{k+1}) - g(x_k) = -\\alpha_k g'(x_k) u + O(\\alpha_k^2).$$ So if\n",
    "$g'(x_k) u > 0$, then $-u$ is a *descent direction*, and thus\n",
    "$g(x_{k+1}) < g(x_k)$ provided $\\alpha_k$ is small enough. \n",
    "The direction $-g'(x)$ is always a descent direction.  Note that if\n",
    "$x_k$ is not a stationary point, then $-u = -g'(x_k)/g''(x_k)$ is a\n",
    "descent direction iff $g'(x_k) u = g'(x_k)^2 / g''(x_k) > 0$. That is,\n",
    "we will only head in the direction of a minimum if $g''(x_k)$ is\n",
    "positive. Of course, $g''$ will be positive and the Newton step will\n",
    "take us in the right direction if we are close enough to a strong local\n",
    "minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_v2(x, g, dg, d2g; dtol=1e-6, atol=1e-6, nsteps=100, monitor=(x)->nothing)\n",
    "    monitor(x)\n",
    "    \n",
    "    # Compute initial value and step\n",
    "    gx = g(x)\n",
    "    dgx = dg(x)\n",
    "    d2gx = d2g(x)\n",
    "    if d2gx > 0.\n",
    "        p = -dgx/d2g(x)\n",
    "    else\n",
    "        p = -dgx\n",
    "    end\n",
    "    α = 1.0\n",
    "\n",
    "    for k = 1:nsteps\n",
    "\n",
    "        # Evaluate attempted step\n",
    "        gxnew = g(x+α*p)\n",
    "        if gxnew < gx\n",
    "            x += α*p\n",
    "            gx = gxnew\n",
    "            dgx = dg(x)\n",
    "            if d2gx > 0.\n",
    "                p = -dgx/d2g(x)\n",
    "            else\n",
    "                p = -dgx\n",
    "            end\n",
    "            p = -dgx/d2g(x)\n",
    "            α = 1.0\n",
    "            monitor(x)\n",
    "            if abs(dgx) < dtol || abs(p) < atol\n",
    "                return x\n",
    "            end\n",
    "        else\n",
    "            p *= 0.5\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge within $(nsteps) steps\")\n",
    "end\n",
    "\n",
    "function plot_newton_v2(x0, xrange=(0.1,2*π))\n",
    "    xhist = Array{Float64,1}([])\n",
    "    x = newton_v2(x0, gtest, dgtest, d2gtest, monitor=(x)->push!(xhist,x))\n",
    "    xx = range(xrange[1], stop=xrange[2], length=100)\n",
    "    l = @layout [a b]  \n",
    "    p1 = plot(xx, gtest.(xx), xlabel=\"x\", ylabel=\"g(x)\", legend=false)\n",
    "    plot!([x], [gtest(x)], marker=true)\n",
    "    yy = abs.(dgtest.(xhist))\n",
    "    p2 = plot(yy[yy .> 0], yscale=:log10, xlabel=\"k\", ylabel=\"|g'(x_k)|\", legend=false)\n",
    "    plot(p1, p2, layout=l)\n",
    "end\n",
    "    \n",
    "plot_newton_v2(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_newton_v1(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  In `plot_newton_v2`, explain the line\n",
    "            plot!([x - dgtest(x)/d2gtest(x)], [gtest(x)-dgtest(x)^2/d2gtest(x)/2], marker=true, markercolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  What happens if you start the `newton_v2` code at the initial guess of $x_0 = 2$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approximate bisection and golden section\n",
    "\n",
    "Assuming that we can compute first derivatives, minimizing in 1D reduces\n",
    "to solving a nonlinear equation, possibly with some guards to prevent\n",
    "the solver from wandering toward a solution that does not correspond to\n",
    "a minimum. We can solve the nonlinear equation using Newton iteration,\n",
    "secant iteration, bisection, or any combination thereof, depending how\n",
    "sanguine we are about computing second derivatives and how much we are\n",
    "concerned with global convergence. But what if we don’t even want to\n",
    "compute first derivatives?\n",
    "\n",
    "To make our life easier, let’s suppose we know that $g$ is twice\n",
    "continuously differentiable and that it has a unique minimum at some\n",
    "$x_* \\in [a,b]$. We know that $g'(x) < 0$ for $a \\leq x < x_*$ and\n",
    "$g'(x) > 0$ for $x_* < x \\leq b$; but how can we get a handle on $g'$\n",
    "without evaluating it? The answer lies in the mean value theorem.\n",
    "Suppose we evaluate $g(a)$, $g(b)$, and $g(x)$ for some $x \\in (a,b)$.\n",
    "What can happen?\n",
    "\n",
    "1.  If $g(a)$ is smallest ($g(a) < g(x) \\leq g(b)$), then by the mean\n",
    "    value theorem, $g'$ must be positive somewhere in $(a,x)$.\n",
    "    Therefore, $x_* < x$.\n",
    "\n",
    "2.  If $g(b)$ is smallest, $x_* > x$.\n",
    "\n",
    "3.  If $g(x)$ is smallest, we only know $x_* \\in [a,b]$.\n",
    "\n",
    "Cases 1 and 2 are terrific, since they mean that we can improve our\n",
    "bounds on the location of $x_*$. But in case 3, we have no improvement.\n",
    "Still, this is promising. What could we get from evaluating $g$ at\n",
    "*four* distinct points $a < x_1 < x_2 < b$? There are really two\n",
    "cases, both of which give us progress.\n",
    "\n",
    "1.  If $g(x_1) < g(x_2)$ (i.e. $g(a)$ or $g(x_1)$ is smallest) then\n",
    "    $x_* \\in [a, x_2]$.\n",
    "\n",
    "2.  If $g(x_1) > g(x_2)$ (i.e. $g(b)$ or $g(x_2)$ is smallest) then\n",
    "    $x_* \\in [x_1,b]$.\n",
    "\n",
    "We could also conceivably have $g(x_1) = g(x_2)$, in which case the\n",
    "minimum must occur somewhere in $(x_1,x_2)$.\n",
    "\n",
    "There are now a couple options. We could choose $x_1$ and $x_2$ to be\n",
    "very close to each other, thereby nearly bisecting the interval in all\n",
    "four cases. This is essentially equivalent to performing a step of\n",
    "bisection to find a root of $g'$, where $g'$ at the midpoint is\n",
    "estimated by a finite difference approximation. With this method, we\n",
    "require two function evaluations to bisect the interval, which means we\n",
    "narrow the interval by $1/\\sqrt{2} \\approx 71\\%$ per evaluation.\n",
    "\n",
    "We can do a little better with a *golden section search*, which uses\n",
    "$x_2 = a+(b-a)/\\phi$ and $x_1 = b + (a-b)/\\phi$, where $\\phi =\n",
    "(1+\\sqrt{5})/2$ (the *golden ratio*). We then narrow to the interval\n",
    "$[a,x_2]$ or to the interval $[x_1,b]$. This only narrows the interval\n",
    "by a factor of $\\phi^{-1}$ (or about 61%) at each step. But in the\n",
    "narrower interval, we get one of the two interior function values “for\n",
    "free” from the previous step, since $x_1 =\n",
    "x_2+(a-x_2)/\\phi$ and $x_2 = x_1+(b-x_1)/\\phi$. Thus, each step only\n",
    "costs one function evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function golden_section(g, a, b; atol=1e-6, monitor=(a,b)->nothing)\n",
    "    monitor(a,b)\n",
    "    ga = g(a)\n",
    "    gb = g(b)\n",
    "    ϕ = (1+sqrt(5))/2\n",
    "    x1 = b+(a-b)/ϕ\n",
    "    x2 = a+(b-a)/ϕ\n",
    "    g1 = g(x1)\n",
    "    g2 = g(x2)\n",
    "    while abs(b-a) > 2*atol\n",
    "        if g1 < g2\n",
    "            b, x2 = x2, x1\n",
    "            gb, g2 = g2, g1\n",
    "            x1 = b+(a-b)/ϕ\n",
    "            g1 = g(x1)\n",
    "        elseif g1 > g2\n",
    "            a, x1 = x1, x2\n",
    "            ga, g1 = g1, g2\n",
    "            x2 = a+(b-a)/ϕ\n",
    "            g2 = g(x2)\n",
    "        elseif g1 == g2\n",
    "            x1 = b+(a-b)/ϕ\n",
    "            x2 = a+(b-a)/ϕ\n",
    "            g1 = g(x1)\n",
    "            g2 = g(x2)\n",
    "        end\n",
    "        monitor(a,b)\n",
    "    end\n",
    "    return (a+b)/2\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_hist = Array{Tuple{Float64,Float64},1}([])\n",
    "golden_section(gtest, 2, 5, monitor=(a,b) -> push!(ab_hist, (a,b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϕ = (1+sqrt(5))/2\n",
    "xmids = [(a+b)/2 for (a,b) in ab_hist]\n",
    "plot(abs.(dgtest.(xmids)), yscale=:log10, legend=false)\n",
    "plot!(abs(dgtest(3.5)) * ϕ.^-(0:length(xmids)-1), linestyle=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Successive parabolic interpolation\n",
    "\n",
    "Bisection and golden section searches are only linearly convergent. Of\n",
    "course, these methods only use coarse information about the relative\n",
    "sizes of function values at the sample points. In the case of\n",
    "root-finding, we were able to get a superlinearly convergent algorithm,\n",
    "the secant iteration, by replacing the linear approximation used in\n",
    "Newton’s method with a linear interpolant. We can do something similar\n",
    "in the case of optimization by interpolating $g$ with a *quadratic*\n",
    "passing through three points, and then finding a new guess based on the\n",
    "minimum of that quadratic. This *method of successive parabolic interpolation* \n",
    "does converge locally superlinearly. But even when $g$\n",
    "is unimodular, successive parabolic interpolation must generally be\n",
    "coupled with something slower but more robust (like golden section\n",
    "search) in order to guarantee good convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function simple_spi(g, a, b; atol=1e-6, nsteps=100, monitor=(a,b,c) -> nothing)\n",
    "    c = (a+b)/2\n",
    "    monitor(a, b, c)\n",
    "    ga = g(a)\n",
    "    gb = g(b)\n",
    "    gc = g(c)\n",
    "    for k = 1:nsteps\n",
    "        g_ab = (ga-gb)/(a-b)\n",
    "        g_bc = (gb-gc)/(b-c)\n",
    "        g_abc = (g_ab-g_bc)/(a-c)\n",
    "        x = (a+b-g_ab/g_abc)/2\n",
    "        a, b, c = b, c, x\n",
    "        ga, gb, gc = gb, gc, g(x)\n",
    "        monitor(a, b, c)\n",
    "        if abs(b-c) < atol\n",
    "            return c\n",
    "        end\n",
    "    end\n",
    "    error(\"Did not converge within given number of steps\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chist = Array{Float64,1}([])\n",
    "x = simple_spi(gtest, 3, 4, atol=1e-8, monitor=(a,b,c) -> push!(chist, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(abs.(dgtest.(chist)), yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chebyshev surrogates (optional)\n",
    "\n",
    "This section is *completely* optional, and uses a reasonable amount of mathematics only touched\n",
    "on in this course.  However, it connects beautifully to our discussion of the uses\n",
    "of eigenvalue problems (and the closely related problem of polynomial root finding) in our discussion.\n",
    "We therefore present it for your reading pleasure.\n",
    "\n",
    "For sufficiently smooth functions, a reasonable strategy for root-finding and optimization is to first\n",
    "approximate the target function by a polynomial, then solve the equivalent problem for the polynomial.\n",
    "So far, when talking about polynomial approximation, we have mostly discussed the power basis,\n",
    "but a more stable approach for approximation on the interval $[-1, 1]$ is to use\n",
    "the *Chebyshev polynomials*\n",
    "$$\n",
    "  T_{j+1}(x) = 2x T_j(x) - T_{j-1}(x)\n",
    "$$\n",
    "with $T_0(x) = 1$ and $T_1(x) = x$.  We can fit the coefficients in a Chebyshev series\n",
    "$$\n",
    "  f(x) \\approx \\sum_{j=0}^{N-1} a_j T_j(x)\n",
    "$$\n",
    "by a discrete cosine transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function chebfit(f, N)\n",
    "    a = zeros(N)\n",
    "    x = cos.(π*((0:N-1) .+ 0.5)/N)\n",
    "    fx = f.(x)\n",
    "    a[1] = sum(fx)/N\n",
    "    a[2] = 2*sum(x.*fx)/N\n",
    "    Tm = 0.0*x.+1.0\n",
    "    Tp = x\n",
    "    for k = 3:N\n",
    "        Tp, Tm = 2*(x.*Tp)-Tm, Tp\n",
    "        a[k] = 2*sum(Tp.*fx)/N\n",
    "    end\n",
    "    return a\n",
    "end\n",
    "\n",
    "function chebeval(a, x)\n",
    "    f = a[1] .+ a[2]*x\n",
    "    Tm = 0.0*x.+1.0\n",
    "    Tp = x\n",
    "    for k = 3:length(a)\n",
    "        Tp, Tm = 2*(x.*Tp)-Tm, Tp\n",
    "        f += a[k]*Tp\n",
    "    end\n",
    "    return f\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chebyshev approximation of our test function on an interval not too near zero does a pretty good job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gtests(x) = gtest(0.5*(1-x)/2 + 5*π*(1+x)/2)\n",
    "a = chebfit(gtests, 30)\n",
    "xxs = range(-1.0, stop=1.0, length=100)\n",
    "maxerr = maximum(abs.(gtests.(xxs) - chebeval(a,xxs)))\n",
    "println(\"Max approx error: $maxerr; a[N] = $(a[end])\")\n",
    "plot(xxs, gtests.(xxs), label=\"g(x)\")\n",
    "plot!(xxs, chebeval(a, xxs), label=\"Approx g(x)\", linestyle=:dash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(abs.(a), yscale=:log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation by a (truncated) Chebyshev series is ultimately a polynomial approximation,\n",
    "and we can compute the roots of a polynomial in terms of an associated matrix for which the\n",
    "polynomial is the characteristic polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function chebzeros(a)\n",
    "    N = length(a)\n",
    "    C = zeros(N-1, N-1)\n",
    "    C[1,:] = -a[end-1:-1:1]/a[end]\n",
    "    for j = 2:N-1\n",
    "        C[j,j-1] += 1.0\n",
    "        C[j-1,j] += 1.0\n",
    "    end\n",
    "    C[N-1,N-2] = 2.0\n",
    "    C /= 2.0\n",
    "    x = eigvals(C)\n",
    "    x = real.(x[imag.(x) .== 0])\n",
    "    x = x[x .>= -1]\n",
    "    x = x[x .<= 1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xroot = chebzeros(a)\n",
    "plot(xxs, gtests.(xxs), label=\"g(x)\")\n",
    "plot!(xxs, chebeval(a, xxs), label=\"Approx g(x)\", linestyle=:dash)\n",
    "scatter!(xroot, 0*xroot, label=\"Roots\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, our goal in the current lecture is to find not the zeros of a function, but the extrema (and particularly\n",
    "the minima).  To do this, we want the roots of the derivative of the approximant $\\hat{g}(x)$.  This can be computed\n",
    "in terms of a backward recurrence on the coefficients; we do not attempt to derive this rather inscrutable-looking\n",
    "formula here, but rather refer to the delightful book of Boyd (*Chebyshev and Fourier Spectral Methods*, second ed)\n",
    "from Appendix A, \"A Bestiary of Basis Functions\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function chebderiv(a)\n",
    "    N = length(a)\n",
    "    b = zeros(N-1)\n",
    "    for k = N-1:-1:1\n",
    "        b[k] = 2*k*a[k+1] + (k+2 < N ? b[k+2] : 0)\n",
    "    end\n",
    "    b[1] /= 2\n",
    "    return b[1:N-1]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dgtests(x) = (5*π-0.5)/2 * dgtest(0.5*(1-x)/2 + 5*π*(1+x)/2)\n",
    "b = chebderiv(a)\n",
    "xxs = range(-1.0, stop=1.0, length=100)\n",
    "maxerr = maximum(abs.(dgtests.(xxs) - chebeval(b,xxs)))\n",
    "println(\"Max approx error: $maxerr; b[N] = $(b[end])\")\n",
    "plot(xxs, dgtests.(xxs), label=\"g'(x)\")\n",
    "plot!(xxs, chebeval(b, xxs), label=\"Approx g''(x)\", linestyle=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having computed the coefficients for $\\hat{g}'(x)$, we can now compute all the critical points\n",
    "of $\\hat{g}$, i.e. all $x$ such that $\\hat{g}'(x) = 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xroot = chebzeros(b)\n",
    "plot(xxs, gtests.(xxs), label=\"g(x)\")\n",
    "plot!(xxs, chebeval(a, xxs), label=\"Approx g(x)\", linestyle=:dash)\n",
    "scatter!(xroot, gtests.(xroot), label=\"Extrema\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the global minimum of $g(x)$ on $[-1,1]$, then, we have the following proposed approach:\n",
    "\n",
    "1.  Approximate $g(x)$ by a polynomial $\\hat{g}(x)$ expressed in a Chebyshev basis.\n",
    "2.  Find the critical points of $\\hat{g}(x)$.\n",
    "3.  Return the critical point (or end point) for which $\\hat{g}(x)$ is minimal.\n",
    "\n",
    "One can further refine this computation with a few steps of Newton iteration (or approximate Newton iteration\n",
    "in which $\\hat{g}'(x)$ is used to estimate $g'(x)$).  Of course, this method can be fooled if one uses too coarse\n",
    "an approximation to $g$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems to ponder\n",
    "\n",
    "1.  Suppose I know $f(0)$, $f(1)$, and a bound $|f''| < M$ on $[0,1]$.\n",
    "    Under what conditions could $f$ possibly have a local minimum in\n",
    "    $[0,1]$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Suppose $f(x)$ is approximated on $[0,1]$ by a polynomial $p(x)$ of degree at most $d$,\n",
    "    and we know that $|f(x)-p(x)| < \\delta$ on the interval. Using a polynomial zero-finding function,\n",
    "    how would we find tight subintervals of $[0,1]$ in which the global minimum of $f(x)$ might lie?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
