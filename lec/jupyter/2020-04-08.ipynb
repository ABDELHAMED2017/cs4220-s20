{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-08\n",
    "\n",
    "## Direct to Iterative\n",
    "\n",
    "For the first part of the semester, we discussed *direct* methods\n",
    "for solving linear systems and least squares problems. These methods\n",
    "typically involve a factorization, such as LU or QR, that reduces the\n",
    "problem to a triangular solve using forward or backward substitution.\n",
    "These methods run to completion in a fixed amount of time, and are\n",
    "backed by reliable software in packages like LAPACK or UMFPACK.\n",
    "\n",
    "There are a few things you need to know to be an informed *user*\n",
    "(not developer) of direct methods:\n",
    "\n",
    "-   You need some facility with matrix algebra, so that you know how to\n",
    "    manipulate matrix factorizations and “push parens” in order to\n",
    "    compute efficiently.\n",
    "\n",
    "-   You need to understand the complexity of different factorizations,\n",
    "    and a little about how to take advantage of common matrix structures\n",
    "    (e.g. low-rank structure, symmetry, orthogonality, or sparsity) in\n",
    "    order to effectively choose between factorizations and algorithms.\n",
    "\n",
    "-   You need to understand a little about conditioning and the\n",
    "    relationship between forward and backward error. This is important\n",
    "    not only for understanding rounding errors, but also for\n",
    "    understanding how other errors (such as measurement errors) can\n",
    "    affect a result.\n",
    "\n",
    "It’s also immensely helpful to understand a bit about how the methods\n",
    "work in practice. On the other hand, you are unlikely to have to build\n",
    "your own dense Gaussian elimination code with blocking for efficiency;\n",
    "you’ll probably use a library routine instead. It’s more important that\n",
    "you understand the ideas behind the factorizations, and how to apply\n",
    "those ideas to use the factorizations effectively in applications.\n",
    "\n",
    "Compared to direct methods, iterative methods provide more room for\n",
    "clever, application-specific twists and turns. An iterative method for\n",
    "solving the linear system $Ax = b$ produces a series of guesses\n",
    "$$\n",
    "  \\hat{x}^1, \\hat{x}^2, \\ldots \\rightarrow x.\n",
    "$$\n",
    "The goal of the iteration is not always to get the exact answer as fast as possible; it\n",
    "is to get a good enough answer, fast enough to be useful. The rate at\n",
    "which the iteration converges to the solution depends not only on the\n",
    "nature of the iterative method, but also on the structure in the\n",
    "problem. The picture is complicated by the fact that different\n",
    "iterations cost different amounts per step, so a “slowly convergent”\n",
    "iteration may in practice get an adequate solution more quickly than a\n",
    "“rapidly convergent” iteration, just because each step in the slowly\n",
    "convergent iteration is so cheap.\n",
    "\n",
    "As with direct methods, though, sophisticated iterative methods are\n",
    "constructed from simpler building blocks. In this lecture, we set up one\n",
    "such building block: stationary iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary Iterations\n",
    "\n",
    "A stationary iteration for the equation $Ax = b$ is typically associated\n",
    "with a *splitting* $A = M-N$, where $M$ is a matrix that is easy to\n",
    "solve (i.e. a triangular or diagonal matrix) and $N$ is everything else.\n",
    "In terms of the splitting, we can rewrite $Ax = b$ as \n",
    "$$\n",
    "  Mx = Nx + b,\n",
    "$$\n",
    "which is the fixed point equation for the iteration\n",
    "$$\n",
    "  Mx^{k+1} = Nx^{k} + b.\n",
    "$$\n",
    "We can equivalently write this iteration in terms of $M$ and $A$\n",
    "$$\n",
    "  x^{k+1} = x^k + M^{-1} (b-Ax^k).\n",
    "$$\n",
    "If we subtract the fixed point equation from\n",
    "the iteration equation, we have the error iteration\n",
    "$$\n",
    "  M e^{k+1} = N e^k\n",
    "$$\n",
    "or\n",
    "$$\n",
    "  e^{k+1} = R e^k, \\quad R = M^{-1} N.\n",
    "$$\n",
    "Note: This is exactly the same thing we saw with power iteration,\n",
    "except that we are not normalizing at every step!\n",
    "\n",
    "We’ve already seen one example of such an iteration (iterative refinement with\n",
    "an approximate factorization); in other cases, we might choose $M$ to be\n",
    "the diagonal part of $A$ (Jacobi iteration) or the upper or lower\n",
    "triangle of $A$ (Gauss-Seidel iteration). We will see in the next\n",
    "lecture that there is an alternate “matrix-free” picture of these\n",
    "iterations that makes sense in the context of some specific examples,\n",
    "but for analysis it is often best to think about the splitting picture.\n",
    "\n",
    "Let's see how we would actually implement something like this.  Usually, we\n",
    "don't care about forming $M$ explicitly, we just want a way to do solves with\n",
    "it.  Let's write the code to reflect that.  We'll provide implementations of\n",
    "solves for both Jacobi and Gauss-Seidel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function jacobi_Msolve(A)\n",
    "    d = diag(A)\n",
    "    return (x) -> x ./ d\n",
    "end\n",
    "\n",
    "function gs_Msolve(A)\n",
    "    L = tril(A)\n",
    "    return (x) -> L \\ x\n",
    "end\n",
    "\n",
    "function stationary(A, b, Msolve; rtol=1e-6, nsteps=100, monitor=(x, rnorm) -> nothing)\n",
    "    x = Msolve(b)\n",
    "    for k = 1:nsteps\n",
    "        r = b-A*x\n",
    "        rnorm = norm(r)\n",
    "        monitor(x, rnorm)\n",
    "        if rnorm < rtol * norm(b)\n",
    "            return x\n",
    "        end\n",
    "        x += Msolve(r)\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at a simple test matrix, and see if we get the right rate of convergence.\n",
    "We know from our analysis of the power iteration that the convergence ought to look\n",
    "something like\n",
    "$$\n",
    "  \\|e^{k+1}\\| \\approx \\rho(R) \\|e^k\\|,\n",
    "$$\n",
    "where $\\rho(R)$ is the *spectral radius*\n",
    "$$\n",
    "  \\rho(R) = \\max_{\\lambda \\in \\Lambda(R)} |\\lambda|.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = [4. 1. 0. ;\n",
    "     1. 4. 1. ;\n",
    "     0. 1. 4. ]\n",
    "Msolve = jacobi_Msolve(A)\n",
    "\n",
    "b = rand(3)\n",
    "xref = A\\b\n",
    "\n",
    "Mjacobi = diagm(diag(A))\n",
    "Rjacobi = Matrix{Float64}(I,3,3)-Mjacobi\\A\n",
    "lambda_max = maximum(abs.(eigvals(Rjacobi)))\n",
    "\n",
    "rhist = Array{Float64,1}([])\n",
    "stationary(A, b, Msolve, monitor=(x, rnorm) -> push!(rhist, rnorm))\n",
    "\n",
    "println(\"Supposed rate constant: $lambda_max\")\n",
    "println(\"Empirical rate constant: $(rhist[end]/rhist[end-1])\")\n",
    "\n",
    "plot(rhist, yscale=:log10, legend=false)\n",
    "plot!(lambda_max.^(1:length(rhist)), linestyle=:dash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convergence: Norms and Eigenvalues\n",
    "\n",
    "We consider two standard approaches to analyzing the convergence of a\n",
    "stationary iteration, both of which revolve around the error iteration\n",
    "matrix $R = M^{-1} N$. These approaches involve taking a norm inequality\n",
    "or using an eigenvalue decomposition. The first approach is often easier\n",
    "to reason about in practice, but the second is arguably more\n",
    "informative.\n",
    "\n",
    "For the norm inequality, note that if $\\|R\\| < 1$ for some operator\n",
    "norm, then the error satisfies\n",
    "$$\n",
    "  \\|e^{k+1}\\| \\leq \\|R\\| \\|e^k\\| \\leq \\|R\\|^{k+1} \\|e^0\\|.\n",
    "$$ Because\n",
    "$\\|R\\|^k$ converges to zero, the iteration eventually converges. As an\n",
    "example, consider the case where $A$ is *strictly row diagonally\n",
    "dominant* (i.e. the sum of the magnitudes of the off-diagonal\n",
    "elements in each row are less than the magnitude of the diagonal\n",
    "element), and let $M$ be the diagonal part of $A$ (Jacobi iteration). In\n",
    "that case, $\\|R\\|_\\infty = \\|M^{-1} N\\|_\\infty < 1$. Therefore, the\n",
    "infinity norm of the error is monontonically decreasing.\n",
    "\n",
    "Note that in finite-dimensional spaces, there is a property of “equivalence\n",
    "of norms” that says that convergence in one norm implies convergence\n",
    "in any other norm; however, this does *not* mean that monotone\n",
    "convergence in one norm implies monotone convergence in any other\n",
    "norm.\n",
    "\n",
    "The $A$ matrix that we saw above is diagonally dominant, so let's look\n",
    "at that as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist = Array{Float64,1}([])\n",
    "stationary(A, b, Msolve, monitor=(x, rnorm) -> push!(rhist, norm(b-A*x, Inf)))\n",
    "lambda_max2 = maximum(abs.(eigvals(Rjacobi)))\n",
    "\n",
    "println(\"Spectrial radius: $lambda_max2\")\n",
    "println(\"Norm of R: $(opnorm(Rjacobi, Inf))\")\n",
    "\n",
    "plot(rhist, yscale=:log10, legend=false)\n",
    "plot!(opnorm(Rjacobi, Inf).^(1:length(rhist)), linestyle=:dash)\n",
    "plot!(lambda_max2.^(1:length(rhist)), linestyle=:dash, color=:blue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from the plot, the convergence estimate we get by looking\n",
    "at $\\|R\\|_{\\infty}$ may be rather loose.\n",
    "\n",
    "Bounding by one the infinity norm (or two norm, or one norm) of the\n",
    "iteration matrix $R$ is *sufficient* to guarantee convergence, but\n",
    "not *necessary*. In order to completely characterize when stationary\n",
    "iterations converge, we need to turn to an eigenvalue decomposition.\n",
    "Suppose $R$ is diagonalizable, and write the eigendecomposition as\n",
    "$$\n",
    "  R = V \\Lambda V^{-1}.\n",
    "$$\n",
    "Now, note that $R^k = V \\Lambda^k V^{-1}$, and therefore\n",
    "$$\n",
    "  \\|e^k\\| = \\|R^k e^0\\| = \\|V \\Lambda^k V^{-1} e^0\\| \\leq \\kappa(V) \\rho(R)^k \\|e^0\\|,\n",
    "$$\n",
    "where $\\rho(R)$ is the *spectral radius* of\n",
    "$R$, i.e. $$\\rho(R) = \\max_{\\lambda \\mbox{ an eig}} |\\lambda|,$$ and\n",
    "$\\kappa(V) = \\|V\\| \\|V^{-1}\\|$. For a diagonalizable matrix, convergence\n",
    "of the iteration happens if and only if the spectral radius of $R$ is\n",
    "less than one. *But* that statement ignores the condition number of\n",
    "the eigenvector matrix! For highly “non-normal” matrices in which the\n",
    "condition number is large, the iteration may appear to make virtually no\n",
    "progress for many steps before eventually it begins to converge at the\n",
    "rate predicted by the spectral radius. This is consistent with the\n",
    "bounds that we can prove, but often surprises people who have not seen\n",
    "it before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  What is $\\|R\\|_\\infty$ for the example used above?  How does it compare to $\\rho(R) \\approx 0.35$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  In general, $\\rho(R) \\leq \\|R\\|$ for any operator norm.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A model problem\n",
    "\n",
    "For the sake of concreteness in our discussion of iterative linear solvers,\n",
    "let’s consider a standard model problem: a\n",
    "discretization of a Poisson equation on a line. That is, we approximate\n",
    "$$\n",
    "  -\\frac{d^2 u}{dx^2} = f, \\quad u(0) = u(1) = 0\n",
    "$$\n",
    "using the second-order finite difference approximation\n",
    "$$\\frac{d^2 u}{dx^2} \\approx \\frac{u(x-h)-2u(x)+u(x+h)}{h^2}$$ where $h$\n",
    "is a small step size. We discretize the problem by meshing $[0,1]$ with\n",
    "evenly spaced points $x_j = jh$ for $j = 0$ to $N+1$ where\n",
    "$h = 1/(N+1)$, then apply this approximation at each point. This\n",
    "procedure yields the equations\n",
    "$$\n",
    "  -u_{j-1}+2u_j-u_{j+1} = h^2 f_j, \\quad j = 1, \\ldots, N\n",
    "$$\n",
    "We can write this in matrix form as an $N \\times (N+2)$ linear systems\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "   -1 & 2 & -1 \\\\\n",
    "      & -1 & 2 & -1 \\\\\n",
    "      &    & \\ddots & \\ddots & \\ddots \\\\\n",
    "      &    &        & -1 & 2 & -1 \\\\\n",
    "      &    &        &    & -1 & 2 & -1\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} u_0 \\\\ u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{N-1} \\\\ u_N \\\\ u_{N+1} \\end{bmatrix} =\n",
    "  h^2 \\begin{bmatrix} f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_{N-1} \\\\ f_N \\end{bmatrix},\n",
    "$$\n",
    "or, in matrix form,\n",
    "$$\n",
    "  \\bar{T} \\bar{u} = h^2 f\n",
    "$$\n",
    "where $\\bar{T} \\in \\mathbb{R}^{N \\times (N+2)}$, $\\bar{u} \\in \\mathbb{R}^{N+2}$ is the vector of \n",
    "values $u_0, \\ldots, u_{N+1}$ (including at the boundaries) and $f \\in \\mathbb{R}^N$ denotes the \n",
    "vector of values  $f_1, \\ldots, f_N$ (only on interior points).  In order to get a square system,\n",
    "we use the boundary conditions on $u_0$ and $u_{N+1}$\n",
    "to move those pieces of the system to the right hand side\n",
    "$$\n",
    "  \\begin{bmatrix}\n",
    "    2 & -1 \\\\\n",
    "   -1 & 2 & -1 \\\\\n",
    "      & \\ddots & \\ddots & \\ddots \\\\\n",
    "      &        & -1 & 2 & -1 \\\\\n",
    "      &        &    & -1 & 2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} u_1 \\\\ u_2 \\\\ \\vdots \\\\ u_{N-1} \\\\ u_N \\end{bmatrix} =\n",
    "  h^2 \\begin{bmatrix} f_1 \\\\ f_2 \\\\ \\vdots \\\\ f_{N-1} \\\\ f_N \\end{bmatrix} \n",
    "  - u_0 \\begin{bmatrix} -1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ 0 \\end{bmatrix}\n",
    "  - u_{N+1} \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\\\ -1 \\end{bmatrix}.\n",
    "$$\n",
    "In our case, the boundary conditions are homogeneous ($u_0 = u_{N+1} = 0$), so\n",
    "in matrix notation we have $$Tu = h^2 f$$ where $u$ and $f$ are vectors in\n",
    "${\\mathbb{R}}^N$ representing the sampled (approximate) solution and the\n",
    "sampled forcing function on the interior mesh points.\n",
    "The matrix $T$ is a frequently-recurring model matrix, the tridiagonal \n",
    "$$\n",
    "  T =\n",
    "  \\begin{bmatrix}\n",
    "   2 & -1 \\\\\n",
    "  -1 &  2 & -1 \\\\\n",
    "     & -1 &  2 & -1 \\\\\n",
    "     &    & \\ddots & \\ddots & \\ddots \\\\\n",
    "     &    &        & -1 & 2 & -1 \\\\\n",
    "     &    &        &    & -1 & 2\n",
    "  \\end{bmatrix}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formT(N) = SymTridiagonal(2*ones(N), -ones(N-1))\n",
    "Ngrid = 11\n",
    "T = formT(Ngrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a vector of grid coordinates and spacing parameter\n",
    "xsall = range(0.0, stop=1.0, length=Ngrid+2)\n",
    "xs = xsall[2:end-1]\n",
    "h = 1/(Ngrid+1)\n",
    "\n",
    "# Form a right hand side and solve the PDE\n",
    "fs = 0.5 .- abs.(xs.-0.5)\n",
    "us = T\\(h^2*fs)\n",
    "\n",
    "# Plot the right hand side and u\n",
    "l = @layout [a b]    \n",
    "p1 = plot(xs, fs, label=\"f(x)\")\n",
    "p2 = plot(xs, us, label=\"u(x)\")\n",
    "plot(p1, p2, layout=l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting and sweeping\n",
    "\n",
    "Splitting is the right linear algebraic framework for discussing\n",
    "convergence of stationary methods, but it is not the way they are\n",
    "usually programmed. The connection between a matrix splitting and a\n",
    "“sweep” of a stationary iteration like Gauss-Seidel or Jacobi iteration\n",
    "is not always immediately obvious, and so it is probably worth spending\n",
    "a moment or two explaining in more detail.\n",
    "\n",
    "Suppose we forgot about how cheap Gaussian elimination is for\n",
    "tridiagonal matrices. How might we solve this system of equations? A\n",
    "natural thought is that we could make an initial guess at the solution,\n",
    "then refine the solution by “sweeping” over each node $j$ and adjusting\n",
    "the value at that node ($u_j$) to be consistent with the values at\n",
    "neighboring nodes. In one sweep, we might compute a new set of values\n",
    "$u^{\\mathrm{new}}$ from the old values $u^{\\mathrm{old}}$, \n",
    "or we might update the values for each node in turn, using the most\n",
    "recent estimate for each update.  These are, respectively, a step of\n",
    "*Jacobi* iteration and a step of *Gauss-Seidel* iteration, which are two\n",
    "standard stationary methods.\n",
    "\n",
    "It's convenient to do this with the full versions of the vectors (i.e.\n",
    "including the boundary values).  Julia being Julia, we could define a\n",
    "new type to make it simpler to iterate over interior points alone,\n",
    "but we will not bother to do so here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function jacobi_sweep!(unew, uold, f, h)\n",
    "    Ngrid= length(unew)-2\n",
    "    for jj = 1:Ngrid\n",
    "        j = jj+1  # Adjust offset since Julia array default to 1-based indexing\n",
    "        unew[j] = (h^2*f[j] + uold[j+1] + uold[j-1])/2\n",
    "    end\n",
    "end\n",
    "\n",
    "function gs_sweep!(u, f, h)\n",
    "    Ngrid = length(unew)-2\n",
    "    for jj = 1:Ngrid\n",
    "        j = jj+1\n",
    "        u[j] = (h^2*f[j] + u[j+1] + u[j-1])/2\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this model problem, each Gauss-Seidel step gives us the same benefit as about two\n",
    "Jacobi steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fall = [0; fs; 0]\n",
    "uref = [0; us; 0]\n",
    "uhat = [0; h^2*fs; 0]\n",
    "ugs  = copy(uhat)\n",
    "unew = zeros(Ngrid+2)\n",
    "\n",
    "err_j  = zeros(100)\n",
    "err_gs = zeros(100)\n",
    "\n",
    "anim = @animate for sweep = 1:100\n",
    "    jacobi_sweep!(unew, uhat, fall, h)\n",
    "    gs_sweep!(ugs, fall, h)\n",
    "    err_j[sweep] = norm(unew-uref)\n",
    "    err_gs[sweep] = norm(ugs-uref)\n",
    "    plot(xsall, uref, linewidth=2, color=:black, label=\"Reference solution\")\n",
    "    plot!(xsall, unew, linecolor=:blue, label=\"Jacobi\")\n",
    "    plot!(xsall, ugs, linecolor=:red, label=\"Gauss-Seidel\")\n",
    "    uhat[:] = unew\n",
    "end\n",
    "gif(anim, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(err_j, yscale=:log10, linecolor=:blue, xlabel=\"Sweeps\", ylabel=\"Error\", label=\"Jacobi\")\n",
    "plot!(err_gs, yscale=:log10, linecolor=:red, label=\"Gauss-Seidel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How should we relate the “sweep” picture to a matrix splitting? The\n",
    "update equation from step $k$ to step $k+1$ in Jacobi is\n",
    "$$\n",
    "  -u_{j-1}^{(k)}+2u_j^{(k+1)}-u_{j+1}^{(k)} = h^2 f_j,\n",
    "$$\n",
    "while the Gauss-Seidel update is\n",
    "$$\n",
    "  -u_{j-1}^{(k+1)}+2u_j^{(k+1)}-u_{j+1}^{(k)} = h^2 f_j.\n",
    "$$\n",
    "In terms of splittings, this means that Jacobi corresponds to taking $M$ to be the\n",
    "diagonal part of the matrix, \n",
    "$$\n",
    "  M =\n",
    "  \\begin{bmatrix}\n",
    "   2 &    \\\\\n",
    "     &  2 &    \\\\\n",
    "     &    &  2 &    \\\\\n",
    "     &    &        & \\ddots &  \\\\\n",
    "     &    &        &    & 2 &  \\\\\n",
    "     &    &        &    &   & 2\n",
    "  \\end{bmatrix}, ~~\n",
    "  N =\n",
    "  \\begin{bmatrix}\n",
    "   0 &  1 \\\\\n",
    "   1 &  0 &  1 \\\\\n",
    "     &  1 &  0 & 1 \\\\\n",
    "     &    & \\ddots & \\ddots & \\ddots \\\\\n",
    "     &    &        & 1 & 0 & 1 \\\\\n",
    "     &    &        &   & 1 & 0\n",
    "  \\end{bmatrix},\n",
    "$$ while Gauss-Seidel corresponds to taking $M$ to be the\n",
    "lower triangle of the matrix, \n",
    "$$\n",
    "  M =\n",
    "  \\begin{bmatrix}\n",
    "   2 &  \\\\\n",
    "  -1 &  2 &  \\\\\n",
    "     & -1 &  2 &  \\\\\n",
    "     &    & \\ddots & \\ddots &  \\\\\n",
    "     &    &        & -1 & 2 &  \\\\\n",
    "     &    &        &    & -1 & 2\n",
    "  \\end{bmatrix}, ~~\n",
    "  N =\n",
    "  \\begin{bmatrix}\n",
    "   0 &  1 \\\\\n",
    "     &  0 &  1 \\\\\n",
    "     &    &  0 & 1 \\\\\n",
    "     &    &    & \\ddots & \\ddots \\\\\n",
    "     &    &        &  & 0 & 1 \\\\\n",
    "     &    &        &    &   & 0\n",
    "  \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "The point of this exercise is that *programming* stationary\n",
    "iterative methods and *analyzing* the same methods may lead\n",
    "naturally to different ways of thinking about the iterations. It’s\n",
    "worthwhile practicing mapping back and forth between these two modes of\n",
    "thought."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Write a code to plot the convergence of Jacobi and Gauss-Seidel in the case when the right hand side $f$ is zero, but the boundary conditions are $u_0 = 0$ and $u_{N+1} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Based on the experiment above, do you have any comments on how you expect the convergence rate to behave as a function of $N$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Solves and Quadratic Minimization\n",
    "\n",
    "We have already briefly described an argument that Jacobi iteration\n",
    "converges for strictly row diagonally dominant matrices. We now discuss\n",
    "an argument that Gauss-Seidel converges (or at least part of such an\n",
    "argument). In the process, we will see a useful way of reformulating the\n",
    "solution of symmetric positive definite linear systems that will prepare\n",
    "us for our upcoming discussion of conjugate gradient methods.\n",
    "\n",
    "Let $A$ be a symmetric positive definite matrix, and consider the\n",
    "“energy” function $$\\phi(x) = \\frac{1}{2} x^T A x - x^T b.$$ The\n",
    "stationary point for this function is the point at which the derivative\n",
    "in any direction is zero. That is, for any direction vector $u$,\n",
    "$$\\begin{aligned}\n",
    "  0\n",
    "  &=\\left. \\frac{d}{d\\epsilon} \\right|_{\\epsilon = 0} \\phi(x+\\epsilon u) \\\\\n",
    "  &=\\frac{1}{2} u^T A x + \\frac{1}{2} x^T A u -  u^T b \\\\\n",
    "  &=u^T (Ax-b)\n",
    "\\end{aligned}$$\n",
    "Except in pathological instances, a\n",
    "directional derivative can be written as the dot product of a direction\n",
    "vector and a gradient; in this case, we have \n",
    "$$\n",
    "  \\nabla \\phi = Ax-b.\n",
    "$$\n",
    "Hence, minimizing $\\phi$ is equivalent to solving $Ax = b$.\n",
    "\n",
    "Now that we have a characterization of the solution of $Ax = b$ in terms\n",
    "of an optimization problem, what can we do with it? One simple approach\n",
    "is to think of a sweep through all the unknowns, adjusting each variable\n",
    "in term to minimize the energy; that is, we compute a correction\n",
    "$\\Delta x_j$ to node $j$ such that\n",
    "$$\\Delta x_j = \\operatorname{argmin}_z \\phi(x+z e_j)$$ Note that\n",
    "$$\\frac{d}{dz} \\phi(x+z e_j) = e_j^T (A(x+ze_j)-b),$$ and the update\n",
    "$x_j := x_j + \\Delta x_j$ is equivalent to choosing a new $x_j$ to set\n",
    "this derivative equal to zero. But this is exactly what the Gauss-Seidel\n",
    "update does! Hence, we can see Gauss-Seidel in two different ways: as a\n",
    "stationary method for solving a linear system, or as an optimization\n",
    "method that constantly makes progress toward a solution that minimizes\n",
    "the energy. The latter perspective can be turned (with a little\n",
    "work) into a convergence proof for Gauss-Seidel on positive-definite\n",
    "linear systems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
