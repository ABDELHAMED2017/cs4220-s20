{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-13\n",
    "\n",
    "## Nonlinear equations and optimization\n",
    "\n",
    "For the remainder of the semester, we will be discussing methods for solving\n",
    "nonlinear systems of equations and multivariate optimization problems.\n",
    "We will devote most of our attention to four related problem classes:\n",
    "$$\\begin{aligned}\n",
    "  f(x) = 0, & \\quad f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^n \\\\\n",
    "  \\min_x f(x), & \\quad f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}} \\\\\n",
    "  \\min_x \\|f(x)\\|^2, & \\quad f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^m  \\\\\n",
    "  f(x(s),s) = 0, & \\quad f : {\\mathbb{R}}^n \\times R \\rightarrow {\\mathbb{R}}^n\n",
    "\\end{aligned}$$\n",
    "We treat these problems as a unified\n",
    "group because the solution methods employ many of the same techniques,\n",
    "and insights gained from one problem can be applied to another. For\n",
    "example:\n",
    "\n",
    "-   We can turn the nonlinear system problem  into a non-negative least\n",
    "    squares problem  problem by observing $f(x) = 0$ iff\n",
    "    $\\|f(x)\\|^2 = 0$.\n",
    "\n",
    "-   The nonlinear least squares problem is a special case of the more\n",
    "    general unconstrained optimization problem . We consider it as a\n",
    "    special case because we can apply ideas for solving *linear*\n",
    "    least squares problem to the nonlinear case.\n",
    "\n",
    "-   For differentiable functions, the minima we seek in the optimization\n",
    "    problem  must occur at points where the gradient is zero, also known\n",
    "    as *stationary points* or *critical points*. We find these\n",
    "    points by solving a system of nonlinear equations.\n",
    "\n",
    "-   We might introduce parameter dependence to understand the\n",
    "    physics of a problem or as a mechanism to “sneak up” on the solution\n",
    "    to otherwise hard problems.\n",
    "\n",
    "In general, we will look to an optimization formulation as a way of\n",
    "judging progress, even if we are solving nonlinear equations. But in\n",
    "constructing algorithms, we will often look at things from the\n",
    "perspective of solving nonlinear systems of equations. Whatever approach\n",
    "we use, the numerical linear algebra tools from the start of the\n",
    "semester will play a central role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Give linear or quadratic examples of each of the classes of problems described above,\n",
    "    along with a comment about ways you know to solve them from earlier in the class."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The big ideas\n",
    "\n",
    "While we will see many technical tricks in the next month, I claim two\n",
    "as fundamental:\n",
    "\n",
    "#### Fixed point iterations\n",
    "\n",
    "All our nonlinear solvers will be *iterative*. We can write most as\n",
    "*fixed point iterations*\n",
    "$$\n",
    "  x^{k+1} = G(x^k),\n",
    "$$ which we hope will converge to a fixed point, i.e. $x^* = G(x^*)$. \n",
    "We often approach convergence analysis through the *error iteration* relating the error\n",
    "$e^k = x^k-x^*$ at successive steps: \n",
    "$$\n",
    "  e^{k+1} = G(x^* + e^k)-G(x^*).\n",
    "$$\n",
    "We have already seen this paradigm when we discussed\n",
    "stationary methods for solving linear systems and when\n",
    "we discussed fixed poiint iterations in one dimension.\n",
    "\n",
    "#### Model-based methods\n",
    "\n",
    "Most nonlinear problems are too hard to solve directly. On the other\n",
    "hand, we can *model* hard nonlinear problems by simpler (possibly\n",
    "linear) problems as a way of building iterative solvers. The most common\n",
    "tactic — but not the only one! — is to approximate the nonlinear\n",
    "function by a linear or quadratic function and apply all the things we\n",
    "know about linear algebra.\n",
    "\n",
    "If there is a third over-arching theme, it is *understanding problem\n",
    "structure*, whether to get good initial guesses for iterations, to\n",
    "obtain convergence proofs for methods, or to understand whether a\n",
    "(possibly non-unique) solution to a nonlinear system of equations or\n",
    "optimization problem is the “right” solution for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential calculus: a refresher\n",
    "\n",
    "We need a good foundation of multivariable differential calculus to\n",
    "construct iterations and to understand their convergence. While you\n",
    "should have this as background already, it is worth spending some time\n",
    "refreshing the concepts and the notation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From ${\\mathbb{R}}$ to ${\\mathbb{R}}^n$\n",
    "\n",
    "A lot of multivariable calculus involves applying concepts from calculus\n",
    "in one variable, one direction at a time. Suppose\n",
    "$f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^m$, and we want to\n",
    "understand the behavior of $f$ near $x \\in {\\mathbb{R}}^n$. We reduce to\n",
    "a one-dimensional problem by looking at the behavior along a direction\n",
    "$0 \\neq u \\in {\\mathbb{R}}^n$: \n",
    "$$\n",
    "  g(s) \\equiv f(x+su).\n",
    "$$\n",
    "The *directional derivative* of $f$ at $x$ in the direction $u$ is\n",
    "$$\\frac{\\partial f}{\\partial u}(x) =\n",
    "  g'(0) = \n",
    "  \\left. \\frac{d}{ds} \\right|_{s=0} f(x+su).\n",
    "$$\n",
    "If we cannot compute directional derivatives explicitly, \n",
    "we may choose to estimate them by a\n",
    "*finite difference approximation*, e.g.\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial u}(x) \\approx \\frac{f(x+hu)-f(x)}{h}\n",
    "$$\n",
    "for sufficiently small $h$. If $f$ is smooth enough, this formula has $O(h)$\n",
    "error. The most frequently used directional derivatives are the\n",
    "derivatives in the directions of the standard basis functions\n",
    "$e_1, \\ldots, e_n$; these are the partial derivatives $\\partial f /\n",
    "\\partial x_j$. We may also sometimes use the more compact notation\n",
    "$f_{i,j} \\equiv \\partial f_i / \\partial x_j$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example function\n",
    "f(x) = [ x[1] + 2*x[2] - 2 ; x[1]^2 + 4*x[2]^2 - 4 ]\n",
    "\n",
    "# Start point, direction, and slice of function\n",
    "# g(s) = f(x0+s*u) = [ x0[1] + 2*x0[2] - 2 + s*u[1] + 2*s*u[2] ;\n",
    "#                      (x0[1]+s*u[1])^2 + 4*(x[2]+s*u[2])^2 - 4 ]\n",
    "x0 = randn(2)\n",
    "u = randn(2)\n",
    "g(s :: Float64) = f(x0 + s*u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-3.0, 3.0, length=100)\n",
    "ss = range(-1.0, stop=1.0, length=100)\n",
    "ls = 10.0/norm(u)\n",
    "\n",
    "p1 = plot(xx, xx, (x, y) -> f([x; y])[2], st=:contourf)\n",
    "plot!([x0[1]-ls*u[1], x0[1]+ls*u[1]], \n",
    "      [x0[2]-ls*u[2], x0[2]+ls*u[2]], xlims=(-3, 3), \n",
    "      ylims=(-3,3), label=\"\\$x_0 + su\\$\")\n",
    "plot!([x0[1]], [x0[2]], marker=true, label=\"\\$x_0\\$\")\n",
    "\n",
    "p2 = plot(ss, [g(s)[2] for s in ss], label=\"\\$g(s)\\$\")\n",
    "plot(p1, p2, layout=(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually compute a directional derivative two ways\n",
    "dg0 = [u[1] + 2*u[2]; 2*x0[1]*u[1] + 8*x0[2]*u[2]]\n",
    "dg0_fd = (g(1e-6)-g(-1e-6))/2e-6\n",
    "\n",
    "# Print relative error between analytic soln and finite diff\n",
    "norm(dg0-dg0_fd)/norm(dg0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compute higher-order derivatives\n",
    "$$\\frac{\\partial^k f}{\\partial u^k}(x) =\n",
    "  g^{(k)}(0) =\n",
    "  \\left. \\frac{d^k}{ds^k} \\right|_{s=0} f(x+su),\n",
    "$$\n",
    "or we can compute mixed directional derivatives by differentiating\n",
    "$\\partial f/\\partial u$ in some new direction $v$. We say $f \\in C^k(\\Omega, {\\mathbb{R}}^m)$\n",
    "for some $\\Omega \\subset {\\mathbb{R}}^n$ if all directional derivatives\n",
    "of $f$ (pure or mixed) up to order $k$ exist and are continuous in\n",
    "$\\Omega$; or, equivalently, if all the partials up to order $k$ exist\n",
    "and are continuous in $\\Omega$. Sometimes the domain $\\Omega$ is clear\n",
    "from context; in this case, we will simply say that $f$ “is $C^k$.” We\n",
    "say a function is $C^0$ if it is continuous.\n",
    "\n",
    "If $f : \\mathbb{R}^n \\rightarrow \\mathbb{R^m}$ and there are\n",
    "$k+1$ continuous directional derivatives around $x$, we\n",
    "have the Taylor expansion \n",
    "$$\n",
    "  f(x+su)\n",
    "  = \\sum_{j=0}^k \\frac{g^{(j)}(0)}{j!} s^j + R_k(s) \\\\\n",
    "  = \\sum_{j=0}^k \\frac{1}{j!} \\frac{\\partial^j f}{\\partial u^j}(x) s^j + R_k(s)\n",
    "$$\n",
    "The remainder term $R_k(s)$ can be written in several forms, though we usually\n",
    "focus on either the integral form\n",
    "$$\n",
    "  R_k(s) = \\int_0^s \\frac{t^k}{k!} \\frac{\\partial^{k+1} f}{\\partial u^k}(t) \\, dt\n",
    "$$\n",
    "or (in the case $m = 1$) the so-called Lagrange form\n",
    "$$\n",
    "  R_k(s) = \\frac{\\partial^{k+1} f}{\\partial u^k} (\\xi)\n",
    "$$\n",
    "for some intermediate point $\\xi \\in [0,s]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check a directional second derivative:\n",
    "#\n",
    "# f(x) = [ x[1] + 2*x[2] - 2 ; x[1]^2 + 4*x[2]^2 - 4 ]\n",
    "# g''(0) = [0; 2*u[1]^2 + 8*u[2]^2]\n",
    "\n",
    "d2g0 = [0; 2*u[1]^2 + 8*u[2]^2]\n",
    "d2g0_fd = (g(-1e-4)-2*g(0.)+g(1e-4) )/1e-8\n",
    "norm(d2g0-d2g0_fd)/norm(d2g0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g0 = g(0.)\n",
    "gtaylor(s) = g0 + (dg0 + d2g0/2*s)*s\n",
    "\n",
    "plot(ss,  [g(s)[2] for s in ss],       label=\"Original\")\n",
    "plot!(ss, [gtaylor(s)[2] for s in ss], label=\"Taylor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  The Taylor series approximation and $g(s)$ lie directly atop each other in the plot above.  Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Suppose $f : \\mathbb{R} \\rightarrow \\mathbb{R}^m$ is twice differentiable.  Argue that\n",
    "    $$\n",
    "      \\|[f(0)+f'(0) s] - f(s)\\| \\leq \\frac{s^2}{2} \\left( \\max_{0 \\leq \\xi \\leq s} \\|f''(\\xi)\\| \\right).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives and approximation\n",
    "\n",
    "The function $f$ is *differentiable* at $x$ if there is a good\n",
    "affine (constant plus linear) approximation\n",
    "$$\n",
    "  f(x+z) = f(x) + f'(x) z + o(\\|z\\|),\n",
    "$$\n",
    "where the *Jacobian* $f'(x)$ (also writen $J(x)$ or $\\partial f/\\partial x$) is \n",
    "the $m \\times n$ matrix whose $(i,j)$ entry is the partial derivative\n",
    "$f_{i,j} = \\partial f_i / \\partial x_j$. If $f$ is differentiable, the\n",
    "Jacobian matrix maps directions to directional derivatives, i.e.\n",
    "$$\n",
    "  \\frac{\\partial f}{\\partial u}(x) = f'(x) u.\n",
    "$$\n",
    "If $f$ is $C^1$ in some open neighborhood of $x$, it is automatically differentiable.\n",
    "There are functions with directional derivatives that are not differentiable, but\n",
    "we will usually restrict our attention to $C^1$ functions if we use\n",
    "differentiability at all.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jacobian matrix\n",
    "df(x) = [ 1.0 2.0 ; 2*x[1]  8*x[2] ]\n",
    "\n",
    "# Sanity check that we get the right behavior in the u direction\n",
    "J0u = df(x0)*u\n",
    "norm(J0u - dg0)/norm(dg0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When multivariable calculus is taught to students without linear algebra\n",
    "as a prerequisite or co-requisite, the chain rule sometimes seems\n",
    "bizarre and difficult to remember. But once you think of derivatives as\n",
    "being about affine approximation, it becomes much simpler. Suppose\n",
    "$h = f \\circ g$ where $g : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^m$\n",
    "and $f : {\\mathbb{R}}^m \\rightarrow {\\mathbb{R}}^p$. Let $y = g(x)$, and\n",
    "consider first order approximations of $f$ and $g$ at $y$ and $x$,\n",
    "respectively: \n",
    "$$\\begin{aligned}\n",
    "  f(y+z) &= f(y) + f'(y) z + o(\\|z\\|) \\\\\n",
    "  g(x+w) &= g(x) + g'(x) w + o(\\|w\\|)\n",
    "\\end{aligned}$$ \n",
    "Then letting $z = g(x+w) - g(x) = g'(x) w + o(\\|w\\|)$, \n",
    "we have \n",
    "$$\\begin{aligned}\n",
    "  h(x+w)\n",
    "  &= f(y) + f'(y) (g'(x) w + o(\\|w\\|)) + o(\\|z\\|) \\\\\n",
    "  &= f(y) + f'(y) g'(x) w + o(\\|w\\|)\n",
    "\\end{aligned}$$\n",
    "Thus, we have $h'(x) = f'(y) g'(x)$; that is, the derivative of the composition is the\n",
    "composition of the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Consider the behavior of f on a circle\n",
    "gg(s :: Float64) = [cos(s); sin(s)]\n",
    "dgg(s :: Float64) = [-sin(s); cos(s)]\n",
    "\n",
    "ss = range(-Float64(π), Float64(π), length=100)\n",
    "p1 = plot(xx, xx, (x, y) -> f([x; y])[2], st=:contourf)\n",
    "plot!(cos.(ss), sin.(ss), label=\"\\$g(s)\\$\")\n",
    "\n",
    "p2 = plot(ss, [f(gg(s))[2] for s in ss], label=\"\\$h_2(s)\\$\")\n",
    "plot(p1, p2, layout=(2, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the chain rule vs finite differences\n",
    "\n",
    "h(s) = f(gg(s))\n",
    "dh(s) = df(gg(s)) * dgg(s)\n",
    "dh_fd(s) = (h(s+1e-4)-h(s-1e-4))/2e-4\n",
    "\n",
    "norm(dh(1.23)-dh_fd(1.23))/norm(dh(1.23))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot h vs linear approximation to h\n",
    "s0 = 1.23   # Reference point for linear approx of g\n",
    "g0 = gg(s0) # Reference point for linear approx of f\n",
    "\n",
    "# Affine approx to h is a composition of affine approximations to f, g\n",
    "ggtangent(s) = g0 + dgg(s0)*(s-s0)\n",
    "ftangent(x) = f(g0) + df(g0)*(x-g0)\n",
    "htangent(s) = ftangent(ggtangent(s))\n",
    "\n",
    "plot(ss, [h(s)[2] for s in ss], label=\"\\$h_2(s)\\$\")\n",
    "plot!(ss, [htangent(s)[2] for s in ss], label=\"\\$\\\\bar{h}_2(s)\\$\")\n",
    "plot!([s0], [h(s0)[2]], marker=true, label=\"\\$(s_0, h(s_0))\\$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Suppose $f : \\mathbb{R} \\rightarrow \\mathbb{R}^m$ is twice differentiable.  Argue that for any $u \\in \\mathbb{R}^m$, \n",
    "    there is a $\\xi \\in [0,s]$ such that\n",
    "    $$\n",
    "      u^T \\left( (f(0) + f'(0) s) - f(s) \\right) = u^T f''(\\xi).\n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A nest of notations\n",
    "\n",
    "A nice notational convention we have seen before, sometimes called\n",
    "*variational* notation (as in “calculus of variations”) is to write\n",
    "a relation between a first order change to $f$ and to $x$. If $f$ is\n",
    "differentiable at $x$, we write this as $$\\delta f = f'(x) \\, \\delta x$$\n",
    "where $\\delta$ should be interpreted as “first order change in.” In\n",
    "introductory calculus classes, this is sometimes called a *total\n",
    "derivative* or *total differential*, though there one usually\n",
    "uses $d$ rather than $\\delta$. There is a good reason for using $\\delta$\n",
    "in variational calculus, though, so that is typically what I do.\n",
    "\n",
    "I like variational notation because I find it more compact than many of\n",
    "the alternatives. For example, if $f$ and $g$ are both differentiable\n",
    "maps from ${\\mathbb{R}}^n$ to ${\\mathbb{R}}^m$ and $h = f^T g$, then I\n",
    "make fewer mistakes writing\n",
    "$$\n",
    "  \\delta h = (\\delta f)^T g + f^T (\\delta g), \\quad\n",
    "  \\delta f = f'(x) \\delta x, \\quad \\delta g = g'(x) \\delta x\n",
    "$$\n",
    "than when I write \n",
    "$$\n",
    "  h'(x) = g^T f'(x) + f^T g'(x)\n",
    "$$\n",
    "even though the the two are exactly the same. We could also write \n",
    "partial derivatives using indicial notation, e.g. \n",
    "$$\n",
    "  h_{,k} = \\sum_{i} (g_i f_{i,k} + g_{i,k} f_i).\n",
    "$$\n",
    "Similarly, I like to write the chain rule for $h = f \\circ g$ where\n",
    "composition makes sense as \n",
    "$$\n",
    "  \\delta h = f'(g(x)) \\delta g, \\quad\n",
    "  \\delta g = g'(x) \\delta x.\n",
    "$$\n",
    "But you could also write\n",
    "$$\n",
    "  h'(x) = f'(g(x)) g'(x)\n",
    "$$\n",
    "or\n",
    "$$\n",
    "  h_{i,k} = \\sum_{j} f_{i,j}(g(x)) g_{j,k}(x).\n",
    "$$\n",
    "I favor variational notation, but switch to alternate notations when it \n",
    "seems to simplify life (e.g. I often switch to indicial notation if I’m working \n",
    "on computational mechanics). You may use any reasonably sensible notation\n",
    "you want in your homework and projects, but should be aware that there\n",
    "is more than one notation out there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz functions\n",
    "\n",
    "A function $f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^m$ is\n",
    "*Lipschitz* with constant $M$ on $\\Omega \\subset {\\mathbb{R}}^n$ if\n",
    "$$\n",
    "  \\forall x, y \\in \\Omega, \\quad \\|f(x)-f(y)\\| \\leq M\\|x-y\\|.\n",
    "$$ \n",
    "Not every continuous function is Lipschitz; but if $\\Omega$ is bounded and\n",
    "closed, then any function $f \\in C^1(\\Omega, {\\mathbb{R}}^m)$ is\n",
    "Lipschitz with constant $M = \\max_{x \\in \\Omega} \\|f'(x)\\|$.\n",
    "\n",
    "Lipschitz constants will come up in several contexts when discussing\n",
    "convergence of iterations. For example, if\n",
    "$G : \\Omega \\rightarrow \\Omega$ is Lipschitz with some constant less\n",
    "than one on $\\Omega$, we call it a *contraction mapping*, and we can\n",
    "show that fixed point iterations with $G$ will converge to a unique\n",
    "fixed point in $\\Omega$. Lipschitz functions also give us a way to\n",
    "reason about approximation quality; for example, if $f'(x)$ is Lipschitz\n",
    "with constant $M$ on $\\Omega$ containing $x$, then we can tighten the\n",
    "usual asymptotic statement about linear approximation of $f$: if the\n",
    "line segment from $x$ to $x+z$ lives in $\\Omega$, then\n",
    "$$\n",
    "  f(x+z) = f(x) + f'(x) z + R(z), \\quad \\|R(z)\\| \\leq \\frac{M}{2} \\|z\\|^2.\n",
    "$$\n",
    "This also gives us a way to control the error in a finite difference\n",
    "approximation of $\\partial f/\\partial u$, for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Is $x \\mapsto \\sqrt{x}$ Lipschitz on $(0,1)$?  On $(1,\\infty)$?  If so, what are the Lipschitz constants?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Show that $x \\mapsto |x|$ is Lipschitz on $\\mathbb{R}$ with Lipschitz constant 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.  Show that if $f$ and $g$ are Lipschitz with constants $M$ and $N$, then $h = f \\circ g$ is Lipschitz with constant $MN$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratics and optimization\n",
    "\n",
    "We now consider the case where\n",
    "$f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}$. If $f$ is $C^1$ on a\n",
    "neighborhood of $x$, the derivative $f'(x)$ is a row vector, and we have\n",
    "$$\n",
    "  f(x+z) = f(x) + f'(x) z + o(\\|z\\|).\n",
    "$$\n",
    "The *gradient* $\\nabla f(x) = f'(x)^T$ points in the direction of \n",
    "steepest ascent for the affine approximation:\n",
    "$$\n",
    "  f(x+su) = f(x) + f'(x) u \\leq f(x) + \\|f'(x)\\| \\|z\\|\n",
    "$$\n",
    "with equality iff $z \\propto \\nabla f(x)$. Note that the gradient and the\n",
    "derivative are *not the same* – one is a row vector, the other a column vector!\n",
    "\n",
    "If $f'(x)$ is nonzero, there is always an ascent direction \n",
    "($\\nabla f(x)$) and a descent direction ($-\\nabla f(x)$) for $f$ starting at $x$.\n",
    "Therefore, if $f$ is $C^1$ then any minimum or maximum must be a\n",
    "*stationary point* or *critical point* where $f'(x) = 0$;\n",
    "equivalently, we could say a stationary point is where $\\nabla f(x) = 0$\n",
    "or where every directional derivative is zero. This fact is sometimes\n",
    "known as the *first derivative test*.\n",
    "\n",
    "If $f$ is a $C^2$ function, we can write a *second-order Taylor series*\n",
    "$$\n",
    "  f(x+z) = f(x) + f'(x) z + \\frac{1}{2} z^T H z + o(\\|z\\|^2)\n",
    "$$\n",
    "where $H$ is the symmetric *Hessian matrix* whose $(i,j)$ entry is the mixed\n",
    "partial $f_{,ij}$. We note in passing that if $f \\in C^3$, or even if\n",
    "$f \\in C^2$ and the second derivatives of $f$ are Lipschitz, then we\n",
    "have the stronger statement that the error term in the expansion is\n",
    "$O(\\|z\\|^3)$.\n",
    "\n",
    "If $x$ is a stationary point then the first-order term in this expansion\n",
    "drops out, leaving us with\n",
    "$$\n",
    "  f(x+z) = f(x) + \\frac{1}{2} z^T H z + o(\\|z\\|^2).\n",
    "$$\n",
    "The function has a strong local minimum or maximum at $x$ if the quadratic part does,\n",
    "i.e. if $H$ is positive definite or negative definite, respectively. If\n",
    "$H$ is strongly indefinite, with both positive and negative eigenvalues,\n",
    "then $x$ is a saddle point. This collection of facts is sometimes known\n",
    "as the *second derivative test*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Consider the function $$\\rho(x,y) = \\frac{\\alpha x^2 + 2 \\beta xy + \\gamma y^2}{x^2 + y^2}.$$\n",
    "    What equation characterizes the stationary points?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Argue that the Hessian of $\\rho$ is singular everywhere that $\\rho$ is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
