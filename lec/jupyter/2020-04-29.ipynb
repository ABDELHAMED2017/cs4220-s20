{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pacing the Path\n",
    "\n",
    "So far, we have focused on nonlinear equations\n",
    "($f : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}^n$) and optimization problems\n",
    "($\\phi : {\\mathbb{R}}^n \\rightarrow {\\mathbb{R}}$). Often, though,\n",
    "nonlinear equations and optimization problems depend on some extra\n",
    "parameter. For example:\n",
    "\n",
    "-   In fitting problems, we care about the solution as a function of a\n",
    "    regularization parameter.\n",
    "\n",
    "-   In population biology, we care about equilibrium population levels\n",
    "    as a function of various parameters: birth rates, death rates,\n",
    "    initial population sizes, etc.\n",
    "\n",
    "-   In mechanics, we care about the deformation of a structure as a\n",
    "    function of load.\n",
    "\n",
    "-   In chemical kinetics, we care about equilibrium chemical\n",
    "    concentrations as a function of temperature.\n",
    "\n",
    "-   In engineering problems involving a tradeoff between two parameters\n",
    "    (e.g. mass and stiffness), we care about the optimal setting of one\n",
    "    parameter given a fixed value of the other.\n",
    "\n",
    "-   In stochastic problems, we may care about the behavior as a function\n",
    "    of the variance of the noise term, or perhaps as a function of an\n",
    "    autocorrelation time.\n",
    "\n",
    "In each case, a solution path is an [implicit function](https://en.wikipedia.org/wiki/Implicit_function_theorem)\n",
    "of the extra parameter.  For these types of problems, *continuation* strategies are often a\n",
    "good choice. The basic picture in a continuation strategy for solutions\n",
    "of an equation $F(x(s),s) = 0$ where\n",
    "$F : {\\mathbb{R}}^n \\times {\\mathbb{R}}\\rightarrow {\\mathbb{R}}^n$\n",
    "starting from some easily computed solution $x(s_0)$ is:\n",
    "\n",
    "-   Given $x(s_j)$, choose a new $s' = s_j + \\Delta s$.\n",
    "\n",
    "-   *Predict* $x(s')$ based on the behavior at $s$. Two common\n",
    "    predictors are\n",
    "\n",
    "    -   *Trivial*: Guess $x(s') \\approx x(s_j)$.\n",
    "\n",
    "    -   *Euler:* Guess\n",
    "        $x(s') \\approx x(s_j) - \\frac{\\partial F}{\\partial x}(x_j,s_j)^{-1} \\frac{\\partial F}{\\partial s}(x_j,s_j)$.\n",
    "\n",
    "-   *Correct* by taking a few steps of Newton iteration.\n",
    "\n",
    "-   Either *accept* $s_{j+1} = s'$ and a corresponding $x(s_j)$ if\n",
    "    the Newton iteration converged, or try again with a smaller $\\Delta\n",
    "      s$. If the Newton iteration converges very quickly, we may\n",
    "    increase $\\Delta s$.\n",
    "\n",
    "Continuation is also natural if we really do care about a problem with\n",
    "no free parameters, but we lack a good initial guess with which to start\n",
    "an iterative method to solve the problem. In that case, a reasonable\n",
    "strategy is often to *introduce* a parameter $s$ such that the\n",
    "equations at $s = 0$ are easy and the equations at $s = 1$ are the ones\n",
    "that we would like to solve. Such a constructed path in problem space is\n",
    "sometimes called a *homotopy*. In many cases, one can show that\n",
    "solutions are continuous (though not necessarily differentiable)\n",
    "functions of the homotopy parameter, so that following a homotopy path\n",
    "with sufficient care can provide *all* solutions even for hard\n",
    "nonlinear problems. For this reason, homotopy methods are particularly\n",
    "effective for solving systems of polynomial equations. Another very\n",
    "popular family of homotopy methods are the interior point methods for\n",
    "constrained optimization problems, which we will touch on briefly next\n",
    "week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tough to Trace\n",
    "\n",
    "As a starting example, let’s consider a variation on the equation from\n",
    "one of our first nonlinear systems lectures, a discretization of the\n",
    "thermal blowup equation\n",
    "$$\n",
    "  \\frac{d^2 u}{dx^2} + \\exp(\\gamma u) = 0\n",
    "$$\n",
    "subject to $u(0) = u(1) = 0$. As before, we approximate the derivative \n",
    "with a mesh to get a system of equations of the form\n",
    "$$\n",
    "  h^{-2} \\left( u_{j-1}-2u_j+u_{j+1} \\right) + \\exp(\\gamma u_j) = 0\n",
    "$$\n",
    "where $u_j$ is the approximate solution at a mesh point $x_j = jh$ with\n",
    "$h = 1/(N+1)$. The boundary conditions are $u_0 = u_{N+1} = 0$, and the\n",
    "difference equations govern the behavior on the interior. Compared to\n",
    "the last time we saw this system, though, we have introduced a new\n",
    "feature: the rate constant $\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function autocatalytic(v, γ)\n",
    "    N = length(v)\n",
    "    fv        = exp.(γ*v)\n",
    "    fv        -= 2*(N+1)^2*v\n",
    "    fv[1:N-1] += (N+1)^2*v[2:N  ]\n",
    "    fv[2:N  ] += (N+1)^2*v[1:N-1]\n",
    "    fv\n",
    "end\n",
    "\n",
    "function Jautocatalytic(v, γ)\n",
    "    N = length(v)\n",
    "    SymTridiagonal(γ*exp.(γ*v) .- 2*(N+1)^2, (N+1)^2 * ones(N-1))\n",
    "end\n",
    "\n",
    "function dγ_autocatalytic(v, γ)\n",
    "    v .* exp.(γ*v)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When $\\gamma$ is equal to zero, the differential equation becomes\n",
    "$$\n",
    "  \\frac{d^2 u}{dx^2} + 1 = 0,\n",
    "$$\n",
    "which has the solution (subject to boundary conditions) \n",
    "$$\n",
    "  u(x; 0) = \\frac{1}{2} x(1-x).\n",
    "$$\n",
    "For larger values of $\\gamma$, things become more interesting.\n",
    "Based on physical reasoning, we expect the solutions to get more unstable (and harder) as\n",
    "$\\gamma$ grows. We therefore consider a strategy in which we\n",
    "incrementally increase $\\gamma$, at each point using a trivial predictor\n",
    "(the solution for the previous $\\gamma$) as an initial guess for a\n",
    "Newton iteration. If the Newton iteration does not converge in a few\n",
    "steps, we try again with a smaller step, stopping once the step size has\n",
    "become too small."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xall = range(0.0, 1.0, length=200)\n",
    "xint = xall[2:199]\n",
    "\n",
    "# Keep a record of parameters, step sizes, and center temperatures\n",
    "γs = []\n",
    "Δγs = []\n",
    "vcenter = []\n",
    "λs = []\n",
    "\n",
    "# Current solution + storage for Newton iterates\n",
    "v = xint.*(1.0 .- xint)/2\n",
    "vnew = copy(v)\n",
    "\n",
    "γ = 0.0\n",
    "Δγ = 0.1\n",
    "while Δγ >= 1e-6 && γ < 4.0\n",
    "    \n",
    "    # Run Newton iteration\n",
    "    converged = false\n",
    "    vnew[:] = v\n",
    "    for k = 1:10\n",
    "        fv = autocatalytic(vnew, γ)\n",
    "        vnew[:] -= Jautocatalytic(vnew, γ)\\fv\n",
    "        if norm(fv) < 1e-8\n",
    "            converged = true\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "    \n",
    "    # Record the step if converged; otherwise, cut the step size and try again\n",
    "    if converged\n",
    "        v[:] = vnew\n",
    "        push!(γs, γ)\n",
    "        push!(Δγs, Δγ)\n",
    "        push!(vcenter, v[99])\n",
    "        push!(λs, maximum(eigvals(Jautocatalytic(v, γ))))\n",
    "        γ += Δγ\n",
    "    else\n",
    "        γ -= Δγ\n",
    "        Δγ /= 2\n",
    "        γ += Δγ\n",
    "    end\n",
    "end\n",
    "\n",
    "# Say where we stopped and plot some diagnostics\n",
    "println(\"Last converged point at γ = $(γs[end])\")\n",
    "plot(γs, vcenter, xlabel=\"gamma\", ylabel=\"vcenter\", legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we get just past $\\gamma = 3.5$, the solution\n",
    "becomes more and more sensitive to small changes in $\\gamma$, and we\n",
    "have to take shorter steps in order to get convergence. This is\n",
    "reflective of an interesting physical phenomenon known as a bifurcation.\n",
    "Mathematically, what we see is the effect of the Jacobian becoming\n",
    "closer and closer to singular at the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(γs, λs, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Picking parameters\n",
    "\n",
    "In the previous section, we saw that continuation allowed us to march\n",
    "$\\gamma$ right up to some critical parameter, but not beyond. We can get\n",
    "a clearer picture of what is going on — and better solver stability — if\n",
    "we look at the same problem as a function of a *different parameter*\n",
    "In particular, let us consider controlling the midpoint\n",
    "value $\\mu$, and letting both $v$ and $\\gamma$ be implicit functions of\n",
    "the midpoint value. That is, we have the equations \n",
    "$$\n",
    "F(v,\\gamma; \\mu) =\n",
    "\\begin{bmatrix}\n",
    "  -h^{-2} T_N v + \\exp(\\gamma v) \\\\\n",
    "  e_{\\mathrm{mid}}^T v - \\mu\n",
    "\\end{bmatrix} = 0\n",
    "$$\n",
    "with the Jacobian matrix (with respect to $v$ and $\\gamma$)\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial (v,\\gamma)} =\n",
    "\\begin{bmatrix}\n",
    "  -h^2 T_n + \\gamma \\operatorname{diag}(\\exp(\\gamma v)) &\n",
    "  v \\odot \\exp(\\gamma v) \\\\\n",
    "  e_{\\mathrm{mid}}^T & 0\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where we use $a \\odot b$ to denote elementwise\n",
    "multiplication.  We use the same continuation process with a trivial\n",
    "predictor to trace out the behavior of the midpoint as a function of\n",
    "$\\gamma$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a record of parameters, step sizes, and center temperatures\n",
    "γs = []\n",
    "vcenter = []\n",
    "\n",
    "# Initial point\n",
    "v = xint.*(1.0 .- xint)/2\n",
    "μ = 0.125\n",
    "γ = 0.0\n",
    "\n",
    "emid = zeros(198)\n",
    "emid[99] = 1.0\n",
    "for μ in range(0.125, 2.125, length=200)\n",
    "    \n",
    "    # Run Newton iteration\n",
    "    converged = false\n",
    "    vγ = [v; γ]\n",
    "    for k = 1:10\n",
    "        fvγ = [autocatalytic(vγ[1:end-1], vγ[end]); vγ[99]-μ]\n",
    "        Jvγ = [Jautocatalytic(vγ[1:end-1], vγ[end])  dγ_autocatalytic(vγ[1:end-1], vγ[end]) ;\n",
    "               emid'                                 0.0                                    ]\n",
    "        vγ[:] -= Jvγ\\fvγ\n",
    "        if norm(fvγ) < 1e-8\n",
    "            converged = true\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Record\n",
    "    if converged\n",
    "        v[:] = vγ[1:end-1]\n",
    "        γ = vγ[end]\n",
    "        push!(γs, γ)\n",
    "        push!(vcenter, v[99])\n",
    "    else\n",
    "        println(\"Nonconvergence at $μ = μ\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "# Say where we stopped and plot some diagnostics\n",
    "plot(γs, vcenter, xlabel=\"gamma\", ylabel=\"vcenter\", legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This picture makes the behavior of the solution close to $\\gamma = 3.5$ a\n",
    "little more clear. The phenomenon shown is called a *fold bifurcation*. Physically,\n",
    "we have that for $\\gamma \\lesssim 3.5$, there are two distinct solutions\n",
    "(one stable and one unstable); as $\\gamma$ increases, these two\n",
    "solutions approach each other until at some critical $\\gamma$ value they\n",
    "“meet.” Beyond the critical value, there is no solution to the\n",
    "equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudoarclength Ideas\n",
    "\n",
    "What if we think we might run into a fold bifurcation, but do not know a\n",
    "good alternate parameter for continuation? A natural idea is to\n",
    "parameterize the solution curve (e.g. $(v(\\gamma),\\gamma)$) in terms of\n",
    "an *arclength* parameter. In practice, we do not care too much about\n",
    "exactly controlling arclength; it is just a mechanism to avoid picking\n",
    "parameters. Therefore, we pursue *pseudo-arclength* strategies as an\n",
    "alternative.\n",
    "\n",
    "For the simplest pseudo-arclength continuation strategy, consider a\n",
    "function $F : {\\mathbb{R}}^{n+1} \\rightarrow {\\mathbb{R}}^n$. Assuming\n",
    "the Jacobian has maximal rank, we expect there to be a solution curve\n",
    "$x : {\\mathbb{R}}\\rightarrow {\\mathbb{R}}^n$ such that $F(x(s)) = 0$.\n",
    "The null vector of the Jacobian $F'$ is tangent to $x$, and so we can\n",
    "use this to predict a new point. The basic procedure to get a new point\n",
    "on the curve starting from $x^j$ is then:\n",
    "\n",
    "-   Consider the Jacobian $F'(x^j) \\in {\\mathbb{R}}^{n \\times (n+1)}$\n",
    "    and compute a null vector $v$ (a simple approach is to compute a QR\n",
    "    factorization). Choose a tangent vector $t^j \\propto v$; usually we\n",
    "    normalize so that $t^{j-1} \\cdot t^j > 0$.\n",
    "\n",
    "-   Move a short distance along the tangent direction (Euler predictor),\n",
    "    or otherwise predict a new point.\n",
    "\n",
    "-   Correct back to the curve.  One approach is the iteration\n",
    "    $$y^{k+1} = y^k - F'(y^k)^\\dagger F(y^k)$$ where\n",
    "    $F'(y^k)^\\dagger \\in {\\mathbb{R}}^{(n+1) \\times n}$ is the\n",
    "    pseudoinverse of the Jacobian. This is equivalent to solving the\n",
    "    problem\n",
    "    $$\n",
    "      \\mbox{minimize } \\|p^k\\|^2 \\mbox{ s.t. } F'(y^k) p^k = -F(y^k).\n",
    "    $$\n",
    "\n",
    "-   If the iteration curves and the new point is OK, accept the point\n",
    "    and move on. Otherwise, reject the point and try again with a\n",
    "    shorter step in the tangent direction.\n",
    "\n",
    "Depending on what linear algebra tools are available to you, you may choose\n",
    "different strategies to compute the tangent vector or correct back to the\n",
    "curve.  Indeed, some peculiarities in the Julia sparse rectangular solvers\n",
    "leads us to prefer something different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep a record of parameters, step sizes, and center temperatures\n",
    "γs = []\n",
    "vcenter = []\n",
    "\n",
    "# Initial point\n",
    "v = xint.*(1.0 .- xint)/2\n",
    "γ = 0.0\n",
    "μ = 0.125\n",
    "\n",
    "# Indicator for last column (used for tangent vec)\n",
    "eγ = zeros(199)\n",
    "eγ[end] = 1\n",
    "\n",
    "# Take an initial vector to establish direction to move on curve\n",
    "tprev = zeros(199)\n",
    "tprev[99] = 1\n",
    "\n",
    "while v[99] <= 2.0\n",
    "    \n",
    "    # Compute a tangent vector in the same direction as before\n",
    "    J = [Jautocatalytic(v, γ)  dγ_autocatalytic(v, γ) ; tprev' ]\n",
    "    t = J\\eγ\n",
    "    t /= norm(t)\n",
    "    tprev[:] = t\n",
    "\n",
    "    # Take Euler predictor step\n",
    "    h = 0.1\n",
    "    vγ = [v; γ] + h*t\n",
    "    \n",
    "    # Correct back to curve\n",
    "    converged = false\n",
    "    for k = 1:10\n",
    "        fvγ = autocatalytic(vγ[1:end-1], vγ[end])\n",
    "        Jvγ = [Jautocatalytic(vγ[1:end-1], vγ[end])  dγ_autocatalytic(vγ[1:end-1], vγ[end]) ; t']\n",
    "        vγ[:] -= Jvγ\\[fvγ; 0]\n",
    "        if norm(fvγ) < 1e-8\n",
    "            converged = true\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "\n",
    "    # Record\n",
    "    if converged\n",
    "        v[:] = vγ[1:end-1]\n",
    "        γ = vγ[end]\n",
    "        push!(γs, γ)\n",
    "        push!(vcenter, v[99])\n",
    "    else\n",
    "        println(\"Nonconvergence in corrector\")\n",
    "        break\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "# Say where we stopped and plot some diagnostics\n",
    "plot(γs, vcenter, xlabel=\"gamma\", ylabel=\"vcenter\", legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## And Points Beyond\n",
    "\n",
    "There is a large and fascinating literature on numerical continuation\n",
    "methods and on the numerical analysis of implicitly defined functions.\n",
    "Beyond the predictor-corrector methods that we have described, there are\n",
    "various other methods that address similar problems: piecewise linear\n",
    "(simplex) continuation, pseudo-transient continuation, and so forth. We\n",
    "can combine continuation ideas with all the other ideas that we have\n",
    "described in the course; for example, one can do clever things with\n",
    "Broyden updates as one walks along the curve. We can also apply step\n",
    "control techniques that some of you may have learned in a class like CS\n",
    "4210 in the context of methods for solving ordinary differential\n",
    "equations.\n",
    "\n",
    "A little knowledge of continuation methods can take you a long way, but\n",
    "if you would like to know more, I recommend [*Introduction to Numerical\n",
    "Continuation Methods*](https://doi.org/10.1137/1.9780898719154) by Allgower and Georg."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "What would you like clarified in these notes?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
