{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-05-08\n",
    "\n",
    "Note: I have [review notes](https://www.cs.cornell.edu/courses/cs4220/2017sp/lec/review.pdf)\n",
    "from previous semesters that do a pretty good job of covering things.\n",
    "I do not plan to put those into notebook format, at least not right now. \n",
    "But I recommend them to you for a written \"big picture\" view.\n",
    "\n",
    "## Logistics\n",
    "\n",
    "- Will close everything by end of May 11\n",
    "- Final will be May 11-May 21 (*new end date*)\n",
    "- Early submissions are appreciated!\n",
    "- Final will be *optional* for S/U (will average rest of grades)\n",
    "- Final still required for letter grade\n",
    "- Final covers whole course\n",
    "  - But this mostly means \"the second half relies on the first half!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The solver stack\n",
    "\n",
    "Big numerical codes come in layers, e.g.\n",
    "\n",
    "- A continuation strategy to do a parameter study\n",
    "- A Newton iteration (or related iteration) to solve nonlinear subproblems\n",
    "- A Krylov solver (or related iteration) for Jacobian solves\n",
    "- A sparse matrix factorization to precondition the Krylov solver\n",
    "- Dense factorizations used inside the sparse solver\n",
    "\n",
    "You usually don't have to implement every layer.  But you have to understand accuracy/stability and performance\n",
    "implications of the choices at each layer in order to make things work correctly.\n",
    "\n",
    "For those of you who are interested in the high-performance computing and systems aspect of building such solver\n",
    "stacks, I will teach CS 5220 (Applications of Parallel Computers) in the Fall 2020 semester."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The nutshell\n",
    "\n",
    "We did plenty of calculus and linear algebra foundations in this class, but the main \"meat\" of the course\n",
    "is in building factorizations and iterations to solve different linear and nonlinear problems.\n",
    "\n",
    "### Factorizations\n",
    "\n",
    "We talked about many factorizations, including\n",
    "\n",
    "- LU factorization / Gaussian elimination\n",
    "- Cholesky factorization\n",
    "- QR factorization\n",
    "- Singular value decomposition\n",
    "- Schur decomposition and symmetric eigendecomposition\n",
    "\n",
    "The first three we actually spent a lot of time learning to compute.  We mostly thought about how to use\n",
    "the latter two.  For most of the linear algebra problems that we talked about in the class, from solving linear systems\n",
    "and least squares to determinants and data compression, we can use one or more of these factorizations.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "How could you compute $\\log |\\det(A)|$ using each of the factorizations mentioned above?  Why might computing\n",
    "$\\det(A)$ in general be numerically tricky?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterations\n",
    "\n",
    "Though this is not the only class of iterations we discussed, our chief iteration\n",
    "building block is *fixed point iterations*\n",
    "$$\n",
    "  x_{k+1} = G(x_k).\n",
    "$$\n",
    "The most important of these is probably Newton's iteration.\n",
    "\n",
    "The general challenges in using iterative methods are:\n",
    "\n",
    "- Getting good enough initial guesses\n",
    "- Deciding when to terminate\n",
    "- Keeping the cost per step low\n",
    "- Keeping the number of steps low\n",
    "\n",
    "The goal is \"right enough, fast enough.\"  What meets those goals is often application specific.\n",
    "Hence, being an informed user of iterative methods often requires more monkeying around than\n",
    "when we solve standard linear algebra problems.  Of course, if someone else has solved a problem\n",
    "for you, that's great (and this is often the case for 1D nonlinear equation solving, for example).\n",
    "\n",
    "#### Questions\n",
    "\n",
    "Suppose $G$ is Lipschitz with constant $\\alpha < 1$ on all of $\\mathbb{R}^n$.\n",
    "How can you argue that the iteration converges to a fixed point $x_*$?  Given an initial bound on $\\|x_0-x_*\\|$,\n",
    "how would you decide how many steps you'd probably need to achieve a given (absolute) error tolerance $\\tau$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Linear algebra\n",
    "\n",
    "You should know about\n",
    "\n",
    "- Concrete and abstract vector spaces\n",
    "  - Standard examples ($\\mathbb{R}^n$, $\\mathcal{P}_d$)\n",
    "  - Norms\n",
    "  - Inner products\n",
    "- Matrices and their interpretations\n",
    "  - Mappings between vector spaces\n",
    "  - Operators on a vector space\n",
    "  - Quadratic forms on a vector space\n",
    "- Block matrices\n",
    "- Matrix norms and operator norms\n",
    "- Matrix vector products (and how to do them fast)\n",
    "- Singular value decomposition and eigenvalues\n",
    "\n",
    "#### Questions\n",
    "\n",
    "What is the Frobenius norm in terms of the SVD?  What is the 2-norm?  Why can the Frobenius norm not be a 2-norm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculus\n",
    "\n",
    "You should know about\n",
    "\n",
    "- Taylor's theorem in 1D\n",
    "- Multivariate Taylor's theorem (out to first or second order)\n",
    "  - Jacobians, gradients, and Hessians, oh my!\n",
    "- Variational notation and matrix calculus\n",
    "\n",
    "#### Questions\n",
    "\n",
    "What is $\\frac{d}{ds}\\left|_{s=0} (A+sE)^{-1} \\right.$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CS background\n",
    "\n",
    "You should know about\n",
    "\n",
    "- Order notation and performance\n",
    "- A little graph theory and how it relates to sparse matrices\n",
    "\n",
    "And, of course, how to program and debug.\n",
    "\n",
    "#### Question\n",
    "\n",
    "What is the graph associated with a tridiagonal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error analysis concepts\n",
    "\n",
    "You should know different ways of thinking about error\n",
    "\n",
    "- Forward error\n",
    "- Backward error / residual error\n",
    "- Normwise vs componentwise error\n",
    "- Absolute vs relative error\n",
    "\n",
    "You should also know about condition numbers and the concept of backward stability.\n",
    "\n",
    "We are not going to drill down on floating point error analysis, but you should have some\n",
    "appreciation for how numbers are represented and the types of things that can go wrong\n",
    "when you compute in floating point.\n",
    "\n",
    "#### Question\n",
    "\n",
    "What is the condition number for the problem of solving $f(x) = 0$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear systems\n",
    "\n",
    "For this part of the class, we talked about $Ax = b$ where $A \\in \\mathbb{R}^{n \\times n}$ is nonsingular.\n",
    "We mostly considered *direct* solvers based on factorization.  This was our first real experience using the\n",
    "concepts of condition numbers to understand error propagation; in particular, we saw\n",
    "$$\n",
    "  \\frac{\\|\\hat{x}-x\\|}{\\|x\\|} \\leq \\kappa(A) \\frac{\\|E\\|}{\\|A\\|} + O(\\|E\\|^2),\n",
    "$$\n",
    "and that we can understand the rounding in a process like Gaussian elimination in terms of the backward error $\\|E\\|$.\n",
    "\n",
    "We saw several ways of thinking about Gaussian elimination, with the Schur complement playing a central role.  We also\n",
    "saw the related concept of Cholesky factorization.  I talked some about sparse solvers, and a\n",
    "lot about the importance of re-using factorization work and putting parens in the right order.\n",
    "We also saw iterative refinement, which was our first example of a fixed point iteration in the class.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "Suppose $H = R^T R$ is a Cholesky factorization.  How would you add a row/column to $H$ and update the factorization\n",
    "in $O(n^2)$ time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least squares\n",
    "\n",
    "The problem of least squares is a natural follow-up for the problem of linear systems.  We talked about different\n",
    "ways of deriving / thinking about the normal equations and the Moore-Penrose pseudoinverse in the full-rank case;\n",
    "the QR factorization and the SVD played substantial roles.  We briefly discussed the perturbation theory for least\n",
    "squares problems, which is much more complicated than in the linear systems case.\n",
    "We also talked about the idea of regularization, with a particular emphasis on Tikhonov regularization (though\n",
    "we also discussed factor selection, the lasso, and truncated SVD). \n",
    "\n",
    "#### Question\n",
    "\n",
    "How could you write the Moore-Penrose pseudoinverse in terms of the (economy) QR factorization or SVD?\n",
    "How could you compute the L-curve used to look at $\\|x_\\lambda\\|$ vs $\\|r_\\lambda\\|$ using these factorizations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues\n",
    "\n",
    "We talked a bit about the Jordan form, the Schur form, the spectral mapping theorem, and various applications of\n",
    "eigenvalue comptuations before we moved on to algorithms.  In terms of algorithms, we built up\n",
    "\n",
    "- Power iteration\n",
    "- Then inverse iteration\n",
    "- Then shift-invert\n",
    "- Then Rayleigh quotient iteration\n",
    "- Then subspace iteration\n",
    "- Then QR iteration\n",
    "- Then Hessenberg reduction\n",
    "- And finally, all the ingredients to gether in the Francis double-shift QR\n",
    "\n",
    "We did not talk as much about algorithms for the symmetric eigenvalue problem, except to point out that the\n",
    "Rayleigh quotient plays a central role.\n",
    "\n",
    "#### Question\n",
    "\n",
    "If $f(z)$ is an analytic function and $f(A)$ makes sense (e.g. there is a power series representation),\n",
    "argue that $f(A) v = 0$ for $v \\neq 0$ iff there is at least one eigenvalue $\\lambda$ of $A$ such that\n",
    "$f(\\lambda) = 0$.  If this is an isolated case (a single simple eigenvalue is involved),\n",
    "how could you extract $\\lambda$ knowing only $A$ and $v$ without reference to $f$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterations for linear systems\n",
    "\n",
    "We talked about two flavors of iterations.  *Stationary* iterations can be analyzed via a matrix splitting:\n",
    "$$\n",
    "  Ax = (M-K)x = b,\n",
    "$$\n",
    "and the iteration becomes\n",
    "$$\n",
    "  M x_{k+1} = K x_k + b.\n",
    "$$\n",
    "The matrix $M$ should be easy to solve with.  Methods like Gauss-Seidel and Jacobi just take pieces of the matrix\n",
    "(a triangular part or diagonal part).  The solve with $M$ is often \"implicit\" -- it looks like a so-called sweeping\n",
    "computation rather than an explicit matrix formation and solve.  Convergence is determined by $R = M^{-1} K$; we\n",
    "usually have to understand structural features of $A$ and $M$ in order to think about how to actually prove convergence.\n",
    "We gave a couple examples: Jacobi iteration with diagonally dominant matrices and Gauss-Seidel with SPD matrices.\n",
    "Often, though, the easiest way to determine convergence on any particular problem is to run the iteration and\n",
    "see if it converges.  We then briefly discussed CG and Krylov subspace solvers.  These are hard to ask questions about\n",
    "in a class like this.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "If $\\|R\\| \\leq \\alpha$, how many steps are needed to achieve a relative residual $\\|r\\|/\\|b\\| < \\tau$ for a given\n",
    "tolerance $\\tau$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solvers in 1D\n",
    "\n",
    "We have several different strategies in 1D for nonlinear equations:\n",
    "\n",
    "- Bisection\n",
    "- Newton iteration (and more general fixed point)\n",
    "- Secant iteration\n",
    "- Safeguarded methods (like Brent) that go between bisection and Newton/secant/etc\n",
    "\n",
    "We also talked briefly about 1D optimization and golden section search.\n",
    "\n",
    "We spent a long time writing error iterations over and over again and making semi-log plots of\n",
    "convergence.  These are clearly things that you should know how to do going forward!\n",
    "\n",
    "#### Questions\n",
    "\n",
    "What is the Newton iteration for computing $\\sqrt{a}$?  For $a \\geq 1$, can you argue that iteration from the\n",
    "initial guess $(1+a)/2$ always converges?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear equations and optimization in $\\mathbb{R}^n$\n",
    "\n",
    "In more than one dimension, things are harder.  We no longer have bisection, and methods like secant become much more\n",
    "complex.  Our basic strategies usually involve Newton and Newton-like iterations.  One of the difficulties with Newton\n",
    "is that forming and factoring Jacobians at every step can be expensive, so we talked about many ways of approximating\n",
    "the Newton fixed point iteration: chord iterations, Shaminskii, Newton-Krylov.  We also talked about non-fixed-point iterations like Broyden and various sequence acceleration and extrapolation schemes.  These latter types of iterations\n",
    "are great for projects, but they are not an easy source of exam questions.\n",
    "\n",
    "The (unconstrained) optimization picture is closely related to the nonlinear equation solving picture.  We're still\n",
    "trying to solve a nonlinear equation, $\\nabla \\phi(x) = 0$.  But the optimization perspective gives us some additional\n",
    "tools for thinking about things.  In particular, we have additional iterations (e.g. gradient descent and similar\n",
    "first-order iterations) and different ways of judging the standard iterations (e.g. checking whether a given step\n",
    "is a descent direction).  That alternate perspective plays a huge role in how we think about *globalization*\n",
    "strategies, of which we discussed three:\n",
    "\n",
    "- Backtracking line search methods and things like the Armijo test\n",
    "- Trust region methods\n",
    "- Continuation methods\n",
    "\n",
    "Approximate Newton iterations for particular problems are a *great* source of exam questions, partly because these are\n",
    "places where there is a lot of room for thoughtful special-purpose tweaks to algorithms.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "Describe a fast algorithm to solve $Ax = b(x_n)$ where $A$ is a nonsingular square matrix and\n",
    "$b : \\mathbb{R} \\rightarrow \\mathbb{R}^n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constrained problems\n",
    "\n",
    "The material on methods for dealing with constraints that we covered this week is optional, and will not\n",
    "be on the exam.  That said, you should absolutely know how to use the method of Lagrange multipliers to\n",
    "write down the stationary point equations for an equality-constrained optimization problems, and how to\n",
    "compute with that.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "How would you solve the problem of minimizing $\\|Ax-b\\|_2$ subject to $\\sum_j x_j = 1$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
