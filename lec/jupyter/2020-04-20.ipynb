{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-20\n",
    "\n",
    "## Life beyond fixed point iterations\n",
    "\n",
    "So far, the methods we have discussed for solving nonlinear systems all\n",
    "involve some flavor of fixed point iteration \n",
    "$$\n",
    "  x_{k+1} = G(x_k).\n",
    "$$ \n",
    "Our chief example of such an iteration is Newton’s method, where\n",
    "$$\n",
    "  G(x) = x - f'(x)^{-1} f(x),\n",
    "$$ \n",
    "but we have also considered various\n",
    "other iterations where the Jacobian is replaced by some more convenient\n",
    "approximation. However, all the methods we have discussed in this\n",
    "setting compute the next point based *only* on the behavior at the\n",
    "current point, and not any previous points.\n",
    "\n",
    "In earlier parts of the class, of course, we *did* consider\n",
    "iterations where the next approximation depends on more than just the\n",
    "current approximation. Two such iterations spring particularly to mind:\n",
    "\n",
    "-   The *secant method* is the 1D iteration in which we approximate\n",
    "    the derivative at the current point by a finite difference through\n",
    "    the last two points; that is,\n",
    "    $$x_{k+1} = x_k - \\frac{x_k-x_{k-1}}{f(x_k)-f(x_{k-1})} f(x_k).$$\n",
    "    Hence, the secant method depends not only on the behavior at the\n",
    "    current point $x_k$, but also on the previous point $x_{k+1}$. The\n",
    "    secant method, along with bisection, is the building block for\n",
    "    Brent’s method, which is at once robust, rapidly convergent, and\n",
    "    derivative free.\n",
    "\n",
    "-   In iterative methods for solving linear systems, we started with\n",
    "    fixed point iterations based on matrix splittings (such as Jacobi\n",
    "    and Gauss-Seidel), but then recommended them primarily as an\n",
    "    approach to preconditioning the more powerful Krylov subspace\n",
    "    methods. One way to think about (preconditioned) Krylov subspaces is\n",
    "    that they are spanned by the iterates of one of these stationary\n",
    "    methods. Taking a general element of this space — a linear\n",
    "    combination of the iterates of the stationary method — improves the\n",
    "    convergence of stationary methods.\n",
    "\n",
    "In this lecture, we describe nonlinear analogues to both of these\n",
    "approaches. The generalization of the secant condition to more than one\n",
    "direction gives *Broyden’s method*, the most popular of the\n",
    "*quasi-Newton* methods that couple a Newton-like update of the\n",
    "approximate solution with an update for an approximate Jacobian. The\n",
    "generalization of the Krylov subspace approach leads us to\n",
    "*extrapolation methods*, and we will describe a particular example\n",
    "known as *reduced rank extrapolation*. We conclude the lecture with\n",
    "a brief discussion of *Anderson acceleration*, a method that can be\n",
    "seen as a generalization either of the secant approach or of Krylov\n",
    "subspace methods.\n",
    "\n",
    "As with last time, we will continue to use the autocatalytic reaction-diffusion\n",
    "equation as a running example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the initial guess from last lecture\n",
    "N = 100\n",
    "xx = range(1, N, length=N)/(N+1)\n",
    "q = xx.*(1.0 .- xx)\n",
    "v0 = 0.5*q\n",
    "\n",
    "function autocatalytic(v)\n",
    "    N = length(v)\n",
    "    fv        = exp.(v)\n",
    "    fv        -= 2*(N+1)^2*v\n",
    "    fv[1:N-1] += (N+1)^2*v[2:N  ]\n",
    "    fv[2:N  ] += (N+1)^2*v[1:N-1]\n",
    "    fv\n",
    "end\n",
    "\n",
    "function Jautocatalytic(v)\n",
    "    N = length(v)\n",
    "    SymTridiagonal(exp.(v) .- 2*(N+1)^2, (N+1)^2 * ones(N-1))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broyden\n",
    "\n",
    "Quasi-Newton methods take the form \n",
    "$$\n",
    "  x_{k+1} = x_k - J_k^{-1} f(x_k)\n",
    "$$\n",
    "together with an updating formula for computing successive approximate\n",
    "Jacobians $J_k$. By far the most popular such updating formula is\n",
    "*Broyden’s (good) update*:\n",
    "$$\n",
    "  J_{k+1} = J_k + \\frac{f(x_{k+1}) s_k^T}{\\|s_k\\|^2}, \\quad s_k \\equiv x_{k+1}-x_k.\n",
    "$$\n",
    "Broyden’s update satisfies the *secant condition*\n",
    "$$\n",
    "  J_{k+1} (x_{k+1}-x_k) = f(x_{k+1})-f(x_k),\n",
    "$$\n",
    "which is the natural\n",
    "generalization of the 1D secant condition. In one dimension, the secant\n",
    "condition is enough to uniquely determine the Jacobian approximation,\n",
    "but this is not the case in more than one dimension. Hence, Broyden’s\n",
    "update gives the matrix $J_{k+1}$ that minimizes $\\|J_{k+1}-J_k\\|_F^2$\n",
    "subject to the secant condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function naive_broyden(x, f, J, rtol=1e-6, nsteps=100; monitor=(x, fx) -> nothing)\n",
    "\n",
    "    x = copy(x)            # Copy x (so we can overwrite without clobbering the initial guess)\n",
    "    J = Array{Float64}(J)  # Dense matrix representation of the initial Jacobian\n",
    "\n",
    "    # Take an initial step to get things started\n",
    "    fx = f(x)\n",
    "    monitor(x, fx)\n",
    "    s = -J\\fx\n",
    "    \n",
    "    for step = 1:nsteps\n",
    "\n",
    "        # Take Broyden step and update Jacobian (overwrite old arrays)\n",
    "        x[:] += s\n",
    "        fx[:] = f(x)\n",
    "        J[:,:] += fx*(s/norm(s)^2)'\n",
    "        s[:] = -J\\fx\n",
    "\n",
    "        # Monitor progress and check for convergence\n",
    "        monitor(x, fx)\n",
    "        if norm(fx) < rtol\n",
    "            return x, fx\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    # We could error out here, but let's just return x and fx\n",
    "    return x, fx\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broyden’s method has three particularly appealing features:\n",
    "\n",
    "-   For sufficiently good starting points $x^0$ (and sufficiently\n",
    "    innocuous initial Jacobian approximations), Broyden’s method is\n",
    "    *$q$-superlinearly* convergent, i.e. there is some constant $K$\n",
    "    and some $\\alpha > 1$ such that\n",
    "    $$\n",
    "      \\|e_{k+1}\\| \\leq K \\|e_k\\|^\\alpha.\n",
    "    $$\n",
    "\n",
    "-   The iteration requires only function values. There is no need for\n",
    "    analytical derivatives.\n",
    "\n",
    "-   Because each step is a rank one update, we can use linear algebra\n",
    "    tricks to avoid the cost of a full factorization at each step that\n",
    "    would normally be required for a Newton-type iteration.\n",
    "\n",
    "The argument that Broyden converges superlinearly is somewhat complex,\n",
    "and we will not cover it in this course; for details of the argument, I suggest \n",
    "Tim Kelley’s [*Iterative Methods for Linear and Nonlinear Equations*](https://doi.org/10.1137/1.9781611970944).\n",
    "The fact that the method requires only function values is\n",
    "obvious from the form of the updates. But the linear algebra tricks bear\n",
    "some further investigation, in part because different tricks are used\n",
    "depending on the type of problem involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist = Array{Float64,1}([])\n",
    "J0 = Jautocatalytic(v0)\n",
    "v = naive_broyden(v0, autocatalytic, J0, 1e-10, monitor=(x, fx)->push!(rhist, norm(fx)))\n",
    "plot(rhist, yscale=:log10, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Give an argument for the claim above that Broyden's update gives the matrix $J_{k+1}$ that minimizes $\\|J_{k+1}-J_k\\|_F^2$ subject to the secant condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense Broyden\n",
    "\n",
    "For moderate-sized problems, implementations of Broyden’s method may\n",
    "maintain a dense factorization of the approximate Jacobian which is\n",
    "updated at each step. This updating can be done economically by\n",
    "exploiting the fact that the update at each step is rank one. For\n",
    "example, the QR factorization of $J_k$ can be updated to a QR\n",
    "factorization of $J_{k+1}$ in $O(n^2)$ time by using a sequence of\n",
    "Givens rotations. More generally, we can compute the QR factorization of\n",
    "the rank one update $A' = A + uv^T$ from the QR factorization\n",
    "$A = \\bar{Q} \\bar{R}$ in three steps:\n",
    "\n",
    "1.  Write $A' = \\bar{Q} (\\bar{R} + \\bar{u} v^T)$ where\n",
    "    $\\bar{u} = \\bar{Q}^T u$.\n",
    "\n",
    "2.  Apply $n-1$ Givens rotations to $\\bar{u}$ to introduce zeros in\n",
    "    position $n, n-1, \\ldots, 2$. Apply those same rotations to the\n",
    "    corresponding rows of $R$, transforming it into an upper Hessenberg\n",
    "    matrix $H$; and apply the transposed rotations to the corresponding\n",
    "    columns of $\\bar{Q}$ to get $A' = \\tilde{Q} (H + \\gamma e_1 v^T)$.\n",
    "\n",
    "3.  Do Hessenberg QR on $H + \\gamma e_1 v^T$ to finish the\n",
    "    decomposition, i.e. apply run Givens rotations to zero out each\n",
    "    subdiagonal entry in turn\n",
    "\n",
    "In total, this computation involves computing $\\bar{u}$ (in $O(n^2)$\n",
    "time) and applying $2n-2$ Givens rotations across the two factors (also\n",
    "a total of $O(n^2)$ time). So updating the factorization of $J_k$ to get\n",
    "a factorization of $J_{k+1}$ takes $O(n^2)$; and once we have a\n",
    "factorization of $J_{k+1}$, linear solves can be computed in $O(n^2)$\n",
    "time as well. So the total cost per Broyden step is $O(n^2)$ (after an\n",
    "initial $O(n^3)$ factorization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function qrupdate!(Q, R, u, v)\n",
    "    n = length(u)\n",
    "    ubar = Q'*u\n",
    "    for j = n:-1:2\n",
    "        G, r = givens(ubar[j-1], ubar[j], j-1, j)\n",
    "        ubar[j] = 0.\n",
    "        ubar[j-1] = r\n",
    "        R[:,j-1:end] = G*R[:,j-1:end]\n",
    "        Q[:,:] = Q[:,:]*G'\n",
    "    end\n",
    "    R[1,:] += ubar[1]*v\n",
    "    for j = 1:n-1\n",
    "        G, r = givens(R[j,j], R[j+1,j], j, j+1)\n",
    "        R[j,j] = r\n",
    "        R[j+1,j]= 0.\n",
    "        R[:,j+1:n] = G*R[:,j+1:n]\n",
    "        Q[:,:] = Q[:,:]*G'\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random matrix and u/v vectors to test things out\n",
    "A = rand(10,10)\n",
    "u = rand(10)\n",
    "v = rand(10)\n",
    "\n",
    "# Julia QR uses something called WY format for the Q factor\n",
    "# -- we need to change to a standard dense representation to update\n",
    "F = qr(A)\n",
    "Q = Matrix(F.Q)\n",
    "R = F.R\n",
    "\n",
    "# Apply the update and check that Q and R satisfy the desired relations\n",
    "qrupdate!(Q, R, u, v)\n",
    "println(\"Check factorization OK: $(norm(Q*R-A-u*v'))\")\n",
    "println(\"Check orthonormality:   $(norm(Q'*Q-I))\")\n",
    "println(\"Check upper triangular: $(norm(tril(R,-1)))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Complete the following code for Broyden with QR updating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dense_broyden(x, f, J, rtol=1e-6, nsteps=100; monitor=(x, fx) -> nothing)\n",
    "\n",
    "    x = copy(x)            # Copy x (so we can overwrite without clobbering the initial guess)\n",
    "    J = Array{Float64}(J)  # Dense matrix representation of the initial Jacobian\n",
    "    F = qr(A)              # Get a QR factorization\n",
    "    Q = Matrix(F.Q)        # Convert the Q factor to standard dense form\n",
    "    R = F.R                # Extract the R factor\n",
    "    \n",
    "    # Take an initial step to get things started\n",
    "    fx = f(x)\n",
    "    monitor(x, fx, J)\n",
    "    s = -J\\fx\n",
    "    \n",
    "    for step = 1:nsteps\n",
    "\n",
    "        # Take Broyden step and update Jacobian (overwrite old arrays)\n",
    "        x[:] += s\n",
    "        fx[:] = f(x)\n",
    "        \n",
    "        # TODO: Replace these two lines!\n",
    "        J[:,:] += fx*(s/norm(s)^2)'\n",
    "        s[:] = -J\\fx\n",
    "\n",
    "        # Monitor progress and check for convergence\n",
    "        monitor(x, fx)\n",
    "        if norm(fx) < rtol\n",
    "            return x, fx\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    # We could error out here, but let's just return x and fx\n",
    "    return x, fx\n",
    "end\n",
    "\n",
    "rhist = Array{Float64,1}([])\n",
    "J0 = Jautocatalytic(v0)\n",
    "v = naive_broyden(v0, autocatalytic, J0, 1e-10, monitor=(x, fx)->push!(rhist, norm(fx)))\n",
    "plot(rhist, yscale=:log10, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Large-scale Broyden\n",
    "\n",
    "For large-scale problems, the $O(n^2)$ time cost of a dense Broyden\n",
    "update is prohibitive — or perhaps the $O(n^2)$ storage cost is the\n",
    "pain. Either way, we would like an alternative with lower complexity. As\n",
    "long as we have a fast solver for the initial Jacobian $J_0$, we can\n",
    "compute the first several Broyden steps using the\n",
    "Sherman-Morrison-Woodbury formula or using a bordered linear system:\n",
    "$$\\begin{bmatrix}\n",
    "    J_0 & -F_k \\\\\n",
    "    S_k^T & D_k\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} s_{k} \\\\ \\mu \\end{bmatrix} =\n",
    "  \\begin{bmatrix} -f(x_k) \\\\ 0 \\end{bmatrix},$$ where $$\\begin{aligned}\n",
    "  F_k &= \\begin{bmatrix} f(x_1) & \\ldots & f(x_k) \\end{bmatrix}, \\\\\n",
    "  S_k &= \\begin{bmatrix} s_0 & \\ldots & s_{k-1} \\end{bmatrix}, \\\\\n",
    "  D_k &= \\operatorname{diag}\\left( \\|s_0\\|^2, \\ldots, \\|s_{k-1}\\|^2 \\right).\\end{aligned}$$\n",
    "Defining vectors $g_k = -J_0^{-1} f(x_k)$, we can rewrite the system as\n",
    "$$\\begin{bmatrix}\n",
    "    I     & G_k \\\\\n",
    "    S_k^T & D_k\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix} s_{k} \\\\ \\mu \\end{bmatrix} =\n",
    "  \\begin{bmatrix} g_k \\\\ 0 \\end{bmatrix}.$$ Defining $w = -\\mu$ and\n",
    "performing block Gaussian elimination then yields $$\\begin{aligned}\n",
    "  (D_k - S_k^T G_k) w &= S_k^T g_k \\\\\n",
    "  s_k &= g_k + G_k w.\\end{aligned}$$ Hence, to go from step $k$ to step\n",
    "$k+1$ in this framework requires one new solve with $J_0$, $O(k^2)$ time\n",
    "to solve the Schur complement system using an existing factorization\n",
    "(or $O(k^3)$ to refactor each time),\n",
    "and $O(nk)$ time to form the new step and extend the Schur complement\n",
    "system for the next solve. As long as $k$ is modest, this is likely an\n",
    "acceptable cost. For larger $k$, though, it may become annoying.\n",
    "Possible solutions include *limited memory Broyden*, which only\n",
    "takes into account the last $m$ steps of the iteration when computing\n",
    "the modified Jacobian; or *restarted Broyden*, which restarts from\n",
    "an approximate Jacobian of $J_0$ after every $m$ Broyden steps.\n",
    "\n",
    "We illustrate the idea with a simple limited memory Broyden code.  We do not\n",
    "attempt anything clever with the Schur complement solve.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function limited_broyden(x, f, J0solve; rtol=1e-6, nsteps=100, m=10, monitor=(x, fx) -> nothing)\n",
    "\n",
    "    x = copy(x)            # Copy x (so we can overwrite without clobbering the initial guess)\n",
    "    G = zeros(length(x), m)\n",
    "    S = zeros(length(x), m)\n",
    "    D = zeros(m, m)\n",
    "\n",
    "    # Take an initial step to get things started\n",
    "    fx = f(x)\n",
    "    monitor(x, fx)\n",
    "    s = -J0solve(fx)\n",
    "    \n",
    "    for step = 1:nsteps\n",
    "        \n",
    "        # Take Broyden step and evaluate function\n",
    "        x[:] += s\n",
    "        fx[:] = f(x)\n",
    "        \n",
    "        # Update G, S, and D\n",
    "        k = mod(step-1, m)+1\n",
    "        S[:,k] = s\n",
    "        G[:,k] = -J0solve(fx)\n",
    "        D[k,k] = norm(s)^2\n",
    "        \n",
    "        # Solve next step (keep track of at most m previous steps)\n",
    "        p = min(step, m)\n",
    "        w = (D[1:p,1:p] - S[:,1:p]'*G[:,1:p])\\(S[:,1:p]'*G[:,k])\n",
    "        s[:] = G[:,k] + G[:,1:p]*w\n",
    "\n",
    "        # Monitor progress and check for convergence\n",
    "        monitor(x, fx)\n",
    "        if norm(fx) < rtol\n",
    "            return x, fx\n",
    "        end\n",
    "\n",
    "    end\n",
    "    \n",
    "    # We could error out here, but let's just return x and fx\n",
    "    return x, fx\n",
    "end\n",
    "\n",
    "rhist = Array{Float64,1}([])\n",
    "J0 = Jautocatalytic(v0)\n",
    "J0solve(x) = J0\\x\n",
    "v = limited_broyden(v0, autocatalytic, J0solve, rtol=1e-10, monitor=(x, fx)->push!(rhist, norm(fx)))\n",
    "plot(rhist, yscale=:log10, legend=false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Given that the cost of a tridiagonal solve is $O(n)$, what is the dominant cost per iteration in the limited-memory Broyden code above?  Could that cost be reduced by more clever book-keeping?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduced rank extrapolation\n",
    "\n",
    "*Extrapolation methods* are sequence transformations that convert a\n",
    "slowly-converging sequence into one that is more rapidly convergent. For\n",
    "reasons that are perhaps as much cultural as technical, these methods\n",
    "are often not covered in most introductory numerical analysis texts and\n",
    "courses, but they can be extremely powerful when applied correctly.\n",
    "\n",
    "We will be concerned with *vector extrapolation methods*, which\n",
    "apply to a vector sequence; popular examples include reduced rank\n",
    "extrapolation, minimal polynomial extrapolation, and vector Padé\n",
    "methods. We will focus on the example of reduced rank extrapolation\n",
    "(RRE), but all of these methods have a similar flavor. We suppose\n",
    "$x_1, x_2, \\ldots \\in {\\mathbb{R}}^n$ converges to some limit $x_*$,\n",
    "albeit slowly. We also suppose an *error model* that describes\n",
    "$e_k = x_k-x_*$; in the case of RRE, the error model is\n",
    "$$\n",
    "  e_k \\approx \\sum_{j=1}^m u_j \\alpha_j^k.\n",
    "$$ \n",
    "If this error model were exact (and if we knew the $\\alpha_j$), \n",
    "we could define a degree $m+1$ polynomial \n",
    "$$\n",
    "  p(z) = \\sum_{i=0}^m \\gamma_i z^i\n",
    "$$\n",
    "such that $p(z) = 1$ and $p(\\alpha_j) = 0$ for each $j$. Then \n",
    "$$\\begin{aligned}\n",
    "  \\sum_{i=0}^m \\gamma_i x_{k+i}\n",
    "  &= \\sum_{i=0}^m \\gamma_i \\left(x_* + \\sum_{j=1}^m u_j \\alpha_j^{k+i}\\right) \\\\\n",
    "  &= p(1) x_* + \\sum_{j=1}^m u_j \\alpha_j^k p(\\alpha_j) \\\\\n",
    "  &= x_*.\n",
    "\\end{aligned}$$\n",
    "Of course, in practice, the error model is not\n",
    "exact, and we do not know the $\\alpha_j$! Nonetheless, we can come up\n",
    "with an appropriate choice of polynomials by asking for coefficients\n",
    "$\\gamma$ such that\n",
    "$$\n",
    "  r_k =  \\sum_{i=0}^m \\gamma_i x_{k+i+1} - \\sum_{i=0}^m \\gamma_i x_{k+i}\n",
    "$$\n",
    "is as small as possible in a least squares sense; writing\n",
    "$f_{k+i} = x_{k+i+1}-x_{k+i}$ and\n",
    "$F_k = \\begin{bmatrix} f_k & \\ldots & f_{k+m} \\end{bmatrix}$, we then\n",
    "have\n",
    "$$\n",
    "  \\mbox{minimize } \\frac{1}{2} \\|F_k \\gamma\\|_2^2 \\quad \\mbox{s.t. } \\sum_i \\gamma_i = 1.\n",
    "$$\n",
    "The Lagrangian for this problem is\n",
    "$$\n",
    "  L(\\gamma, \\mu) = \\frac{1}{2} \\gamma^T (F_k^T F_k) \\gamma + \\mu (e^T \\gamma - 1),\n",
    "$$\n",
    "where $e$ is the vector of all ones; differentiating gives us the stationary equations\n",
    "$$\n",
    "  \\begin{bmatrix} F_k^T F_k & e \\\\ e^T & 0 \\end{bmatrix}\n",
    "  \\begin{bmatrix} \\gamma \\\\ \\mu \\end{bmatrix} =\n",
    "  \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}.\n",
    "$$\n",
    "\n",
    "If $F_k = Q_k R_k$ is an economy QR decomposition, the solution to this\n",
    "minimization problem is\n",
    "$$\n",
    "  \\gamma = (R_k^{-1} y)/\\|y\\|^2, \\quad y = R_k^{-T} e, \\quad \n",
    "  e = \\begin{bmatrix} 1 \\\\ \\vdots \\\\1 \\end{bmatrix}.\n",
    "$$\n",
    "In principle, we can compute the QR decomposition of $F_{k+1}$ from the QR decomposition of\n",
    "$F_k$ relatively quickly (in $O(nm)$ time). Hence, each time we want to\n",
    "compute one more vector in the extrapolated sequence, the cost is that\n",
    "of forming one more vector in the original sequence followed by $O(nm)$\n",
    "additional work.  However, to demonstrate the idea, let's just write this in\n",
    "the most obvious way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function rre(X)\n",
    "    m = size(X)[2]-1\n",
    "    if m == 0\n",
    "        return X[:,1]\n",
    "    end\n",
    "    F = qr(X[:,2:end]-X[:,1:end-1])\n",
    "    y = F.R'\\ones(m)\n",
    "    γ = (F.R\\y)/norm(y)^2\n",
    "    return X[:,2:end]*γ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhist0 = Array{Float64,1}([])\n",
    "rhistr = Array{Float64,1}([])\n",
    "J0 = Jautocatalytic(0*v0)\n",
    "X = zeros(length(v0), 20)\n",
    "X[:,1] = 0*v0\n",
    "fx = autocatalytic(X[:,1])\n",
    "for k = 2:20\n",
    "    X[:,k] = X[:,k-1]-J0\\fx\n",
    "    fx[:] = autocatalytic(X[:,k])\n",
    "    xrre = rre(X[:,1:k])\n",
    "    push!(rhist0, norm(fx[:]))\n",
    "    push!(rhistr, norm(autocatalytic(xrre)))\n",
    "end\n",
    "\n",
    "plot(rhist0, yscale=:log10, label=\"Original sequence\")\n",
    "plot!(rhistr, label=\"RRE-accelerated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When applied to a the iterates of a stationary iterative method for\n",
    "solving $Ax = b$, reduced rank extrapolation is formally the same as the\n",
    "GMRES iteration. In numerical practice, though, the orthogonalization\n",
    "that takes place when forming the Krylov subspace bases in GMRES\n",
    "provides much better numerical stability than we would get from RRE.\n",
    "Whether we apply the method to linear or nonlinear problems, the\n",
    "matrices $U_k$ in RRE are often rather ill conditioned, and the\n",
    "coefficient vectors $\\gamma$ have large positive and negative entries,\n",
    "so that forming $\\sum_i \\gamma_i x_{k+i}$ may lead to significant\n",
    "cancellation. For this reason, one may want to choose modest values of\n",
    "$m$ in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Explain why the QR-based algorithm given above to minimize $\\|F_k \\gamma\\|^2$ subject to $e^T \\gamma = 1$ satisfies the stationary conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anderson acceleration\n",
    "\n",
    "*Anderson acceleration* is an old method, common in computational\n",
    "chemistry, that has recently become popular for more varied problems\n",
    "thanks to the work of Tim Kelley, Homer Walker, and various others. In\n",
    "many ways, it is similar to reduced rank extrapolation applied to a\n",
    "sequence of iterates from a fixed point iteration. If we wish to find\n",
    "$x_*$ such that $$f(x_*) = g(x_*) - x_* = 0,$$ then reduced rank\n",
    "extrapolation — without any linear algebraic trickery — at step $k$\n",
    "looks like \n",
    "$$\\begin{aligned}\n",
    "  m_k & = \\min(m,k) \\\\\n",
    "  F_k &= \\begin{bmatrix} f(x_{k-m_k}) & \\ldots & f(x_k) \\end{bmatrix} \\\\\n",
    "  \\gamma^{(k)} &= \\operatorname{argmin} \\|F_k \\gamma\\|_2 \\mbox{ s.t. }\n",
    "  \\sum_{i=0}^{m_k} \\gamma_i^{(k)} = 1 \\\\\n",
    "  s_{k+1} &= \\sum_{i=0}^{m_k} \\gamma_i^{(k)} g(x_{k-m_k+i})\n",
    "\\end{aligned}$$\n",
    "In this application of reduced rank extrapolation, the output sequence\n",
    "$s_k$ and the input sequence $x_k$ (defined by the iteration\n",
    "$x_{k+1} = g(x_k)$) are distinct entities. By contrast, in Anderson\n",
    "acceleration we have just *one* sequence: \n",
    "$$\\begin{aligned}\n",
    "  m_k & = \\min(m,k) \\\\\n",
    "  F_k &= \\begin{bmatrix} f(x_{k-m_k}) & \\ldots & f(x_k) \\end{bmatrix} \\\\\n",
    "  \\gamma^{(k)} &= \\operatorname{argmin} \\|F_k \\gamma\\|_2 \\mbox{ s.t. }\n",
    "  \\sum_{i=0}^{m_k} \\gamma_i^{(k)} = 1 \\\\\n",
    "  x_{k+1} &= \\sum_{i=0}^{m_k} \\gamma_i^{(k)} g(x_{k-m_k+i})\n",
    "\\end{aligned}$$\n",
    "The only difference in the pseudocode is that the last step generates a\n",
    "new $x_{k+1}$ (which feeds into the next iteration), rather than\n",
    "generating a separate output sequence $s_{k+1}$ and computing the next\n",
    "input iterate by a fixed point step. Thus, the difference between\n",
    "Anderson acceleration and reduced rank extrapolation is morally the same\n",
    "as the difference between Gauss-Seidel and Jacobi iteration: in the\n",
    "former case, we try to work with the most recent guesses available.\n",
    "\n",
    "Unsurprisingly, Anderson acceleration (like RRE) is equivalent to GMRES\n",
    "when applied to linear fixed point iterations. Anderson acceleration can\n",
    "also be seen as a multi-secant generalization of Broyden’s iteration.\n",
    "For a good overview of the literature and of different ways of thinking\n",
    "about the method (as well as a variety of interesting applications), I\n",
    "recommend “[Anderson Acceleration for Fixed-Point Iterations](https://doi.org/10.1137/10078356X)” by Walker\n",
    "and Ni (SIAM J. Numer. Anal., Vol. 49, No. 4, pp. 1715–1735)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function anderson_step(X, gX)\n",
    "    m = size(gX)[2]\n",
    "    F = qr(gX-X)\n",
    "    y = F.R'\\ones(m)\n",
    "    γ = (F.R\\y)/norm(y)^2\n",
    "    return gX*γ\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rhistaa = Array{Float64,1}([])\n",
    "J0 = Jautocatalytic(0*v0)\n",
    "\n",
    "X  = zeros(length(v0), 20)\n",
    "gX = zeros(length(v0), 20)\n",
    "\n",
    "X[:,1] = 0*v0\n",
    "fx = autocatalytic(X[:,1])\n",
    "gX[:,1] = X[:,1] - J0\\fx\n",
    "\n",
    "for k = 2:20\n",
    "    X[:,k] = anderson_step(X[:,1:k-1], gX[:,1:k-1])\n",
    "    fx[:] = autocatalytic(X[:,k])\n",
    "    gX[:,k] = X[:,k] - J0\\fx\n",
    "    push!(rhistaa, norm(fx[:]))\n",
    "end\n",
    "\n",
    "plot(rhist0, yscale=:log10, label=\"Original sequence\")\n",
    "plot!(rhistaa, label=\"AA-accelerated\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
