{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture notes for 2020-04-10\n",
    "\n",
    "## Extrapolation: A Hint of Things to Come\n",
    "\n",
    "Stationary iterations are simple. Methods like Jacobi or Gauss-Seidel\n",
    "are easy to program, and it’s (relatively) easy to analyze their\n",
    "convergence. But these methods are also often slow. We’ll talk next time\n",
    "about more powerful *Krylov subspace* methods that use stationary\n",
    "iterations as a building block.\n",
    "\n",
    "There are many ways to motivate Krylov subspace methods. We’ll pick one\n",
    "motivating idea that extends beyond the land of linear solvers and into\n",
    "other applications as well. The key to this idea is the observation that\n",
    "the error in our iteration follows a simple pattern:\n",
    "$$\n",
    "x^{(k)}-x = e^{(k)} = R^k e^{(0)}, \\quad R = M^{-1} N.\n",
    "$$\n",
    "For large $k$, the behavior of the error is dominated by the largest eigenvalue\n",
    "and the associated eigenvector, i.e.\n",
    "$$\n",
    "e^{(k+1)} \\approx \\lambda_1 e^{(k)}.\n",
    "$$ \n",
    "Note that this means\n",
    "$$\n",
    "x^{(k)}-x^{(k+1)} = e^{(k)}-e^{(k+1)} \\approx (1-\\lambda_1) e^{(k)}.\n",
    "$$\n",
    "If we have an estimate for $\\lambda_1$, we can write\n",
    "$$\n",
    "x = x^{(k)} - e^{(k)} \\approx\n",
    "  x^{(k)}-\\frac{x^{(k)}-x^{(k+1)}}{1-\\lambda_1}.\n",
    "$$ \n",
    "That is, we might hope to get a better estimate of $x$ than is provided by $x^{(k)}$ or\n",
    "$x^{(k+1)}$ individually by taking an appropriate linear combination of\n",
    "$x^{(k)}$ and $x^{(k+1)}$.  We illustrate this with the following example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "using Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = rand(4,4)\n",
    "A = A'*A + 4 * I\n",
    "Msolve(x) = x ./ diag(A)\n",
    "\n",
    "b = rand(4)\n",
    "xref = A\\b\n",
    "\n",
    "R = I - diagm(1 ./diag(A)) * A\n",
    "λs = eigvals(R)\n",
    "\n",
    "println(\"Eigenvalues of R: $λs\")\n",
    "\n",
    "# Run iteration and extrapolation\n",
    "x = Msolve(b)\n",
    "λ1 = minimum(λs)\n",
    "err_jac = zeros(100)\n",
    "err_cor = zeros(100)\n",
    "for k = 1:100\n",
    "    r = b-A*x\n",
    "    dx = Msolve(r)\n",
    "    xnew = x + dx\n",
    "    xcor = x + dx/(1-λ1)\n",
    "    x[:] = xnew\n",
    "    err_jac[k] = norm(xnew-xref)\n",
    "    err_cor[k] = norm(xcor-xref)\n",
    "end\n",
    "\n",
    "plot(err_jac[err_jac .> 0], yscale=:log10, label=\"Jacobi\")\n",
    "plot!(err_cor[err_cor .> 0], linestyle=:dash, label=\"Accelerated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This idea generalizes: if we have a sequence\n",
    "of approximations $x^{(0)}, \\ldots, x^{(k)}$, why not ask for the “best”\n",
    "approximation that can be written as a linear combination of the\n",
    "$x^{(j)}$? This is the notion underlying methods such as the conjugate\n",
    "gradient iteration, which we will discuss shortly.\n",
    "\n",
    "#### Questions\n",
    "\n",
    "1.  Consider the acceleration code above with\n",
    "    $$A = \\begin{bmatrix} 4 & 1 \\\\ 1 & 4 & 1 \\\\ & 1 & 4 \\end{bmatrix}.$$\n",
    "    Do you get the same acceleration?  Can you explain why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Big Picture\n",
    "\n",
    "We now start with our discussion of Krylov subspace methods in general,\n",
    "and the famous method of conjugate gradients (CG) in particular. Though\n",
    "this is the iterative method of choice for most positive definite\n",
    "systems, it may be as famously confusing as it is famous. In order\n",
    "to avoid getting lost in the weeds, it seems worthwhile to start with a\n",
    "roadmap:\n",
    "\n",
    "-   We begin with the observation that if vectors $x^{(0)}, \\ldots, x^{(k)}$\n",
    "    are increasingly good approximations to $x$,\n",
    "    then some linear combination of these vectors may produce an even better\n",
    "    approximation. If the original sequence is produced by a stationary\n",
    "    iteration, these vectors span a *Krylov subspace*.\n",
    "\n",
    "-   One can generally show that a big enough Krylov subspace will\n",
    "    contain a good approximation to $x$. Alas, this does not tell us how\n",
    "    to find which vector in the space is best (or even good)! Attempting\n",
    "    to minimize the norm of the error is usually impossible, but it is\n",
    "    possible to minimize the residual (leading to GMRES or MINRES), an\n",
    "    energy function (CG), or some other error-related quantity.\n",
    "\n",
    "-   The basic framework of a Krylov subspace plus a method of choosing\n",
    "    approximations from the space allows us to describe some theoretical\n",
    "    properties of several iterations without telling us why (or if) we\n",
    "    can implement them efficiently and stably. A key practical point is\n",
    "    the computation of well-conditioned bases for the Krylov subspaces,\n",
    "    e.g., using the *Lanczos* algorithm (symmetric case) or the\n",
    "    *Arnoldi* algorithm (nonsymmetric case).\n",
    "    \n",
    "## From Stationary Methods to Krylov Subspaces\n",
    "\n",
    "Earlier in these notes, we tried to motivate the idea that we can\n",
    "improve the convergence of a stationary method by replacing the sequence\n",
    "of guesses\n",
    "$$\n",
    "  x^0, x^1, \\ldots \\rightarrow x\n",
    "$$\n",
    "with *linear combinations* \n",
    "$$\n",
    "  \\tilde{x}^k = \\sum_{j=1}^k \\alpha_{kj} x^j.\n",
    "$$\n",
    "We could always choose $\\alpha_{kj}$ to be one for $k = j$ and zero\n",
    "otherwise, in which case we have the original stationary method; but by\n",
    "choosing the coefficients more carefully, we might do better.\n",
    "\n",
    "We’ve so far written stationary methods as \n",
    "$$\n",
    "  M x^{j+1} = N x^j + b.\n",
    "$$\n",
    "This is equivalent to\n",
    "$$\n",
    "  x^{j+1} = x^j + M^{-1} r^j, \\quad r^j \\equiv b-Ax^j,\n",
    "$$\n",
    "or\n",
    "$$\n",
    "  x^{j+1} = R x^j + M^{-1} b\n",
    "$$\n",
    "where $R = I-M^{-1} A$ is the iteration\n",
    "matrix we’ve seen in our previous analysis. If $x^0 = M^{-1} b$, this\n",
    "gives $$x^j = \\sum_{i=1}^j R^i M^{-1} b.$$ If we look at this expression\n",
    "closely, we might notice that the space spanned by the first $k$\n",
    "iterates of the stationary method is all vectors of the form\n",
    "$$\n",
    "  \\sum_{i=0}^j c_i R^i M^{-1} b.\n",
    "$$\n",
    "If we look a little harder, we might\n",
    "observe that this is equivalent to the space of all vectors of the form\n",
    "$$\n",
    "  \\sum_{i=0}^j c_i (M^{-1} A)^i M^{-1} b = p(M^{-1} A) M^{-1} b\n",
    "$$ where\n",
    "$p(z) = \\sum_{i=1}^j c_i z^i$ is a polynomial of degree at most $j$.\n",
    "\n",
    "In general, the $d$-dimensional Krylov subspace generated by a matrix\n",
    "$A$ and vector $B$ is $$\\begin{aligned}\n",
    "  {\\mathcal{K}}_d(A,b)\n",
    "  &= \\operatorname{span}\\{ b, Ab, A^2 b, \\ldots, A^{d-1} b \\} \\\\\n",
    "  &= \\{ p(A) b : p \\in {\\mathcal{P}}_{d-1} \\}.\\end{aligned}$$ As we have\n",
    "just observed, the iterates of a stationary method form a basis for\n",
    "nested Krylov subspaces generated by $M^{-1}A$ and $M^{-1}b$. If the\n",
    "stationary method converges, we know the Krylov subspaces will\n",
    "eventually contain pretty good approximations to $A^{-1} b$. Let’s now\n",
    "spell this out a little more carefully."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  Recall the *Cayley-Hamilton* theorem, which says that if $p$ is the characteristic polynomial of $A$, then $p(A) = 0$.  Supposing $A$ is nonsingular, we can write $p(z) = z q(z) + \\det(-A)$.  Argue that this implies that $A^{-1} = -q(A)/\\det(A)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the Hamilton-Cayley theorem for a 3-by-3 matrix\n",
    "A = [ 4. 1. 0. ;\n",
    "      1. 4. 1. ;\n",
    "      0. 1. 4. ]\n",
    "λs = eigvals(A)\n",
    "c = [1, -sum(λs), λs[1]*λs[2] + λs[1]*λs[3] + λs[2]*λs[3], -prod(λs)]\n",
    "pA = c[1]*A^3 + c[2]*A^2 + c[3]*A + c[4]*I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Illustrate the statement argued above\n",
    "invA = -(c[1]*A^2 + c[2]*A + c[3]*I)/c[4]\n",
    "invA*A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.  Based on the argument above, argue that for $A \\in \\mathbb{R}^{n \\times n}$, \n",
    "    we have $A^{-1} b \\in \\mathcal{K}_n(A, b)$; that is, the $n$-dimensional Krylov space\n",
    "    generated by $A$ and $b$ contains $A^{-1} b$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Power of Polynomials\n",
    "\n",
    "We showed a moment ago that the first $m$ iterates of a stationary\n",
    "method form a basis for the space\n",
    "$$\n",
    "  {\\mathcal{K}}_{m+1}(R, M^{-1}b) = {\\mathcal{K}}_{m+1}(M^{-1} A, M^{-1} b)\n",
    " $$\n",
    "What can we say about the quality of this space for approximation? As it\n",
    "turns out, this turns into a question about polynomial approximations.\n",
    "We will not spell out all the details (nor will this appear on any exams\n",
    "or homework problems for this class), but it’s worth spending a few\n",
    "moments giving an idea of what happens.\n",
    "\n",
    "We have seen that the iterates of the stationary method are\n",
    "$$\n",
    "  x^{(k)} = x + e^{(k)} = x + R^k e^{(0)}\n",
    "$$\n",
    "We would like to take a linear combination\n",
    "$$\n",
    "  \\tilde x^{(m)} = \\sum_{k=0}^m \\gamma_{mk} x^{(k)} = p_m(1) x + p_m(R) e^{(0)}\n",
    "$$\n",
    "where $p_m(z) = \\sum_{k=0}^d \\gamma_{mk} z^k$. Moreover, if $R$ is\n",
    "diagonalizable with $R = V \\Lambda V^{-1}$, then\n",
    "$$\n",
    "  p_m(R) = V p(\\Lambda) V^{-1}.\n",
    "$$\n",
    "For any $p_m$ with $p_m(1) = 1$, we have\n",
    "$$\\begin{aligned}\n",
    "  \\|\\tilde{e}^{(m)}\\|\n",
    "  &= \\|\\tilde{x}^{(m)}-x\\| \\\\\n",
    "  &= \\|p_m(R) e^{(0)}\\| = \\|V p(\\Lambda) V^{-1} e^{(0)}\\| \\\\\n",
    "  &\\leq \\kappa(V) \\max_{\\lambda_j} |p(\\lambda_j)| \\|e^{(0)}\\|.\n",
    "\\end{aligned}$$\n",
    "Hence, we would really like to choose the polynomial that is one at $1$\n",
    "and as small as possible on each of the eigenvalues.\n",
    "\n",
    "A good building block for this type of analysis is the [*Chebyshev polynomials*](https://en.wikipedia.org/wiki/Chebyshev_polynomials):\n",
    "$$\\begin{aligned}\n",
    "  T_0(x) &= 1 \\\\\n",
    "  T_1(x) &= x \\\\\n",
    "  T_{n+1}(x) &= 2x T_n(x) - T_{n-1}(x), \\quad n > 0\n",
    "\\end{aligned}$$\n",
    "Some trigonometry gives us that\n",
    "$$\n",
    "  T_n(x) =\n",
    "  \\begin{cases}\n",
    "    \\cos(n \\cos^{-1}(x)), & |x| \\leq 1 \\\\\n",
    "    \\cosh(n \\cosh^{-1}(x)), & x \\geq 1 \\\\\n",
    "    (-1)^n \\cosh(n \\cosh^{-1}(-x)), & x \\leq -1.\n",
    "  \\end{cases}\n",
    "$$\n",
    "That is, the Chebyshev polynomials oscillate back and forth between $\\pm 1$ on the interval $[-1, 1]$,\n",
    "and grow exponentially quickly outside that interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-1.1, stop=1.1, length=100)\n",
    "Tm = [1. for x in xx]\n",
    "Tn = [x for x in xx]\n",
    "p1 = plot(xx, Tm, label=\"\\$T_0\\$\")\n",
    "for k = 1:6\n",
    "    plot!(xx, Tn, label=\"\\$T_$k\\$\")\n",
    "    Tnew = 2*xx.*Tn - Tm\n",
    "    Tm[:] = Tn\n",
    "    Tn[:] = Tnew\n",
    "end\n",
    "plot(p1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all eigenvalues $\\lambda_j$ of $R$ are real, then we have\n",
    "$$\n",
    "  \\max_{\\lambda_j} |p(\\lambda_j)| \\leq \\max_{|z|<\\rho(R)} |p(z)|,\n",
    "$$\n",
    "and a reasonable way to choose polynomials is to minimize $|p_m(z)|$ on\n",
    "$[-\\rho(R),\\rho(R)]$ subject to the constraint $p_m(1) = 1$. The\n",
    "solution to this problem is the *scaled Chebyshev polynomials*,\n",
    "$$\n",
    "  p_m(z) = \\frac{T_m(z/\\rho)}{T_m(1/\\rho)}\n",
    "$$\n",
    "with which we can show that on $[-\\rho(R), \\rho(R)]$, the optimal $p_m$ satisfies \n",
    "$$\n",
    "\\begin{aligned}\n",
    "  |p_m(z)|\n",
    "  & \\leq \\frac{2}{1+m\\sqrt{2(1-\\rho(R))}} \\\\\n",
    "  & = 2(1-m\\sqrt{2(1-\\rho(R))}) + O(m^2(1-\\rho(R))).\n",
    "\\end{aligned}\n",
    "$$ \n",
    "While the number of steps for the basic stationary iteration to reduce the\n",
    "error by a fixed amount scales roughly like $(1-\\rho(R))^{-1}$, the\n",
    "number of steps to reduce the bound on the optimal error scales like\n",
    "$(1-\\rho(R))^{-1/2}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = range(-1., stop=1, length=1000)\n",
    "ρ = 0.95\n",
    "\n",
    "# Keep track of Tk(1/ρ)\n",
    "Tm1 = 1.\n",
    "Tn1 = 1.0/ρ\n",
    "abs_pm = zeros(100)\n",
    "for k = 1:100\n",
    "    abs_pm[k] = 1.0/Tn1\n",
    "    Tnew1 = 2/ρ * Tn1 - Tm1\n",
    "    Tm1, Tn1 = Tn1, Tnew1\n",
    "end\n",
    "\n",
    "α_theory = exp(-acosh(1/ρ))\n",
    "α_empirical = abs_pm[end]/abs_pm[end-1]\n",
    "\n",
    "println(\"Predicted asymptotic rate: $α_theory\")\n",
    "println(\"Observed rate:             $α_empirical\")\n",
    "\n",
    "plot(abs_pm, yscale=:log10, label=\"\\$|p_m(\\\\rho)|\\$\")\n",
    "plot!(2.0 ./ (1.0 .+ (1:100)*sqrt(2*(1-ρ))), label=\"Coarse bound\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the Chebyshev bounds are correct, and involve a beautiful bit of\n",
    "approximation theory, they are limited. For one thing, they fall apart\n",
    "on non-normal matrices with complex spectra. Even in the SPD case, these\n",
    "bounds are often highly pessimistic in practice. When the eigenvalues of\n",
    "$R$ come in clusters, a relatively low degree polynomial can do a good\n",
    "job of approximating $\\lambda^{-1}$ at each of the clusters, and hence a\n",
    "relatively low-dimensional Krylov subspace may provide excellent\n",
    "solutions.\n",
    "\n",
    "All of this is to say that the detailed convergence theory for Krylov\n",
    "subspace methods can be quite complicated, but understanding a little\n",
    "about the eigenvalues of the problem can provide at least a modicum of\n",
    "insight.\n",
    "\n",
    "For theoretical work, we are fine writing Krylov subspaces as\n",
    "polynomials in $R$ applied to $M^{-1} b$. In practical computations,\n",
    "though, we need a basis. Because the iterates of the stationary method\n",
    "are converge to $x$, they tend to form a very ill-conditioned basis. We\n",
    "would like to keep the same Krylov subspace, but have a different basis\n",
    "– say, for instance, an orthonormal basis. We turn to this task next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "\n",
    "1.  The bound above is quite pessimistic for $\\rho = 0.95$.  Re-run the code for $\\rho$ closer to 1.  What do\n",
    "    you observe?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In the code fragment above, we predicted an asymptotically linear rate of convergence with constant $\\alpha = \\exp(\\cosh^{-1}(1/\\rho))$.  Explain where this comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Lanczos Idea\n",
    "\n",
    "What is good about the “power basis” for a Krylov subspace? That is, why\n",
    "might we like to write\n",
    "$${\\mathcal{K}}_m(A,b) = \\operatorname{span}\\{ b, Ab, A^2b, \\ldots, A^{m-1}b \\}$$\n",
    "rather than choosing a different basis? Though it’s terrible for\n",
    "numerical stability, there are two features of the basis that are\n",
    "attractive:\n",
    "\n",
    "-   The power bases span nested Krylov subspaces. Given the vectors\n",
    "    $b, \\ldots, A^{m-1} b$ spanning ${\\mathcal{K}}_m(A,b)$, we only need\n",
    "    one more vector ($A^d b$) to span ${\\mathcal{K}}_{m+1}(A,b)$.\n",
    "\n",
    "-   Each successive vector can be computed from the previous vector with\n",
    "    a single multiplication by $A$. There is no other overhead.\n",
    "\n",
    "While we dislike the power basis from the perspective of stability, we\n",
    "would like to keep these attractive features for any alternative basis.\n",
    "\n",
    "We’ve already described one approach to converting the vectors in a\n",
    "power basis into a more convenient set of vectors. Define the matrix\n",
    "$$\n",
    "X^{(m)} = \\begin{bmatrix} b & Ab & A^2 b & \\ldots & A^{m-1} b \\end{bmatrix}\n",
    "$$\n",
    "and consider the economy QR decomposition\n",
    "$$\n",
    "X^{(m)} = Q^{(m)} R^{(m)}, \\quad\n",
    "Q^{(m)} = \\begin{bmatrix} q_1 & q_2 & \\ldots & q_{m} \\end{bmatrix}.\n",
    "$$\n",
    "The columns of $Q^{(m)}$ are orthonormal and for any $k \\leq m$, the\n",
    "first $k$ columns of $Q^{(m)}$ span the same space as the first $k$\n",
    "columns of $X^{(m)}$. But forming $X^{(m)}$ and running QR is\n",
    "unattractive for two reasons. First, a dense QR decomposition may\n",
    "involve a nontrivial amount of extra work, particularly as $m$ gets\n",
    "large; and second, simply forming the rounded version of $X^{(m)}$ is\n",
    "enough to get us into numerical trouble, even if we were able to run QR\n",
    "with no additional rounding errors. We need a better approach.\n",
    "\n",
    "One helpful observation is that\n",
    "$$\n",
    "{\\mathcal{R}}(AQ^{(k)}) = {\\mathcal{R}}(AX^{(k)}) \\subseteq\n",
    "  {\\mathcal{R}}(X^{(k+1)}) = {\\mathcal{R}}(Q^{(k+1)}).\n",
    "$$\n",
    "That is, $Aq_k$ can always be written as a linear combination of $q_1, \\ldots, q_{k+1}$.\n",
    "In matrix terms, this means we can write\n",
    "$$\n",
    "AQ^{(k)} = Q^{(k+1)} \\bar{H}^{(k)}$$ where $$\\bar{H}^{(m)} =\n",
    "\\begin{bmatrix}\n",
    "  h_{11} & h_{12} & h_{13} & \\ldots & h_{1,k-1} & h_{1k} \\\\\n",
    "  h_{21} & h_{22} & h_{23} &        & h_{2,k-1} & h_{2k} \\\\\n",
    "  0     & h_{32} & h_{33} &        & h_{3,k-1} & h_{3k} \\\\\n",
    "        & 0      & h_{43} &       & h_{4,k-1} & h_{4k} \\\\\n",
    "        &        & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n",
    "        &        &        & 0      &  h_{k,k-1} & h_{kk} \\\\\n",
    "        &        &        &        &  0        & h_{k+1,k}\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "A matrix with this structure (all elements below the\n",
    "first subdiagonal equal to zero) is called *upper Hessenberg*.\n",
    "Alternately, we write $$AQ^{(k)} = Q^{(k)} H^{(k)} + q_{k+1} h_{k+1,k}$$\n",
    "where $H^{(k)}$ is the square matrix consisting of all but the last row\n",
    "of $\\bar{H}^{(k)}$. This formula is sometimes called an *Arnoldi\n",
    "decomposition*; it turns out to be crucial in the development of\n",
    "GMRES, one of the most popular iterative solvers for nonsymmetric linear\n",
    "systems. For the moment, though, we want to focus on the symmetric case,\n",
    "and so we will move on.\n",
    "\n",
    "When $A$ is symmetric, we have that $$(Q^{(k)})^T A Q^{(k)} = H^{(k)}$$\n",
    "is also symmetric, as well as being upper Hessenberg; in this case, the\n",
    "matrix $H^{(k)}$ is actually *tridiagonal*, and we write $H^{(k)}$\n",
    "as $T^{(k)}$ to emphasize this fact. Conventionally, $T^{(k)}$ is\n",
    "written as $$\n",
    "T^{(k)} =\n",
    "\\begin{bmatrix}\n",
    "  \\alpha_1 & \\beta_1 \\\\\n",
    "  \\beta_1 & \\alpha_2 & \\beta_2 \\\\\n",
    "          & \\beta_2 & \\alpha_3 & \\ddots \\\\\n",
    "          &         & \\ddots & \\ddots & \\beta_{k-1} \\\\\n",
    "          &         &        & \\beta_{k-1} & \\alpha_k\n",
    "\\end{bmatrix}.\n",
    "$$\n",
    "Converting from matrix notation back into vector\n",
    "notation, we have\n",
    "$$\n",
    "A q_{k} = \\beta_{k-1} q_{k-1} + \\alpha_k q_k + \\beta_k q_{k+1},\n",
    "$$\n",
    "which we can rearrange as\n",
    "$$\n",
    "\\beta_k q_{k+1} = A q_k - \\alpha_k q_k - \\beta_{k-1} q_{k-1}.\n",
    "$$\n",
    "where\n",
    "$$\\begin{aligned}\n",
    "  \\alpha_k &= q_k^T A q_k \\\\\n",
    "  \\beta_{k-1} &= q_k^T A q_{k-1} = q_{k-1}^T A q_k.\\end{aligned}\n",
    "$$\n",
    "Putting everything together, we have the *Lanczos iteration*, in\n",
    "which we obtain each successive vector $q_{k+1}$ by forming $Aq_k$,\n",
    "orthogonalizing against the $q_k$ and $q_{k-1}$ by Gram-Schmidt, and\n",
    "normalizing. We saw this iteration earlier in the semester when we\n",
    "talked about eigenvalue problems and compared different methods of\n",
    "matrix tridiagonalization.\n",
    "\n",
    "Presented as a *fait accompli*, the Lanczos iteration looks like\n",
    "magic. It seems even more like magic when one realizes that despite an\n",
    "instability in the iteration (because of the use of Gram-Schmidt for\n",
    "orthonormalization at each step), the iteration still produces useful\n",
    "information in floating point. The Lanczos iteration is the basis for\n",
    "one of the most popular iterative methods for solving eigenvalue\n",
    "problems, and in that setting it is important to acknowledge and deal\n",
    "with the instability in the method. For the moment, though, we are still\n",
    "interested in solving linear systems, and the method of Conjugate\n",
    "Gradients (also built on Lanczos) turns out to still work great.\n",
    "\n",
    "### Addendum: Three-Term Recurrences\n",
    "\n",
    "The Lanczos iteration allows us to generate a sequence of orthonormal\n",
    "vectors using a three-term recurrence. As it turns out, the same\n",
    "approach leads to three-term recurrences that generate families of\n",
    "orthogonal polynomials, including the Chebyshev polynomials mentioned in\n",
    "passing earlier and the Legendre polynomials that play a significant\n",
    "role in the development of Gaussian quadrature. I consider the details\n",
    "beyond of these connections to be beyond the scope of the current class.\n",
    "But the connections are too beautiful and numerous to not mention that\n",
    "they exist. I would hate for you to walk away from this class with the\n",
    "impression that the mathematical development of the Lanczos iteration is\n",
    "only some quirky trick in numerical linear algebra that gets you part of\n",
    "the way to CG.\n",
    "\n",
    "## From Lanczos to CG\n",
    "\n",
    "We developed the Lanczos iteration, which for a symmetric matrix $A$\n",
    "implicitly generates the decomposition\n",
    "$$A Q^{(k)} = Q^{(k)} T^{(k)} + \\beta_k q_{k+1}$$ where $T^{(k)}$ is a\n",
    "tridiagonal matrix with $\\alpha_1, \\ldots,\n",
    "\\alpha_k$ on the diagonal and $\\beta_1, \\ldots, \\beta_{k-1}$ on the\n",
    "subdiagonal and superdiagonal. The columns of $Q^{(k)}$ form an\n",
    "orthonormal basis for the Krylov subspace ${\\mathcal{K}}_{k}(A,b)$, and\n",
    "are a numerically superior alternative to the power basis. We now turn\n",
    "to using this decomposition to solve linear systems.\n",
    "\n",
    "The *conjugate gradient* algorithm can be characterized as a method\n",
    "that chooses an approximation $\\tilde{x}^{(k)} \\in {\\mathcal{K}}_k(A,b)$\n",
    "by minimizing the energy function\n",
    "$$\\phi(z) = \\frac{1}{2} z^T A z - z^T b$$ over the subspace. Writing\n",
    "$\\tilde{x}^{(k)} = Q^{(k)} u$, and using the fact that $$\\begin{aligned}\n",
    "  (Q^{(k)})^T A Q^{(k)} & = T^{(k)} \\\\\n",
    "  (Q^{(k)})^T b &= \\|b\\| e_1\\end{aligned}$$ we have\n",
    "$$\\phi(Q^{(k)} u) = \\frac{1}{2} u^T T^{(k)} u - \\|b\\| u^T e_1.$$ The\n",
    "stationary equations in terms of $u$ are then $$T^{(k)} u = \\|b\\| e_1.$$\n",
    "\n",
    "In principle, we could solve find the CG solution by forming and solving\n",
    "this tridiagonal system at each step, then taking an appropriate linear\n",
    "combination of the Lanczos basis vectors. Unfortunately, this would\n",
    "require that we keep around the Lanczos vectors, which eventually may\n",
    "take quite a bit of storage. This is essentially what happens in methods\n",
    "like GMRES, but for the method of conjugate gradients, we have not yet\n",
    "exhausted our supply of cleverness. It turns out that we can derive a\n",
    "short recurrence relating the solutions at consecutive steps and their\n",
    "residuals. There are several different ways to this recurrence: one can\n",
    "work from a factorization of the nested tridiagonal matrices $T^{(k)}$,\n",
    "or work out the recurrence based on the optimization interpretation of\n",
    "the problem (this leads to the name “conjugate gradients”). For a\n",
    "detailed discussion, my preferred reference is [*Templates for the\n",
    "Solution of Linear Systems*](http://www.netlib.org/linalg/html_templates/Templates.html), a short book available from SIAM which\n",
    "is also freely available online. The paper “[Introduction to the\n",
    "Conjugate Gradient Method Without the Agonizing Pain](https://www.cs.cmu.edu/~quake-papers/painless-conjugate-gradient.pdf)” provides a longer\n",
    "and possibly gentler introduction.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
